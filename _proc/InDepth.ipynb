{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: In-depth explanation of MonoDense layer\n",
    "output-file: indepth.html\n",
    "title: In-depth explanation\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest method to achieve monotonicity by construc-\n",
    "tion is to constrain the weights of the fully connected neural\n",
    "network to have only non-negative (for non-decreasing vari-\n",
    "ables) or only non-positive values (for non-ascending) vari-\n",
    "ables when used in conjunction with a monotonic activation\n",
    "function, a technique known for 30 years (Archer & Wang,\n",
    "1993). When used in conjunction with saturated (bounded)\n",
    "activation functions such as the sigmoid and hyperbolic tan-\n",
    "gent, these models are difficult to train, i.e. they do not\n",
    "converge to a good solution. On the other hand, when used\n",
    "with non-saturated (unbounded) convex activation functions\n",
    "such as ReLU (Nair & Hinton, 2010), the resulting models\n",
    "are always convex (Liu et al., 2020), severely limiting the\n",
    "applicability of the method in practice.\n",
    "\n",
    "\n",
    "Our main contribution is a modification of the method above\n",
    "which, in conjunction with non-saturated activation func-\n",
    "tions, is capable of approximating non-convex functions\n",
    "as well: when the original activation function is used with\n",
    "additional two monotonic activation functions constructed\n",
    "from it in a neural network with constrained weights, it can\n",
    "approximate any monotone continuous functions.\n",
    "The resulting model is guaranteed to be monotonic, can be\n",
    "used in conjunction with popular convex monotonic non-\n",
    "saturated activation function, doesn’t have any additional\n",
    "parameters compared to a non-monotonic fully-connected\n",
    "network for the same task, and can be trained without any\n",
    "additional requirements on the learning procedure. Experi-\n",
    "mental results show it is exceeding the performance of all\n",
    "other state-of-the-art methods, all while being both simpler\n",
    "(in the number of parameters) and easier to train.\n",
    "Our contributions can be summarized as follows:\n",
    "\n",
    "1. A modification to an existing constrained neural net-\n",
    "work layer enabling it to model arbitrary monotonic\n",
    "function when used with non-saturated monotone con-\n",
    "vex activation functions such as ReLU, ELU, SELU,\n",
    "and alike.\n",
    "\n",
    "2. Experimental comparisons with other recent works\n",
    "showing that the proposed architecture can yield equal\n",
    "or better results than the previous state-of-the-art and\n",
    "with significantly fewer parameters.\n",
    "\n",
    "3. A proof showing that the proposed architecture can\n",
    "approximate any monotone continuous function on a\n",
    "compact subset of Rn for a large class of non-saturated\n",
    "activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "Most of the commonly used activation functions such as ReLU, ELU, SELU, etc. are monotonically increasing zero-centred, convex, lower-bounded non-polynomial functions. When used in a fully-connected, feed-forward neural network with at least one hidden layer and with unconstrained weights, they can approximate any continuous function on a compact subset. The simplest way to construct a monotonic neural network is to constrain its weights when used in conjunction with a monotone activation function. However, when the activation function is convex as well, the constrained neural network is not able to approximate non-convex functions. \n",
    "\n",
    "\n",
    "To better illustrate this, and to propose a simple solution in this particular example, we refer the readers to plots below where the goal is to approximate the simple cubic function $x^3$ using a neural network with a single hidden layer with either $2$ or $32$ neurons and with ReLU activation. A cubic function is apt for our illustration since it is concave in the considered interval $[-1, 0]$ and convex in the interval $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# # | hide\n",
    "\n",
    "# kind = \"Unconstrained ReLU\"\n",
    "# units = 2\n",
    "# model = create_model(kind, units=units)\n",
    "# model.summary()\n",
    "# train_model(model, lr=0.3)\n",
    "# models[(kind, units)] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"width:60%\">\n",
    "\n",
    "<img src=\"../../../images/nbs/images/Unconstrained_ReLU.png\" alt=\"Unconstrained_ReLU\"/>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle\">\n",
    "\n",
    "The plot to the left shows two fully-connected neural networks with one hidden layer with 2 and 32 neurons and ReLU activations approximating the qubic function on the interval [-1, 1].\n",
    "    \n",
    "An unconstrained ReLU network with n neurons can\n",
    "approximate both concave and convex segments of the\n",
    "cubic function using at most n + 1 piecewise linear\n",
    "segments. Increasing the number of neurons will pro-\n",
    "vide a better fit with the function being approximated.\n",
    "Notice that even though the cubic function is mono-\n",
    "tone, there is no guarantee that the trained model will\n",
    "be monotone as well.\n",
    "    \n",
    "</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<img src=\"../../../images/nbs/images/Unconstrained_ReLU.png\" alt=\"Contrained_ReLU\"/>\n",
    "\n",
    "    \n",
    "</td>\n",
    "<td style=\"vertical-align:middle\">\n",
    "\n",
    "If we constrain the weights of the network to be non-\n",
    "negative while still employing ReLU activation, the\n",
    "resulting model is monotone and convex. We can no\n",
    "longer approximate non-convex segments such as the\n",
    "cubic function on [−1, 0] in the figure, and increasing\n",
    "the number of neurons from 2 to 32 does not yield any\n",
    "significant improvement in the approximation.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "<img src=\"../../../images/nbs/images/Contrained_with_ReLU-based_activations.png\"/>\n",
    "    \n",
    "</td>\n",
    "<td style=\"vertical-align:middle\">\n",
    "\n",
    "Our proposed solution uses a combination of three activation functions in the hidden layer in order to gain\n",
    "the ability to model non-convex, monotone continuous\n",
    "functions. Notice that increasing the number of neu-\n",
    "rons increases the number of piecewise linear segments\n",
    "to approximate the cubic function. The resulting net-\n",
    "work is monotone by construction even when trained\n",
    "on noisy data.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "    \n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actvation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our construction is based on generating two additional activation functions from a typical non-saturated activation function such as ReLU, ELU and SELU.\n",
    "\n",
    "We use $\\breve{\\mathcal{A}}$ to denote the set of all zero-centred, monotonically increasing, convex, lower-bounded functions. Let $\\breve{\\rho} \\in \\breve{\\mathcal{A}}$. Then\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}(x) = -\\breve{\\rho}(-x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\rho}(x) = \\begin{cases}\n",
    "      \\breve{\\rho}(x+1)-\\breve{\\rho}(1) & \\text{if }x < 0\\\\\n",
    "      \\hat{\\rho}(x-1)+\\breve{\\rho}(1) & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of such activation functions are given in figures below:\n",
    "\n",
    "![ReLU-based_activations](images/ReLU-based_activations.png) ![ELU-based_activations](images/ELU-based_activations.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonicity indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our construction is preconditioned on a priori knowledge of (partial) monotonicity of a multivariate, multidimensional function $f$. Let $f: K \\mapsto \\mathbb{R}^m$ be defined on a compact segment $K \\subseteq \\mathbb{R}^n$. Then we define its $n$-dimensional *monotonicity indicator vector* $\\mathbf{t} = [t_1, \\dots, t_n]$ element-wise as follows:\n",
    "\n",
    "$$\n",
    "    t_j= \\begin{cases}\n",
    "      1 & \\text{if }\\cfrac{\\partial f(\\mathbf{x})_i} {\\partial x_j} \\geq 0 \\ \n",
    "    \\text{ for each } i \\in \\{1, \\dots , m\\}\\\\\n",
    "      -1 & \\text{if }\\cfrac{\\partial f(\\mathbf{x})_i} {\\partial x_j} \\leq 0 \\ \n",
    "    \\text{ for each } i \\in \\{1, \\dots , m\\}\\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases} \n",
    "    \\: \n",
    "$$\n",
    "\n",
    "Given an $(m \\times n)$-dimensional matrix $\\mathbf{M}$ and $n$-dimensional monotonicity indicator vector $\\mathbf{t}$, we define the operation $|.|_{t}$ assigning an $(m \\times n)$-dimensional matrix $\\mathbf{M'} = |\\mathbf{M}|_{t}$ to $\\mathbf{M}$ element-wise as follows:\n",
    "\n",
    "$$\n",
    "    m'_{j,i}= \\begin{cases}\n",
    "      |m_{j,i}| & \\text{if }t_i=1\\\\\n",
    "      -|m_{j,i}| & \\text{if }t_i=-1\\\\\n",
    "      m_{j,i} & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a kernel $W\\in \\mathbb{R}^{9 × 12}$ with 12 units and 9 inputs before and after applying the monotonicity indicator $t =(-1, -1, -1, 0, 0, 0, 1, 1, 1)$:\n",
    "\n",
    "![original kernel](images/kernel__W__in__mathbb_R___9_x_12__.png)\n",
    "![replaced kernel](images/kernel__(|W_T|_t)_T__after_applying_monotonicity_indicator__t=(-1,_-1,_-1,_0,_0,_0,_1,_1,_1)_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monotonic Dense Unit ([`MonoDense`](https://airtai.github.io/monotonic-nn/monodenselayer.html#monodense) class) uses weight constrains and activation functions constructed as explained above to construct partially monotonic neural networks. The below is the figure from the paper for reference.\n",
    "\n",
    "In the constructor of [`MonoDense`](https://airtai.github.io/monotonic-nn/monodenselayer.html#monodense) class:\n",
    "\n",
    "- the parameter `monotonicity_indicator` corresponds to **t** in the figure below, and\n",
    "\n",
    "- parameters `is_convex`, `is_concave` and `activation_weights` are used to calculate the activation selector **s** as follows:\n",
    "\n",
    "    - if `is_convex` or `is_concave` is **True**, then the activation selector **s** will be (`units`, 0, 0) and (0, `units`, 0), respecively.\n",
    "\n",
    "    - if both  `is_convex` or `is_concave` is **False**, then the `activation_weights` represent ratios between $\\breve{s}$, $\\hat{s}$ and $\\tilde{s}$, respecively. E.g. if `activation_weights = (2, 2, 1)` and `units = 10`, then\n",
    "    \n",
    "$$\n",
    "(\\breve{s}, \\hat{s}, \\tilde{s}) = (4, 4, 2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mono-dense-layer-diagram.png](images/mono-dense-layer-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow is an example of a batched input to `MoneDense` layer with batch size 9 and 12 inputs features.\n",
    "\n",
    "![](images/input__x__in__mathbb_R___9_×_12___with_batch_size_9_and_12_inputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below is an example of a kernel with 18 units and 12 input features.\n",
    "\n",
    "![kernel](images/kernel__(|W_T|_t)_T__in__mathbb_R___12_x_18___after_applying__t=[1,_1,_1,_1,_0,_0,_0,_0,_-1,_-1,_-1,_-1]__in__mathbb_R___12__.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input $x$ is multiplied with kernel $(|W^T|_t)^T \\in \\mathbb{R}^{12 × 18}$ after applying monotonicity indicator $t \\in \\mathbb{R}^{12}$ to it and then the bias $b$ (initially set to 0) is added to it:\n",
    "\n",
    "![output](images/batched_output__y__in__mathbb_R___9_x_18__.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of our proposed monotonic dense unit is its simplicity. We can build deep neural nets with different architectures by plugging in our monotonic dense blocks. We have two functions for building neural networks using [`MonoDense`](https://airtai.github.io/monotonic-nn/monodenselayer.html#monodense) layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-1 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example shown in the figure below corresponds to the standard MLP type of neural network architecture used in general, where each of the input features is concatenated to form one single input feature vector $\\mathbf{x}$ and fed into the network, with the only difference being that instead of standard fully connected or dense layers, we employ monotonic dense units throughout. For the first (or input layer) layer, the indicator vector $\\mathbf{t}$, is used to identify the monotonicity property of the input feature with respect to the output. Specifically, $\\mathbf{t}$ is set to $1$ for those components in the input feature vector that are monotonically increasing and is set to $-1$ for those components that are monotonically decreasing and set to $0$ if the feature is non-monotonic. For the subsequent hidden layers, monotonic dense units with the indicator vector $\\mathbf{t}$ always being set to $1$ are used in order to preserve monotonicity. Finally, depending on whether the problem at hand is a regression problem or a classification problem (or even a multi-task problem), an appropriate activation function (such as linear activation or sigmoid or softmax) to obtain the final output.\n",
    "\n",
    "![type-1](images/type-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " a (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " b (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " c (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " d (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4)            0           ['a[0][0]',                      \n",
      "                                                                  'b[0][0]',                      \n",
      "                                                                  'c[0][0]',                      \n",
      "                                                                  'd[0][0]']                      \n",
      "                                                                                                  \n",
      " mono_dense_0 (MonoDense)       (None, 64)           320         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64)           0           ['mono_dense_0[0][0]']           \n",
      "                                                                                                  \n",
      " mono_dense_1_increasing (MonoD  (None, 64)          4160        ['dropout[0][0]']                \n",
      " ense)                                                                                            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['mono_dense_1_increasing[0][0]']\n",
      "                                                                                                  \n",
      " mono_dense_2_increasing (MonoD  (None, 10)          650         ['dropout_1[0][0]']              \n",
      " ense)                                                                                            \n",
      "                                                                                                  \n",
      " tf.nn.softmax (TFOpLambda)     (None, 10)           0           ['mono_dense_2_increasing[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = {name: Input(name=name, shape=(1,)) for name in list(\"abcd\")}\n",
    "\n",
    "outputs = MonoDense.create_type_1(\n",
    "    inputs=inputs,\n",
    "    units=64,\n",
    "    final_units=10,\n",
    "    activation=\"elu\",\n",
    "    n_layers=3,\n",
    "    final_activation=\"softmax\",\n",
    "    monotonicity_indicator=dict(a=1, b=0, c=-1, d=0),\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-2 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows another example of a neural network architecture that can be built employing proposed monotonic dense blocks. The difference when compared to the architecture described above lies in the way input features are fed into the hidden layers of neural network architecture. Instead of concatenating the features directly, this architecture provides flexibility to employ any form of complex feature extractors for the non-monotonic features and use the extracted feature vectors as inputs. Another difference is that each monotonic input is passed through separate monotonic dense units. This provides an advantage since depending on whether the input is completely concave or convex or both, we can adjust the activation selection vector $\\mathbf{s}$ appropriately along with an appropriate value for the indicator vector $\\mathbf{t}$. Thus, each of the monotonic input features has a separate monotonic dense layer associated with it. Thus as the major difference to the above-mentioned architecture, we concatenate the feature vectors instead of concatenating the inputs directly. The subsequent parts of the network are similar to the architecture described above wherein for the rest of the hidden monotonic dense units, the indicator vector $\\mathbf{t}$ is always set to $1$ to preserve monotonicity.\n",
    "\n",
    "![type-2](images/type-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " a (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " b (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " c (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " d (InputLayer)                 [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " mono_dense_a_increasing_convex  (None, 8)           16          ['a[0][0]']                      \n",
      "  (MonoDense)                                                                                     \n",
      "                                                                                                  \n",
      " dense_b (Dense)                (None, 8)            16          ['b[0][0]']                      \n",
      "                                                                                                  \n",
      " mono_dense_c_decreasing (MonoD  (None, 8)           16          ['c[0][0]']                      \n",
      " ense)                                                                                            \n",
      "                                                                                                  \n",
      " dense_d (Dense)                (None, 8)            16          ['d[0][0]']                      \n",
      "                                                                                                  \n",
      " preprocessed_features (Concate  (None, 32)          0           ['mono_dense_a_increasing_convex[\n",
      " nate)                                                           0][0]',                          \n",
      "                                                                  'dense_b[0][0]',                \n",
      "                                                                  'mono_dense_c_decreasing[0][0]',\n",
      "                                                                  'dense_d[0][0]']                \n",
      "                                                                                                  \n",
      " mono_dense_0_convex (MonoDense  (None, 32)          1056        ['preprocessed_features[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32)           0           ['mono_dense_0_convex[0][0]']    \n",
      "                                                                                                  \n",
      " mono_dense_1_increasing_convex  (None, 32)          1056        ['dropout_2[0][0]']              \n",
      "  (MonoDense)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 32)           0           ['mono_dense_1_increasing_convex[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " mono_dense_2_increasing_convex  (None, 10)          330         ['dropout_3[0][0]']              \n",
      "  (MonoDense)                                                                                     \n",
      "                                                                                                  \n",
      " tf.nn.softmax_1 (TFOpLambda)   (None, 10)           0           ['mono_dense_2_increasing_convex[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,506\n",
      "Trainable params: 2,506\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = {name: Input(name=name, shape=(1,)) for name in list(\"abcd\")}\n",
    "outputs = MonoDense.create_type_2(\n",
    "    inputs,\n",
    "    units=32,\n",
    "    final_units=10,\n",
    "    activation=\"elu\",\n",
    "    final_activation=\"softmax\",\n",
    "    n_layers=3,\n",
    "    dropout=0.2,\n",
    "    monotonicity_indicator=dict(a=1, b=0, c=-1, d=0),\n",
    "    is_convex=dict(a=True, b=False, c=False, d=False),\n",
    ")\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
