{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monotonic Dense Layer\n",
    "\n",
    "> Additional layers for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from typing import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from numpy.typing import ArrayLike, NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MonotonicDense(Dense):\n",
    "    \"\"\"Monotonic counterpart of the regular Dense Layer of tf.Keras\n",
    "\n",
    "    Args:\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use.\n",
    "        indicator_vector: Vector to indicate which of the inputs are monotonically\n",
    "         increasing or monotonically decreasing or non-monotonic. Has value 1 for\n",
    "          monotonically increasing and -1 for monotonically decreasing and 0 for\n",
    "          non-monotonic variables.\n",
    "        convexity_indicator: If the value is 0 or 1, then all elements of the\n",
    "          activation selector will be 0 or 1, respectevely. If None, epsilon will be\n",
    "          used to determine the number of 0 and 1 in the activation selector.\n",
    "        epsilon: Percentage of elements with value 1 in the activation vector if\n",
    "          convexity_indicator is None, ignored otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        indicator_vector: Optional[NDArray[np.int_]] = None,\n",
    "        convexity_indicator: Optional[int] = None,\n",
    "        epsilon: float = 0.5,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(MonotonicDense, self).__init__(units, activation=None, **kwargs)\n",
    "        self.org_activation = activation\n",
    "\n",
    "        self.indicator_vector = (\n",
    "            np.array(indicator_vector, dtype=\"int32\")\n",
    "            if indicator_vector is not None\n",
    "            else None\n",
    "        )\n",
    "        self.convexity_indicator = convexity_indicator\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(MonotonicDense, self).build(input_shape)\n",
    "        if self.indicator_vector is not None:\n",
    "            self.indicator_vector = tf.reshape(\n",
    "                tf.constant(self.indicator_vector),\n",
    "                (-1, 1),\n",
    "            )\n",
    "        if self.convexity_indicator is not None:\n",
    "            self.convexity_indicator = tf.constant(self.convexity_indicator)\n",
    "\n",
    "    def get_activation_selector(self, inputs) -> tf.Tensor:\n",
    "        n_ones = int(round(self.units * self.epsilon))\n",
    "        n_zeros = self.units - n_ones\n",
    "        activation_selector = tf.concat(\n",
    "            [\n",
    "                tf.ones(shape=(tf.shape(inputs)[0], n_ones), dtype=\"bool\"),\n",
    "                tf.zeros(shape=(tf.shape(inputs)[0], n_zeros), dtype=\"bool\"),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        return activation_selector\n",
    "\n",
    "    @contextmanager\n",
    "    def replace_kernel(self):\n",
    "        \"\"\"Replace kernel with absolute values of weights based on indicator vector\n",
    "\n",
    "        This is implemented as context manager to ensure rollback functionality\n",
    "        after the calculation is finished.\n",
    "        \"\"\"\n",
    "        kernel_org = self.kernel\n",
    "\n",
    "        if self.indicator_vector is not None:\n",
    "            abs_kernel = tf.abs(self.kernel)\n",
    "            kernel_1 = tf.where(\n",
    "                self.indicator_vector == 1,\n",
    "                abs_kernel,\n",
    "                self.kernel,\n",
    "            )\n",
    "            self.kernel = tf.where(self.indicator_vector == -1, -abs_kernel, kernel_1)\n",
    "        else:\n",
    "            self.kernel = tf.abs(self.kernel)\n",
    "        yield\n",
    "\n",
    "        self.kernel = kernel_org\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # calculate linear forward pass with weights set to either positive or\n",
    "        # negative values based on the value of indicator_vector\n",
    "        with self.replace_kernel():\n",
    "            y = super(MonotonicDense, self).call(inputs)\n",
    "\n",
    "        if self.org_activation:\n",
    "            activation = tf.keras.activations.get(self.org_activation)\n",
    "            if self.convexity_indicator == 1:\n",
    "                y = activation(y)\n",
    "            elif self.convexity_indicator == -1:\n",
    "                y = -activation(-y)\n",
    "            else:\n",
    "                activation_selector = self.get_activation_selector(inputs)\n",
    "\n",
    "                y1 = activation(y)\n",
    "                y2 = -activation(-y)\n",
    "\n",
    "                y = tf.where(activation_selector, y1, y2)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[0.8502902 , 2.1193693 , 1.1444832 , 0.9975339 , 0.9893499 ],\n",
       "       [0.7763262 , 2.0163271 , 1.1297605 , 0.94500107, 1.0620797 ],\n",
       "       [0.5978056 , 1.4017718 , 0.98523664, 0.79769945, 0.766499  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "x = rng.uniform(size=(3, 4))\n",
    "\n",
    "layer = MonotonicDense(units=5)\n",
    "y = layer(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
