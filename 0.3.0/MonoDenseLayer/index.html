
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Monotonic Neural Networks implemented in Keras">
      
      
        <meta name="author" content="AIRT Technologies d.o.o.">
      
      
        <link rel="canonical" href="https://monotonic.airt.ai/0.3.0/MonoDenseLayer/">
      
      
      
      <link rel="icon" href="../overrides/images/airt_icon_blue.svg">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.15">
    
    
      
        <title>Monotonic dense layer - Monotonic Neural Networks</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../overrides/css/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-F7V44YPJHR"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-F7V44YPJHR",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-F7V44YPJHR",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>

  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  
  
    
  
  
  <meta property="og:type" content="website" />
  <meta property="og:title" content="Monotonic Neural Networks - Monotonic dense layer" />
  <meta property="og:description" content="Monotonic Neural Networks implemented in Keras" />
  <meta property="og:url" content="https://monotonic.airt.ai/0.3.0/MonoDenseLayer/" />
  <meta property="og:image" content="https://opengraph.githubassets.com/1686060464.736638/airtai/monotonic-nn" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Monotonic Neural Networks - Monotonic dense layer" />
  <meta name="twitter:description" content="Monotonic Neural Networks implemented in Keras" />
  <meta name="twitter:image" content="https://opengraph.githubassets.com/1686060464.736638/airtai/monotonic-nn" />

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="light-blue">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#monotonic-dense-layer" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
          <aside class="md-banner md-banner--warning">
            <div class="md-banner__inner md-grid md-typeset">
              
  You're not viewing the latest version.
  <!-- nosemgrep -->
  <a href="../.."> 
    <strong>Click here to go to latest.</strong>
  </a>

            </div>
            <script>var el=document.querySelector("[data-md-component=outdated]"),outdated=__md_get("__outdated",sessionStorage);!0===outdated&&el&&(el.hidden=!1)</script>
          </aside>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Monotonic Neural Networks" class="md-header__button md-logo" aria-label="Monotonic Neural Networks" data-md-component="logo">
      
  <img src="../overrides/images/airt_icon_blue.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Monotonic Neural Networks
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Monotonic dense layer
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="light-blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
            
              <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
              </label>
            
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_2">
            
              <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
              </label>
            
          
        </form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/airtai/monotonic-nn" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    monotonic-nn
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Monotonic Neural Networks" class="md-nav__button md-logo" aria-label="Monotonic Neural Networks" data-md-component="logo">
      
  <img src="../overrides/images/airt_icon_blue.svg" alt="logo">

    </a>
    Monotonic Neural Networks
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/airtai/monotonic-nn" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    monotonic-nn
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Introduction
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../InDepth/" class="md-nav__link">
        In-depth explanation
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Experiments
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Experiments
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../experiments/AutoMPG/" class="md-nav__link">
        Auto MPG
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../experiments/Heart/" class="md-nav__link">
        Heart disease
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../experiments/Compas/" class="md-nav__link">
        COMPAS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../experiments/Blog/" class="md-nav__link">
        Blog
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../experiments/Loan/" class="md-nav__link">
        Loan
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../License/" class="md-nav__link">
        License
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
          airt
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          airt
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1_1" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_1_1" id="__nav_5_1_1_label" tabindex="0">
          keras
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_5_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1_1">
          <span class="md-nav__icon md-icon"></span>
          keras
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1_1_1" >
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_1_1_1" id="__nav_5_1_1_1_label" tabindex="0">
          experiments
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_1_1_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1_1_1">
          <span class="md-nav__icon md-icon"></span>
          experiments
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/airt/keras/experiments/create_tuner_stats/" class="md-nav__link">
        create_tuner_stats
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/airt/keras/experiments/df2ds/" class="md-nav__link">
        df2ds
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/airt/keras/experiments/find_hyperparameters/" class="md-nav__link">
        find_hyperparameters
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/airt/keras/experiments/get_train_n_test_data/" class="md-nav__link">
        get_train_n_test_data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/airt/keras/experiments/peek/" class="md-nav__link">
        peek
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1_1_2" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5_1_1_2" id="__nav_5_1_1_2_label" tabindex="0">
          layers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_5_1_1_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1_1_2">
          <span class="md-nav__icon md-icon"></span>
          layers
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../api/airt/keras/layers/MonoDense/" class="md-nav__link">
        MonoDense
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../CHANGELOG/" class="md-nav__link">
        Releases
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#imports" class="md-nav__link">
    Imports
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monotonic-dense-layer_1" class="md-nav__link">
    Monotonic Dense Layer
  </a>
  
    <nav class="md-nav" aria-label="Monotonic Dense Layer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#actvation-functions" class="md-nav__link">
    Actvation Functions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_activation_functions" class="md-nav__link">
    get_activation_functions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_saturated_activation" class="md-nav__link">
    get_saturated_activation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply_activations" class="md-nav__link">
    apply_activations
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monotonicity-indicator" class="md-nav__link">
    Monotonicity indicator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_monotonicity_indicator" class="md-nav__link">
    get_monotonicity_indicator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#replace_kernel_using_monotonicity_indicator" class="md-nav__link">
    replace_kernel_using_monotonicity_indicator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply_monotonicity_indicator_to_kernel" class="md-nav__link">
    apply_monotonicity_indicator_to_kernel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monotonic-dense-layer_2" class="md-nav__link">
    Monotonic Dense Layer
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#monodense" class="md-nav__link">
    MonoDense
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mono-blocks" class="md-nav__link">
    Mono blocks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#type-1-architecture" class="md-nav__link">
    Type-1 architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#type-2-architecture" class="md-nav__link">
    Type-2 architecture
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="monotonic-dense-layer">Monotonic dense layer<a class="headerlink" href="#monotonic-dense-layer" title="Permanent link">¤</a></h1>
<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

<h2 id="imports">Imports<a class="headerlink" href="#imports" title="Permanent link">¤</a></h2>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">environ</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TF_FORCE_GPU_ALLOW_GROWTH&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
</code></pre></div>
<h2 id="monotonic-dense-layer_1">Monotonic Dense Layer<a class="headerlink" href="#monotonic-dense-layer_1" title="Permanent link">¤</a></h2>
<h3 id="actvation-functions">Actvation Functions<a class="headerlink" href="#actvation-functions" title="Permanent link">¤</a></h3>
<p>We use <span class="arithmatex">\(\breve{\mathcal{A}}\)</span> to denote the set of all zero-centred,
monotonically increasing, convex, lower-bounded functions.</p>
<p>Let <span class="arithmatex">\(\breve{\rho} \in \breve{\mathcal{A}}\)</span>. Then</p>
<p>In the code below, the following names are used for denotation of the
above functions:</p>
<ul>
<li>
<p><code>convex_activation</code> denotes <span class="arithmatex">\(\breve{\rho}\)</span>,</p>
</li>
<li>
<p><code>concave_activation</code> denotes <span class="arithmatex">\(\hat{\rho}\)</span>, and</p>
</li>
<li>
<p><code>saturated_activation</code> denotes <span class="arithmatex">\(\tilde{\rho}\)</span>.</p>
</li>
</ul>
<hr />
<p><a
href="https://github.com/airtai/monotonic-nn/blob/main/airt/_components/mono_dense_layer.py#L48"
target="_blank" style="float:right; font-size:smaller">source</a></p>
<h3 id="get_activation_functions">get_activation_functions<a class="headerlink" href="#get_activation_functions" title="Permanent link">¤</a></h3>
<blockquote>
<div class="highlight"><pre><span></span><code> get_activation_functions (activation:Union[str,Callable[[Union[tensorflow
                           .python.types.core.Tensor,tensorflow.python.typ
                           es.core.TensorProtocol,int,float,bool,str,bytes
                           ,complex,tuple,list,numpy.ndarray,numpy.generic
                           ]],Union[tensorflow.python.types.core.Tensor,te
                           nsorflow.python.types.core.TensorProtocol,int,f
                           loat,bool,str,bytes,complex,tuple,list,numpy.nd
                           array,numpy.generic]],NoneType]=None)
</code></pre></div>
</blockquote>
<hr />
<p><a
href="https://github.com/airtai/monotonic-nn/blob/main/airt/_components/mono_dense_layer.py#L22"
target="_blank" style="float:right; font-size:smaller">source</a></p>
<h3 id="get_saturated_activation">get_saturated_activation<a class="headerlink" href="#get_saturated_activation" title="Permanent link">¤</a></h3>
<blockquote>
<div class="highlight"><pre><span></span><code> get_saturated_activation (convex_activation:Callable[[Union[tensorflow.py
                           thon.types.core.Tensor,tensorflow.python.types.
                           core.TensorProtocol,int,float,bool,str,bytes,co
                           mplex,tuple,list,numpy.ndarray,numpy.generic]],
                           Union[tensorflow.python.types.core.Tensor,tenso
                           rflow.python.types.core.TensorProtocol,int,floa
                           t,bool,str,bytes,complex,tuple,list,numpy.ndarr
                           ay,numpy.generic]], concave_activation:Callable
                           [[Union[tensorflow.python.types.core.Tensor,ten
                           sorflow.python.types.core.TensorProtocol,int,fl
                           oat,bool,str,bytes,complex,tuple,list,numpy.nda
                           rray,numpy.generic]],Union[tensorflow.python.ty
                           pes.core.Tensor,tensorflow.python.types.core.Te
                           nsorProtocol,int,float,bool,str,bytes,complex,t
                           uple,list,numpy.ndarray,numpy.generic]],
                           a:float=1.0, c:float=1.0)
</code></pre></div>
</blockquote>
<hr />
<p><a
href="https://github.com/airtai/monotonic-nn/blob/main/airt/_components/mono_dense_layer.py#L70"
target="_blank" style="float:right; font-size:smaller">source</a></p>
<h3 id="apply_activations">apply_activations<a class="headerlink" href="#apply_activations" title="Permanent link">¤</a></h3>
<blockquote>
<div class="highlight"><pre><span></span><code> apply_activations (x:Union[tensorflow.python.types.core.Tensor,tensorflow
                    .python.types.core.TensorProtocol,int,float,bool,str,b
                    ytes,complex,tuple,list,numpy.ndarray,numpy.generic],
                    units:int, convex_activation:Callable[[Union[tensorflo
                    w.python.types.core.Tensor,tensorflow.python.types.cor
                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl
                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.
                    python.types.core.Tensor,tensorflow.python.types.core.
                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,
                    list,numpy.ndarray,numpy.generic]], concave_activation
                    :Callable[[Union[tensorflow.python.types.core.Tensor,t
                    ensorflow.python.types.core.TensorProtocol,int,float,b
                    ool,str,bytes,complex,tuple,list,numpy.ndarray,numpy.g
                    eneric]],Union[tensorflow.python.types.core.Tensor,ten
                    sorflow.python.types.core.TensorProtocol,int,float,boo
                    l,str,bytes,complex,tuple,list,numpy.ndarray,numpy.gen
                    eric]], saturated_activation:Callable[[Union[tensorflo
                    w.python.types.core.Tensor,tensorflow.python.types.cor
                    e.TensorProtocol,int,float,bool,str,bytes,complex,tupl
                    e,list,numpy.ndarray,numpy.generic]],Union[tensorflow.
                    python.types.core.Tensor,tensorflow.python.types.core.
                    TensorProtocol,int,float,bool,str,bytes,complex,tuple,
                    list,numpy.ndarray,numpy.generic]],
                    is_convex:bool=False, is_concave:bool=False,
                    activation_weights:Tuple[float,float,float]=(7.0, 7.0,
                    2.0))
</code></pre></div>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">plot_applied_activation</span><span class="p">(</span>
    <span class="n">activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">save_pdf</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">save_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;plots&quot;</span><span class="p">,</span>
    <span class="n">font_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">font</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="n">font_size</span><span class="p">}</span>
    <span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s2">&quot;font&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">font</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">256</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

    <span class="p">(</span>
        <span class="n">convex_activation</span><span class="p">,</span>
        <span class="n">concave_activation</span><span class="p">,</span>
        <span class="n">saturated_activation</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">=</span> <span class="n">get_activation_functions</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">apply_activations</span><span class="p">(</span>
        <span class="n">h</span><span class="p">,</span>
        <span class="n">convex_activation</span><span class="o">=</span><span class="n">convex_activation</span><span class="p">,</span>
        <span class="n">concave_activation</span><span class="o">=</span><span class="n">concave_activation</span><span class="p">,</span>
        <span class="n">saturated_activation</span><span class="o">=</span><span class="n">saturated_activation</span><span class="p">,</span>
        <span class="n">units</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">activation_weights</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="n">linestyle</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">h</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$h$&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">plot_kwargs</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;${\rho}(h)$&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">plot_kwargs</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;Applying &quot;</span>
        <span class="o">+</span> <span class="p">(</span><span class="n">activation</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="s2">&quot;__name__&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">activation</span><span class="p">)</span>
        <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;-based activations to </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">-dimensional vector&quot;</span>
        <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot; $h$&quot;</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">save_pdf</span><span class="p">:</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">title</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;.pdf&quot;</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;pdf&quot;</span><span class="p">)</span>
    <span class="c1">#         print(f&quot;Saved figure to: {path}&quot;)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">activation</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;ReLU&quot;</span><span class="p">,</span> <span class="s2">&quot;ELU&quot;</span><span class="p">,</span> <span class="s2">&quot;SELU&quot;</span><span class="p">]:</span>
    <span class="n">plot_applied_activation</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">save_pdf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p><img alt="" src="images/nbs/MonoDenseLayer_files/figure-commonmark/cell-8-output-1.png" /></p>
<p><img alt="" src="images/nbs/MonoDenseLayer_files/figure-commonmark/cell-8-output-2.png" /></p>
<p><img alt="" src="images/nbs/MonoDenseLayer_files/figure-commonmark/cell-8-output-3.png" /></p>
<p><img alt="" src="images/nbs/MonoDenseLayer_files/figure-commonmark/cell-8-output-4.png" /></p>
<h3 id="monotonicity-indicator">Monotonicity indicator<a class="headerlink" href="#monotonicity-indicator" title="Permanent link">¤</a></h3>
<hr />
<p><a
href="https://github.com/airtai/monotonic-nn/blob/main/airt/_components/mono_dense_layer.py#L114"
target="_blank" style="float:right; font-size:smaller">source</a></p>
<h3 id="get_monotonicity_indicator">get_monotonicity_indicator<a class="headerlink" href="#get_monotonicity_indicator" title="Permanent link">¤</a></h3>
<blockquote>
<div class="highlight"><pre><span></span><code> get_monotonicity_indicator (monotonicity_indicator:Union[numpy.__array_li
                             ke._SupportsArray[numpy.dtype],numpy.__nested
                             _sequence._NestedSequence[numpy.__array_like.
                             _SupportsArray[numpy.dtype]],bool,int,float,c
                             omplex,str,bytes,numpy.__nested_sequence._Nes
                             tedSequence[Union[bool,int,float,complex,str,
                             bytes]]], input_shape:Tuple[int,...],
                             units:int)
</code></pre></div>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">units</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>

<span class="k">for</span> <span class="n">monotonicity_indicator</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
<span class="p">]:</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">actual</span> <span class="o">=</span> <span class="n">get_monotonicity_indicator</span><span class="p">(</span>
        <span class="n">monotonicity_indicator</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">units</span><span class="o">=</span><span class="mi">3</span>
    <span class="p">)</span>

    <span class="c1"># rank is 2</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">actual</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="c1"># it is broadcastable to the kernel shape of (input_shape[-1], units)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">expected</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">expected</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">get_monotonicity_indicator</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">units</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">get_monotonicity_indicator</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">units</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">e</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">args</span> <span class="o">==</span> <span class="p">(</span>
    <span class="s2">&quot;operands could not be broadcast together with remapped shapes [original-&gt;remapped]: (3,1)  and requested shape (2,3)&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<hr />
<p><a
href="https://github.com/airtai/monotonic-nn/blob/main/airt/_components/mono_dense_layer.py#L166"
target="_blank" style="float:right; font-size:smaller">source</a></p>
<h3 id="replace_kernel_using_monotonicity_indicator">replace_kernel_using_monotonicity_indicator<a class="headerlink" href="#replace_kernel_using_monotonicity_indicator" title="Permanent link">¤</a></h3>
<blockquote>
<div class="highlight"><pre><span></span><code> replace_kernel_using_monotonicity_indicator
                                              (layer:keras.layers.core.den
                                              se.Dense, monotonicity_indic
                                              ator:Union[tensorflow.python
                                              .types.core.Tensor,tensorflo
                                              w.python.types.core.TensorPr
                                              otocol,int,float,bool,str,by
                                              tes,complex,tuple,list,numpy
                                              .ndarray,numpy.generic])
</code></pre></div>
</blockquote>
<hr />
<p><a
href="https://github.com/airtai/monotonic-nn/blob/main/airt/_components/mono_dense_layer.py#L144"
target="_blank" style="float:right; font-size:smaller">source</a></p>
<h3 id="apply_monotonicity_indicator_to_kernel">apply_monotonicity_indicator_to_kernel<a class="headerlink" href="#apply_monotonicity_indicator_to_kernel" title="Permanent link">¤</a></h3>
<blockquote>
<div class="highlight"><pre><span></span><code> apply_monotonicity_indicator_to_kernel
                                         (kernel:tensorflow.python.ops.var
                                         iables.Variable, monotonicity_ind
                                         icator:Union[numpy.__array_like._
                                         SupportsArray[numpy.dtype],numpy.
                                         __nested_sequence._NestedSequence
                                         [numpy.__array_like._SupportsArra
                                         y[numpy.dtype]],bool,int,float,co
                                         mplex,str,bytes,numpy.__nested_se
                                         quence._NestedSequence[Union[bool
                                         ,int,float,complex,str,bytes]]])
</code></pre></div>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">display_kernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">typing</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="nb">float</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">cm</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;coolwarm_r&quot;</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>

    <span class="n">display</span><span class="p">(</span>
        <span class="n">df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">background_gradient</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">units</span> <span class="o">=</span> <span class="mi">18</span>
<span class="n">input_len</span> <span class="o">=</span> <span class="mi">7</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">)</span>

<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_len</span><span class="p">,)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original kernel:&quot;</span><span class="p">)</span>
<span class="n">display_kernel</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Kernel after applying monotocity indicator 1 for all values:&quot;</span><span class="p">)</span>
<span class="n">monotonicity_indicator</span> <span class="o">=</span> <span class="n">get_monotonicity_indicator</span><span class="p">(</span>
    <span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">units</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">replace_kernel_using_monotonicity_indicator</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">monotonicity_indicator</span><span class="p">):</span>
    <span class="n">display_kernel</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Original kernel:
Kernel after applying monotocity indicator 1 for all values:
</code></pre></div>
<style type="text/css">
#T_0b53b_row0_col0, #T_0b53b_row0_col1, #T_0b53b_row0_col3, #T_0b53b_row0_col5, #T_0b53b_row0_col6, #T_0b53b_row0_col8, #T_0b53b_row0_col9, #T_0b53b_row0_col12, #T_0b53b_row0_col14, #T_0b53b_row0_col15, #T_0b53b_row1_col0, #T_0b53b_row1_col3, #T_0b53b_row1_col4, #T_0b53b_row1_col6, #T_0b53b_row1_col9, #T_0b53b_row1_col10, #T_0b53b_row1_col13, #T_0b53b_row2_col0, #T_0b53b_row2_col2, #T_0b53b_row2_col3, #T_0b53b_row2_col5, #T_0b53b_row2_col8, #T_0b53b_row2_col10, #T_0b53b_row2_col13, #T_0b53b_row2_col14, #T_0b53b_row2_col15, #T_0b53b_row3_col0, #T_0b53b_row3_col1, #T_0b53b_row3_col6, #T_0b53b_row3_col7, #T_0b53b_row3_col9, #T_0b53b_row3_col11, #T_0b53b_row3_col14, #T_0b53b_row3_col15, #T_0b53b_row3_col16, #T_0b53b_row3_col17, #T_0b53b_row4_col0, #T_0b53b_row4_col2, #T_0b53b_row4_col4, #T_0b53b_row4_col5, #T_0b53b_row4_col6, #T_0b53b_row4_col9, #T_0b53b_row4_col14, #T_0b53b_row4_col15, #T_0b53b_row4_col16, #T_0b53b_row4_col17, #T_0b53b_row5_col0, #T_0b53b_row5_col1, #T_0b53b_row5_col4, #T_0b53b_row5_col6, #T_0b53b_row5_col7, #T_0b53b_row5_col10, #T_0b53b_row5_col11, #T_0b53b_row5_col12, #T_0b53b_row5_col15, #T_0b53b_row6_col0, #T_0b53b_row6_col3, #T_0b53b_row6_col5, #T_0b53b_row6_col8, #T_0b53b_row6_col12, #T_0b53b_row6_col15, #T_0b53b_row6_col16 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_0b53b_row0_col2, #T_0b53b_row0_col4, #T_0b53b_row0_col7, #T_0b53b_row0_col10, #T_0b53b_row0_col11, #T_0b53b_row0_col13, #T_0b53b_row0_col16, #T_0b53b_row0_col17, #T_0b53b_row1_col1, #T_0b53b_row1_col2, #T_0b53b_row1_col5, #T_0b53b_row1_col7, #T_0b53b_row1_col8, #T_0b53b_row1_col11, #T_0b53b_row1_col12, #T_0b53b_row1_col14, #T_0b53b_row1_col15, #T_0b53b_row1_col16, #T_0b53b_row1_col17, #T_0b53b_row2_col1, #T_0b53b_row2_col4, #T_0b53b_row2_col6, #T_0b53b_row2_col7, #T_0b53b_row2_col9, #T_0b53b_row2_col11, #T_0b53b_row2_col12, #T_0b53b_row2_col16, #T_0b53b_row2_col17, #T_0b53b_row3_col2, #T_0b53b_row3_col3, #T_0b53b_row3_col4, #T_0b53b_row3_col5, #T_0b53b_row3_col8, #T_0b53b_row3_col10, #T_0b53b_row3_col12, #T_0b53b_row3_col13, #T_0b53b_row4_col1, #T_0b53b_row4_col3, #T_0b53b_row4_col7, #T_0b53b_row4_col8, #T_0b53b_row4_col10, #T_0b53b_row4_col11, #T_0b53b_row4_col12, #T_0b53b_row4_col13, #T_0b53b_row5_col2, #T_0b53b_row5_col3, #T_0b53b_row5_col5, #T_0b53b_row5_col8, #T_0b53b_row5_col9, #T_0b53b_row5_col13, #T_0b53b_row5_col14, #T_0b53b_row5_col16, #T_0b53b_row5_col17, #T_0b53b_row6_col1, #T_0b53b_row6_col2, #T_0b53b_row6_col4, #T_0b53b_row6_col6, #T_0b53b_row6_col7, #T_0b53b_row6_col9, #T_0b53b_row6_col10, #T_0b53b_row6_col11, #T_0b53b_row6_col13, #T_0b53b_row6_col14, #T_0b53b_row6_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.35</td>
<td>0.16</td>
<td>-0.14</td>
<td>0.44</td>
<td>-0.41</td>
<td>0.15</td>
<td>0.46</td>
<td>-0.33</td>
<td>0.02</td>
<td>0.13</td>
<td>-0.41</td>
<td>-0.05</td>
<td>0.46</td>
<td>-0.03</td>
<td>0.00</td>
<td>0.26</td>
<td>-0.47</td>
<td>-0.30</td>
</tr>
<tr>
<td>1</td>
<td>0.01</td>
<td>-0.42</td>
<td>-0.45</td>
<td>0.34</td>
<td>0.41</td>
<td>-0.23</td>
<td>0.35</td>
<td>-0.36</td>
<td>-0.04</td>
<td>0.06</td>
<td>0.07</td>
<td>-0.29</td>
<td>-0.28</td>
<td>0.48</td>
<td>-0.38</td>
<td>-0.06</td>
<td>-0.23</td>
<td>-0.37</td>
</tr>
<tr>
<td>2</td>
<td>0.23</td>
<td>-0.31</td>
<td>0.18</td>
<td>0.15</td>
<td>-0.45</td>
<td>0.06</td>
<td>-0.16</td>
<td>-0.11</td>
<td>0.45</td>
<td>-0.09</td>
<td>0.03</td>
<td>-0.24</td>
<td>-0.37</td>
<td>0.21</td>
<td>0.11</td>
<td>0.01</td>
<td>-0.46</td>
<td>-0.37</td>
</tr>
<tr>
<td>3</td>
<td>0.29</td>
<td>0.36</td>
<td>-0.07</td>
<td>-0.18</td>
<td>-0.46</td>
<td>-0.45</td>
<td>0.25</td>
<td>0.32</td>
<td>-0.12</td>
<td>0.22</td>
<td>-0.18</td>
<td>0.27</td>
<td>-0.18</td>
<td>-0.07</td>
<td>0.35</td>
<td>0.32</td>
<td>0.18</td>
<td>0.39</td>
</tr>
<tr>
<td>4</td>
<td>0.35</td>
<td>-0.27</td>
<td>0.13</td>
<td>-0.40</td>
<td>0.44</td>
<td>0.21</td>
<td>0.06</td>
<td>-0.31</td>
<td>-0.30</td>
<td>0.46</td>
<td>-0.44</td>
<td>-0.18</td>
<td>-0.26</td>
<td>-0.34</td>
<td>0.36</td>
<td>0.33</td>
<td>0.12</td>
<td>0.04</td>
</tr>
<tr>
<td>5</td>
<td>0.04</td>
<td>0.21</td>
<td>-0.02</td>
<td>-0.36</td>
<td>0.39</td>
<td>-0.13</td>
<td>0.30</td>
<td>0.35</td>
<td>-0.12</td>
<td>-0.43</td>
<td>0.44</td>
<td>0.32</td>
<td>0.06</td>
<td>-0.30</td>
<td>-0.29</td>
<td>0.24</td>
<td>-0.44</td>
<td>-0.13</td>
</tr>
<tr>
<td>6</td>
<td>0.38</td>
<td>-0.04</td>
<td>-0.30</td>
<td>0.17</td>
<td>-0.03</td>
<td>0.37</td>
<td>-0.03</td>
<td>-0.18</td>
<td>0.42</td>
<td>-0.39</td>
<td>-0.33</td>
<td>-0.19</td>
<td>0.02</td>
<td>-0.41</td>
<td>-0.44</td>
<td>0.42</td>
<td>0.38</td>
<td>-0.21</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_5233c_row0_col0, #T_5233c_row0_col1, #T_5233c_row0_col2, #T_5233c_row0_col3, #T_5233c_row0_col4, #T_5233c_row0_col5, #T_5233c_row0_col6, #T_5233c_row0_col7, #T_5233c_row0_col8, #T_5233c_row0_col9, #T_5233c_row0_col10, #T_5233c_row0_col11, #T_5233c_row0_col12, #T_5233c_row0_col13, #T_5233c_row0_col14, #T_5233c_row0_col15, #T_5233c_row0_col16, #T_5233c_row0_col17, #T_5233c_row1_col0, #T_5233c_row1_col1, #T_5233c_row1_col2, #T_5233c_row1_col3, #T_5233c_row1_col4, #T_5233c_row1_col5, #T_5233c_row1_col6, #T_5233c_row1_col7, #T_5233c_row1_col8, #T_5233c_row1_col9, #T_5233c_row1_col10, #T_5233c_row1_col11, #T_5233c_row1_col12, #T_5233c_row1_col13, #T_5233c_row1_col14, #T_5233c_row1_col15, #T_5233c_row1_col16, #T_5233c_row1_col17, #T_5233c_row2_col0, #T_5233c_row2_col1, #T_5233c_row2_col2, #T_5233c_row2_col3, #T_5233c_row2_col4, #T_5233c_row2_col5, #T_5233c_row2_col6, #T_5233c_row2_col7, #T_5233c_row2_col8, #T_5233c_row2_col9, #T_5233c_row2_col10, #T_5233c_row2_col11, #T_5233c_row2_col12, #T_5233c_row2_col13, #T_5233c_row2_col14, #T_5233c_row2_col15, #T_5233c_row2_col16, #T_5233c_row2_col17, #T_5233c_row3_col0, #T_5233c_row3_col1, #T_5233c_row3_col2, #T_5233c_row3_col3, #T_5233c_row3_col4, #T_5233c_row3_col5, #T_5233c_row3_col6, #T_5233c_row3_col7, #T_5233c_row3_col8, #T_5233c_row3_col9, #T_5233c_row3_col10, #T_5233c_row3_col11, #T_5233c_row3_col12, #T_5233c_row3_col13, #T_5233c_row3_col14, #T_5233c_row3_col15, #T_5233c_row3_col16, #T_5233c_row3_col17, #T_5233c_row4_col0, #T_5233c_row4_col1, #T_5233c_row4_col2, #T_5233c_row4_col3, #T_5233c_row4_col4, #T_5233c_row4_col5, #T_5233c_row4_col6, #T_5233c_row4_col7, #T_5233c_row4_col8, #T_5233c_row4_col9, #T_5233c_row4_col10, #T_5233c_row4_col11, #T_5233c_row4_col12, #T_5233c_row4_col13, #T_5233c_row4_col14, #T_5233c_row4_col15, #T_5233c_row4_col16, #T_5233c_row4_col17, #T_5233c_row5_col0, #T_5233c_row5_col1, #T_5233c_row5_col2, #T_5233c_row5_col3, #T_5233c_row5_col4, #T_5233c_row5_col5, #T_5233c_row5_col6, #T_5233c_row5_col7, #T_5233c_row5_col8, #T_5233c_row5_col9, #T_5233c_row5_col10, #T_5233c_row5_col11, #T_5233c_row5_col12, #T_5233c_row5_col13, #T_5233c_row5_col14, #T_5233c_row5_col15, #T_5233c_row5_col16, #T_5233c_row5_col17, #T_5233c_row6_col0, #T_5233c_row6_col1, #T_5233c_row6_col2, #T_5233c_row6_col3, #T_5233c_row6_col4, #T_5233c_row6_col5, #T_5233c_row6_col6, #T_5233c_row6_col7, #T_5233c_row6_col8, #T_5233c_row6_col9, #T_5233c_row6_col10, #T_5233c_row6_col11, #T_5233c_row6_col12, #T_5233c_row6_col13, #T_5233c_row6_col14, #T_5233c_row6_col15, #T_5233c_row6_col16, #T_5233c_row6_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.35</td>
<td>0.16</td>
<td>0.14</td>
<td>0.44</td>
<td>0.41</td>
<td>0.15</td>
<td>0.46</td>
<td>0.33</td>
<td>0.02</td>
<td>0.13</td>
<td>0.41</td>
<td>0.05</td>
<td>0.46</td>
<td>0.03</td>
<td>0.00</td>
<td>0.26</td>
<td>0.47</td>
<td>0.30</td>
</tr>
<tr>
<td>1</td>
<td>0.01</td>
<td>0.42</td>
<td>0.45</td>
<td>0.34</td>
<td>0.41</td>
<td>0.23</td>
<td>0.35</td>
<td>0.36</td>
<td>0.04</td>
<td>0.06</td>
<td>0.07</td>
<td>0.29</td>
<td>0.28</td>
<td>0.48</td>
<td>0.38</td>
<td>0.06</td>
<td>0.23</td>
<td>0.37</td>
</tr>
<tr>
<td>2</td>
<td>0.23</td>
<td>0.31</td>
<td>0.18</td>
<td>0.15</td>
<td>0.45</td>
<td>0.06</td>
<td>0.16</td>
<td>0.11</td>
<td>0.45</td>
<td>0.09</td>
<td>0.03</td>
<td>0.24</td>
<td>0.37</td>
<td>0.21</td>
<td>0.11</td>
<td>0.01</td>
<td>0.46</td>
<td>0.37</td>
</tr>
<tr>
<td>3</td>
<td>0.29</td>
<td>0.36</td>
<td>0.07</td>
<td>0.18</td>
<td>0.46</td>
<td>0.45</td>
<td>0.25</td>
<td>0.32</td>
<td>0.12</td>
<td>0.22</td>
<td>0.18</td>
<td>0.27</td>
<td>0.18</td>
<td>0.07</td>
<td>0.35</td>
<td>0.32</td>
<td>0.18</td>
<td>0.39</td>
</tr>
<tr>
<td>4</td>
<td>0.35</td>
<td>0.27</td>
<td>0.13</td>
<td>0.40</td>
<td>0.44</td>
<td>0.21</td>
<td>0.06</td>
<td>0.31</td>
<td>0.30</td>
<td>0.46</td>
<td>0.44</td>
<td>0.18</td>
<td>0.26</td>
<td>0.34</td>
<td>0.36</td>
<td>0.33</td>
<td>0.12</td>
<td>0.04</td>
</tr>
<tr>
<td>5</td>
<td>0.04</td>
<td>0.21</td>
<td>0.02</td>
<td>0.36</td>
<td>0.39</td>
<td>0.13</td>
<td>0.30</td>
<td>0.35</td>
<td>0.12</td>
<td>0.43</td>
<td>0.44</td>
<td>0.32</td>
<td>0.06</td>
<td>0.30</td>
<td>0.29</td>
<td>0.24</td>
<td>0.44</td>
<td>0.13</td>
</tr>
<tr>
<td>6</td>
<td>0.38</td>
<td>0.04</td>
<td>0.30</td>
<td>0.17</td>
<td>0.03</td>
<td>0.37</td>
<td>0.03</td>
<td>0.18</td>
<td>0.42</td>
<td>0.39</td>
<td>0.33</td>
<td>0.19</td>
<td>0.02</td>
<td>0.41</td>
<td>0.44</td>
<td>0.42</td>
<td>0.38</td>
<td>0.21</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="n">monotonicity_indicator</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">monotonicity_indicator</span> <span class="o">=</span> <span class="n">get_monotonicity_indicator</span><span class="p">(</span>
    <span class="n">monotonicity_indicator</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">units</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Monotocity indicator:&quot;</span><span class="p">)</span>
<span class="n">display_kernel</span><span class="p">(</span><span class="n">monotonicity_indicator</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Kernel after applying the monotocity indicator:&quot;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">replace_kernel_using_monotonicity_indicator</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">monotonicity_indicator</span><span class="p">):</span>
    <span class="n">display_kernel</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Monotocity indicator:
Kernel after applying the monotocity indicator:
</code></pre></div>
<style type="text/css">
#T_a0e52_row0_col0, #T_a0e52_row1_col0 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_a0e52_row2_col0, #T_a0e52_row3_col0 {
  background-color: #b40426;
  color: #f1f1f1;
}
#T_a0e52_row4_col0, #T_a0e52_row5_col0, #T_a0e52_row6_col0 {
  background-color: #dcdddd;
  color: #000000;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.00</td>
</tr>
<tr>
<td>1</td>
<td>1.00</td>
</tr>
<tr>
<td>2</td>
<td>-1.00</td>
</tr>
<tr>
<td>3</td>
<td>-1.00</td>
</tr>
<tr>
<td>4</td>
<td>0.00</td>
</tr>
<tr>
<td>5</td>
<td>0.00</td>
</tr>
<tr>
<td>6</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_08219_row0_col0, #T_08219_row0_col1, #T_08219_row0_col2, #T_08219_row0_col3, #T_08219_row0_col4, #T_08219_row0_col5, #T_08219_row0_col6, #T_08219_row0_col7, #T_08219_row0_col8, #T_08219_row0_col9, #T_08219_row0_col10, #T_08219_row0_col11, #T_08219_row0_col12, #T_08219_row0_col13, #T_08219_row0_col14, #T_08219_row0_col15, #T_08219_row0_col16, #T_08219_row0_col17, #T_08219_row1_col0, #T_08219_row1_col1, #T_08219_row1_col2, #T_08219_row1_col3, #T_08219_row1_col4, #T_08219_row1_col5, #T_08219_row1_col6, #T_08219_row1_col7, #T_08219_row1_col8, #T_08219_row1_col9, #T_08219_row1_col10, #T_08219_row1_col11, #T_08219_row1_col12, #T_08219_row1_col13, #T_08219_row1_col14, #T_08219_row1_col15, #T_08219_row1_col16, #T_08219_row1_col17, #T_08219_row4_col0, #T_08219_row4_col2, #T_08219_row4_col4, #T_08219_row4_col5, #T_08219_row4_col6, #T_08219_row4_col9, #T_08219_row4_col14, #T_08219_row4_col15, #T_08219_row4_col16, #T_08219_row4_col17, #T_08219_row5_col0, #T_08219_row5_col1, #T_08219_row5_col4, #T_08219_row5_col6, #T_08219_row5_col7, #T_08219_row5_col10, #T_08219_row5_col11, #T_08219_row5_col12, #T_08219_row5_col15, #T_08219_row6_col0, #T_08219_row6_col3, #T_08219_row6_col5, #T_08219_row6_col8, #T_08219_row6_col12, #T_08219_row6_col15, #T_08219_row6_col16 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_08219_row2_col0, #T_08219_row2_col1, #T_08219_row2_col2, #T_08219_row2_col3, #T_08219_row2_col4, #T_08219_row2_col5, #T_08219_row2_col6, #T_08219_row2_col7, #T_08219_row2_col8, #T_08219_row2_col9, #T_08219_row2_col10, #T_08219_row2_col11, #T_08219_row2_col12, #T_08219_row2_col13, #T_08219_row2_col14, #T_08219_row2_col15, #T_08219_row2_col16, #T_08219_row2_col17, #T_08219_row3_col0, #T_08219_row3_col1, #T_08219_row3_col2, #T_08219_row3_col3, #T_08219_row3_col4, #T_08219_row3_col5, #T_08219_row3_col6, #T_08219_row3_col7, #T_08219_row3_col8, #T_08219_row3_col9, #T_08219_row3_col10, #T_08219_row3_col11, #T_08219_row3_col12, #T_08219_row3_col13, #T_08219_row3_col14, #T_08219_row3_col15, #T_08219_row3_col16, #T_08219_row3_col17, #T_08219_row4_col1, #T_08219_row4_col3, #T_08219_row4_col7, #T_08219_row4_col8, #T_08219_row4_col10, #T_08219_row4_col11, #T_08219_row4_col12, #T_08219_row4_col13, #T_08219_row5_col2, #T_08219_row5_col3, #T_08219_row5_col5, #T_08219_row5_col8, #T_08219_row5_col9, #T_08219_row5_col13, #T_08219_row5_col14, #T_08219_row5_col16, #T_08219_row5_col17, #T_08219_row6_col1, #T_08219_row6_col2, #T_08219_row6_col4, #T_08219_row6_col6, #T_08219_row6_col7, #T_08219_row6_col9, #T_08219_row6_col10, #T_08219_row6_col11, #T_08219_row6_col13, #T_08219_row6_col14, #T_08219_row6_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.35</td>
<td>0.16</td>
<td>0.14</td>
<td>0.44</td>
<td>0.41</td>
<td>0.15</td>
<td>0.46</td>
<td>0.33</td>
<td>0.02</td>
<td>0.13</td>
<td>0.41</td>
<td>0.05</td>
<td>0.46</td>
<td>0.03</td>
<td>0.00</td>
<td>0.26</td>
<td>0.47</td>
<td>0.30</td>
</tr>
<tr>
<td>1</td>
<td>0.01</td>
<td>0.42</td>
<td>0.45</td>
<td>0.34</td>
<td>0.41</td>
<td>0.23</td>
<td>0.35</td>
<td>0.36</td>
<td>0.04</td>
<td>0.06</td>
<td>0.07</td>
<td>0.29</td>
<td>0.28</td>
<td>0.48</td>
<td>0.38</td>
<td>0.06</td>
<td>0.23</td>
<td>0.37</td>
</tr>
<tr>
<td>2</td>
<td>-0.23</td>
<td>-0.31</td>
<td>-0.18</td>
<td>-0.15</td>
<td>-0.45</td>
<td>-0.06</td>
<td>-0.16</td>
<td>-0.11</td>
<td>-0.45</td>
<td>-0.09</td>
<td>-0.03</td>
<td>-0.24</td>
<td>-0.37</td>
<td>-0.21</td>
<td>-0.11</td>
<td>-0.01</td>
<td>-0.46</td>
<td>-0.37</td>
</tr>
<tr>
<td>3</td>
<td>-0.29</td>
<td>-0.36</td>
<td>-0.07</td>
<td>-0.18</td>
<td>-0.46</td>
<td>-0.45</td>
<td>-0.25</td>
<td>-0.32</td>
<td>-0.12</td>
<td>-0.22</td>
<td>-0.18</td>
<td>-0.27</td>
<td>-0.18</td>
<td>-0.07</td>
<td>-0.35</td>
<td>-0.32</td>
<td>-0.18</td>
<td>-0.39</td>
</tr>
<tr>
<td>4</td>
<td>0.35</td>
<td>-0.27</td>
<td>0.13</td>
<td>-0.40</td>
<td>0.44</td>
<td>0.21</td>
<td>0.06</td>
<td>-0.31</td>
<td>-0.30</td>
<td>0.46</td>
<td>-0.44</td>
<td>-0.18</td>
<td>-0.26</td>
<td>-0.34</td>
<td>0.36</td>
<td>0.33</td>
<td>0.12</td>
<td>0.04</td>
</tr>
<tr>
<td>5</td>
<td>0.04</td>
<td>0.21</td>
<td>-0.02</td>
<td>-0.36</td>
<td>0.39</td>
<td>-0.13</td>
<td>0.30</td>
<td>0.35</td>
<td>-0.12</td>
<td>-0.43</td>
<td>0.44</td>
<td>0.32</td>
<td>0.06</td>
<td>-0.30</td>
<td>-0.29</td>
<td>0.24</td>
<td>-0.44</td>
<td>-0.13</td>
</tr>
<tr>
<td>6</td>
<td>0.38</td>
<td>-0.04</td>
<td>-0.30</td>
<td>0.17</td>
<td>-0.03</td>
<td>0.37</td>
<td>-0.03</td>
<td>-0.18</td>
<td>0.42</td>
<td>-0.39</td>
<td>-0.33</td>
<td>-0.19</td>
<td>0.02</td>
<td>-0.41</td>
<td>-0.44</td>
<td>0.42</td>
<td>0.38</td>
<td>-0.21</td>
</tr>
</tbody>
</table>
<h3 id="monotonic-dense-layer_2">Monotonic Dense Layer<a class="headerlink" href="#monotonic-dense-layer_2" title="Permanent link">¤</a></h3>
<p>This is an implementation of our Monotonic Dense Unit or Constrained
Monotone Fully Connected Layer. The below is the figure from the paper
for reference.</p>
<p>In the code, the variable <code>monotonicity_indicator</code> corresponds to <strong>t</strong>
in the figure and the variable <code>activation_selector</code> corresponds to
<strong>s</strong>.</p>
<p>Parameters <code>convexity_indicator</code> and <code>epsilon</code> are used to calculate
<code>activation_selector</code> as follows: - if <code>convexity_indicator</code> is -1 or 1,
then <code>activation_selector</code> will have all elements 0 or 1, respecively. -
if <code>convexity_indicator</code> is <code>None</code>, then <code>epsilon</code> must have a value
between 0 and 1 and corresponds to the percentage of elements of
<code>activation_selector</code> set to 1.</p>
<p><img alt="mono-dense-layer-diagram.png" src="../images/nbs/images/mono-dense-layer-diagram.png" /></p>
<hr />
<h3 id="monodense">MonoDense<a class="headerlink" href="#monodense" title="Permanent link">¤</a></h3>
<blockquote>
<div class="highlight"><pre><span></span><code> MonoDense (units:int, activation:Union[str,Callable[[Union[tensorflow.pyt
            hon.types.core.Tensor,tensorflow.python.types.core.TensorProto
            col,int,float,bool,str,bytes,complex,tuple,list,numpy.ndarray,
            numpy.generic]],Union[tensorflow.python.types.core.Tensor,tens
            orflow.python.types.core.TensorProtocol,int,float,bool,str,byt
            es,complex,tuple,list,numpy.ndarray,numpy.generic]],NoneType]=
            None, monotonicity_indicator:Union[numpy.__array_like._Support
            sArray[numpy.dtype],numpy.__nested_sequence._NestedSequence[nu
            mpy.__array_like._SupportsArray[numpy.dtype]],bool,int,float,c
            omplex,str,bytes,numpy.__nested_sequence._NestedSequence[Union
            [bool,int,float,complex,str,bytes]]]=1, is_convex:bool=False,
            is_concave:bool=False,
            activation_weights:Tuple[float,float,float]=(7.0, 7.0, 2.0),
            **kwargs:Any)
</code></pre></div>
</blockquote>
<p>Monotonic counterpart of the regular Dense Layer of tf.keras</p>
<p>This is an implementation of our Monotonic Dense Unit or Constrained
Monotone Fully Connected Layer. The below is the figure from the paper
for reference.</p>
<ul>
<li>
<p>the parameter <code>monotonicity_indicator</code> corresponds to <strong>t</strong> in the
  figure below, and</p>
</li>
<li>
<p>parameters <code>is_convex</code>, <code>is_concave</code> and <code>activation_weights</code> are used
  to calculate the activation selector <strong>s</strong> as follows:</p>
</li>
<li>
<p>if <code>is_convex</code> or <code>is_concave</code> is <strong>True</strong>, then the activation
    selector <strong>s</strong> will be (<code>units</code>, 0, 0) and (0, <code>units</code>, 0),
    respecively.</p>
</li>
<li>
<p>if both <code>is_convex</code> or <code>is_concave</code> is <strong>False</strong>, then the
    <code>activation_weights</code> represent ratios between <span class="arithmatex">\(\breve{s}\)</span>, <span class="arithmatex">\(\hat{s}\)</span>
    and <span class="arithmatex">\(\tilde{s}\)</span>, respecively. E.g. if
    <code>activation_weights = (2, 2, 1)</code> and <code>units = 10</code>, then</p>
</li>
</ul>
<div class="arithmatex">\[
(\breve{s}, \hat{s}, \tilde{s}) = (4, 4, 2)
\]</div>
<p><img alt="mono-dense-layer-diagram.png" src="../../../../../images/nbs/images/nbs/images/mono-dense-layer-diagram.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">units</span> <span class="o">=</span> <span class="mi">18</span>
<span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">x_len</span> <span class="o">=</span> <span class="mi">11</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">x_len</span><span class="p">))</span>

<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">set_random_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="k">for</span> <span class="n">monotonicity_indicator</span> <span class="ow">in</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
    <span class="mi">1</span><span class="p">,</span>
    <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x_len</span><span class="p">,)),</span>
    <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">x_len</span><span class="p">,)),</span>
<span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;*&quot;</span> <span class="o">*</span> <span class="mi">120</span><span class="p">)</span>
    <span class="n">mono_layer</span> <span class="o">=</span> <span class="n">MonoDense</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="n">units</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
        <span class="n">monotonicity_indicator</span><span class="o">=</span><span class="n">monotonicity_indicator</span><span class="p">,</span>
        <span class="n">activation_weights</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;input:&quot;</span><span class="p">)</span>
    <span class="n">display_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">mono_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;monotonicity_indicator = </span><span class="si">{</span><span class="n">monotonicity_indicator</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">display_kernel</span><span class="p">(</span><span class="n">mono_layer</span><span class="o">.</span><span class="n">monotonicity_indicator</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;kernel:&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">replace_kernel_using_monotonicity_indicator</span><span class="p">(</span>
        <span class="n">mono_layer</span><span class="p">,</span> <span class="n">mono_layer</span><span class="o">.</span><span class="n">monotonicity_indicator</span>
    <span class="p">):</span>
        <span class="n">display_kernel</span><span class="p">(</span><span class="n">mono_layer</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output:&quot;</span><span class="p">)</span>
    <span class="n">display_kernel</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ok&quot;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>************************************************************************************************************************
input:
WARNING:tensorflow:5 out of the last 5 calls to &lt;function apply_activations&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
monotonicity_indicator = [1, 1, 1, 1, 0, 0, 0, 0, -1, -1, -1]
kernel:
output:
************************************************************************************************************************
input:
monotonicity_indicator = 1
kernel:
output:
************************************************************************************************************************
input:
monotonicity_indicator = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
kernel:
output:
************************************************************************************************************************
input:
monotonicity_indicator = -1
kernel:
output:
************************************************************************************************************************
input:
monotonicity_indicator = [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]
kernel:
output:
ok
</code></pre></div>
<style type="text/css">
#T_7b780_row0_col0, #T_7b780_row0_col2, #T_7b780_row0_col3, #T_7b780_row0_col6, #T_7b780_row0_col10, #T_7b780_row1_col0, #T_7b780_row1_col1, #T_7b780_row1_col2, #T_7b780_row1_col3, #T_7b780_row1_col5, #T_7b780_row1_col7, #T_7b780_row2_col0, #T_7b780_row2_col4, #T_7b780_row2_col5, #T_7b780_row2_col6, #T_7b780_row2_col7, #T_7b780_row2_col8, #T_7b780_row3_col1, #T_7b780_row3_col2, #T_7b780_row3_col6, #T_7b780_row3_col7, #T_7b780_row3_col8, #T_7b780_row3_col10, #T_7b780_row4_col0, #T_7b780_row4_col1, #T_7b780_row4_col2, #T_7b780_row4_col3, #T_7b780_row4_col4, #T_7b780_row4_col5, #T_7b780_row4_col6, #T_7b780_row4_col7, #T_7b780_row5_col2, #T_7b780_row5_col4, #T_7b780_row5_col7, #T_7b780_row5_col8, #T_7b780_row5_col9, #T_7b780_row5_col10, #T_7b780_row6_col2, #T_7b780_row6_col7, #T_7b780_row6_col8, #T_7b780_row6_col9, #T_7b780_row7_col0, #T_7b780_row7_col1, #T_7b780_row7_col3, #T_7b780_row7_col8, #T_7b780_row7_col10, #T_7b780_row8_col0, #T_7b780_row8_col1, #T_7b780_row8_col2, #T_7b780_row8_col10 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_7b780_row0_col1, #T_7b780_row0_col4, #T_7b780_row0_col5, #T_7b780_row0_col7, #T_7b780_row0_col8, #T_7b780_row0_col9, #T_7b780_row1_col4, #T_7b780_row1_col6, #T_7b780_row1_col8, #T_7b780_row1_col9, #T_7b780_row1_col10, #T_7b780_row2_col1, #T_7b780_row2_col2, #T_7b780_row2_col3, #T_7b780_row2_col9, #T_7b780_row2_col10, #T_7b780_row3_col0, #T_7b780_row3_col3, #T_7b780_row3_col4, #T_7b780_row3_col5, #T_7b780_row3_col9, #T_7b780_row4_col8, #T_7b780_row4_col9, #T_7b780_row4_col10, #T_7b780_row5_col0, #T_7b780_row5_col1, #T_7b780_row5_col3, #T_7b780_row5_col5, #T_7b780_row5_col6, #T_7b780_row6_col0, #T_7b780_row6_col1, #T_7b780_row6_col3, #T_7b780_row6_col4, #T_7b780_row6_col5, #T_7b780_row6_col6, #T_7b780_row6_col10, #T_7b780_row7_col2, #T_7b780_row7_col4, #T_7b780_row7_col5, #T_7b780_row7_col6, #T_7b780_row7_col7, #T_7b780_row7_col9, #T_7b780_row8_col3, #T_7b780_row8_col4, #T_7b780_row8_col5, #T_7b780_row8_col6, #T_7b780_row8_col7, #T_7b780_row8_col8, #T_7b780_row8_col9 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.30</td>
<td>-1.04</td>
<td>0.75</td>
<td>0.94</td>
<td>-1.95</td>
<td>-1.30</td>
<td>0.13</td>
<td>-0.32</td>
<td>-0.02</td>
<td>-0.85</td>
<td>0.88</td>
</tr>
<tr>
<td>1</td>
<td>0.78</td>
<td>0.07</td>
<td>1.13</td>
<td>0.47</td>
<td>-0.86</td>
<td>0.37</td>
<td>-0.96</td>
<td>0.88</td>
<td>-0.05</td>
<td>-0.18</td>
<td>-0.68</td>
</tr>
<tr>
<td>2</td>
<td>1.22</td>
<td>-0.15</td>
<td>-0.43</td>
<td>-0.35</td>
<td>0.53</td>
<td>0.37</td>
<td>0.41</td>
<td>0.43</td>
<td>2.14</td>
<td>-0.41</td>
<td>-0.51</td>
</tr>
<tr>
<td>3</td>
<td>-0.81</td>
<td>0.62</td>
<td>1.13</td>
<td>-0.11</td>
<td>-0.84</td>
<td>-0.82</td>
<td>0.65</td>
<td>0.74</td>
<td>0.54</td>
<td>-0.67</td>
<td>0.23</td>
</tr>
<tr>
<td>4</td>
<td>0.12</td>
<td>0.22</td>
<td>0.87</td>
<td>0.22</td>
<td>0.68</td>
<td>0.07</td>
<td>0.29</td>
<td>0.63</td>
<td>-1.46</td>
<td>-0.32</td>
<td>-0.47</td>
</tr>
<tr>
<td>5</td>
<td>-0.64</td>
<td>-0.28</td>
<td>1.49</td>
<td>-0.87</td>
<td>0.97</td>
<td>-1.68</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.59</td>
<td>0.71</td>
<td>0.79</td>
</tr>
<tr>
<td>6</td>
<td>-0.35</td>
<td>-0.46</td>
<td>0.86</td>
<td>-0.19</td>
<td>-1.28</td>
<td>-1.13</td>
<td>-0.92</td>
<td>0.50</td>
<td>0.14</td>
<td>0.69</td>
<td>-0.43</td>
</tr>
<tr>
<td>7</td>
<td>0.16</td>
<td>0.63</td>
<td>-0.31</td>
<td>0.46</td>
<td>-0.66</td>
<td>-0.36</td>
<td>-0.38</td>
<td>-1.20</td>
<td>0.49</td>
<td>-0.47</td>
<td>0.01</td>
</tr>
<tr>
<td>8</td>
<td>0.48</td>
<td>0.45</td>
<td>0.67</td>
<td>-0.10</td>
<td>-0.42</td>
<td>-0.08</td>
<td>-1.69</td>
<td>-1.45</td>
<td>-1.32</td>
<td>-1.00</td>
<td>0.40</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_4a600_row0_col0, #T_4a600_row1_col0, #T_4a600_row2_col0, #T_4a600_row3_col0 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_4a600_row4_col0, #T_4a600_row5_col0, #T_4a600_row6_col0, #T_4a600_row7_col0 {
  background-color: #dcdddd;
  color: #000000;
}
#T_4a600_row8_col0, #T_4a600_row9_col0, #T_4a600_row10_col0 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.00</td>
</tr>
<tr>
<td>1</td>
<td>1.00</td>
</tr>
<tr>
<td>2</td>
<td>1.00</td>
</tr>
<tr>
<td>3</td>
<td>1.00</td>
</tr>
<tr>
<td>4</td>
<td>0.00</td>
</tr>
<tr>
<td>5</td>
<td>0.00</td>
</tr>
<tr>
<td>6</td>
<td>0.00</td>
</tr>
<tr>
<td>7</td>
<td>0.00</td>
</tr>
<tr>
<td>8</td>
<td>-1.00</td>
</tr>
<tr>
<td>9</td>
<td>-1.00</td>
</tr>
<tr>
<td>10</td>
<td>-1.00</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_f83c8_row0_col0, #T_f83c8_row0_col1, #T_f83c8_row0_col2, #T_f83c8_row0_col3, #T_f83c8_row0_col4, #T_f83c8_row0_col5, #T_f83c8_row0_col6, #T_f83c8_row0_col7, #T_f83c8_row0_col8, #T_f83c8_row0_col9, #T_f83c8_row0_col10, #T_f83c8_row0_col11, #T_f83c8_row0_col12, #T_f83c8_row0_col13, #T_f83c8_row0_col14, #T_f83c8_row0_col15, #T_f83c8_row0_col16, #T_f83c8_row0_col17, #T_f83c8_row1_col0, #T_f83c8_row1_col1, #T_f83c8_row1_col2, #T_f83c8_row1_col3, #T_f83c8_row1_col4, #T_f83c8_row1_col5, #T_f83c8_row1_col6, #T_f83c8_row1_col7, #T_f83c8_row1_col8, #T_f83c8_row1_col9, #T_f83c8_row1_col10, #T_f83c8_row1_col11, #T_f83c8_row1_col12, #T_f83c8_row1_col13, #T_f83c8_row1_col14, #T_f83c8_row1_col15, #T_f83c8_row1_col16, #T_f83c8_row1_col17, #T_f83c8_row2_col0, #T_f83c8_row2_col1, #T_f83c8_row2_col2, #T_f83c8_row2_col3, #T_f83c8_row2_col4, #T_f83c8_row2_col5, #T_f83c8_row2_col6, #T_f83c8_row2_col7, #T_f83c8_row2_col8, #T_f83c8_row2_col9, #T_f83c8_row2_col10, #T_f83c8_row2_col11, #T_f83c8_row2_col12, #T_f83c8_row2_col13, #T_f83c8_row2_col14, #T_f83c8_row2_col15, #T_f83c8_row2_col16, #T_f83c8_row2_col17, #T_f83c8_row3_col0, #T_f83c8_row3_col1, #T_f83c8_row3_col2, #T_f83c8_row3_col3, #T_f83c8_row3_col4, #T_f83c8_row3_col5, #T_f83c8_row3_col6, #T_f83c8_row3_col7, #T_f83c8_row3_col8, #T_f83c8_row3_col9, #T_f83c8_row3_col10, #T_f83c8_row3_col11, #T_f83c8_row3_col12, #T_f83c8_row3_col13, #T_f83c8_row3_col14, #T_f83c8_row3_col15, #T_f83c8_row3_col16, #T_f83c8_row3_col17, #T_f83c8_row4_col0, #T_f83c8_row4_col2, #T_f83c8_row4_col4, #T_f83c8_row4_col5, #T_f83c8_row4_col6, #T_f83c8_row4_col9, #T_f83c8_row4_col14, #T_f83c8_row4_col15, #T_f83c8_row4_col16, #T_f83c8_row4_col17, #T_f83c8_row5_col0, #T_f83c8_row5_col1, #T_f83c8_row5_col4, #T_f83c8_row5_col6, #T_f83c8_row5_col7, #T_f83c8_row5_col10, #T_f83c8_row5_col11, #T_f83c8_row5_col12, #T_f83c8_row5_col15, #T_f83c8_row6_col0, #T_f83c8_row6_col3, #T_f83c8_row6_col5, #T_f83c8_row6_col8, #T_f83c8_row6_col12, #T_f83c8_row6_col15, #T_f83c8_row6_col16, #T_f83c8_row7_col0, #T_f83c8_row7_col2, #T_f83c8_row7_col4, #T_f83c8_row7_col6, #T_f83c8_row7_col7, #T_f83c8_row7_col10, #T_f83c8_row7_col11, #T_f83c8_row7_col12, #T_f83c8_row7_col16, #T_f83c8_row7_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_f83c8_row4_col1, #T_f83c8_row4_col3, #T_f83c8_row4_col7, #T_f83c8_row4_col8, #T_f83c8_row4_col10, #T_f83c8_row4_col11, #T_f83c8_row4_col12, #T_f83c8_row4_col13, #T_f83c8_row5_col2, #T_f83c8_row5_col3, #T_f83c8_row5_col5, #T_f83c8_row5_col8, #T_f83c8_row5_col9, #T_f83c8_row5_col13, #T_f83c8_row5_col14, #T_f83c8_row5_col16, #T_f83c8_row5_col17, #T_f83c8_row6_col1, #T_f83c8_row6_col2, #T_f83c8_row6_col4, #T_f83c8_row6_col6, #T_f83c8_row6_col7, #T_f83c8_row6_col9, #T_f83c8_row6_col10, #T_f83c8_row6_col11, #T_f83c8_row6_col13, #T_f83c8_row6_col14, #T_f83c8_row6_col17, #T_f83c8_row7_col1, #T_f83c8_row7_col3, #T_f83c8_row7_col5, #T_f83c8_row7_col8, #T_f83c8_row7_col9, #T_f83c8_row7_col13, #T_f83c8_row7_col14, #T_f83c8_row7_col15, #T_f83c8_row8_col0, #T_f83c8_row8_col1, #T_f83c8_row8_col2, #T_f83c8_row8_col3, #T_f83c8_row8_col4, #T_f83c8_row8_col5, #T_f83c8_row8_col6, #T_f83c8_row8_col7, #T_f83c8_row8_col8, #T_f83c8_row8_col9, #T_f83c8_row8_col10, #T_f83c8_row8_col11, #T_f83c8_row8_col12, #T_f83c8_row8_col13, #T_f83c8_row8_col14, #T_f83c8_row8_col15, #T_f83c8_row8_col16, #T_f83c8_row8_col17, #T_f83c8_row9_col0, #T_f83c8_row9_col1, #T_f83c8_row9_col2, #T_f83c8_row9_col3, #T_f83c8_row9_col4, #T_f83c8_row9_col5, #T_f83c8_row9_col6, #T_f83c8_row9_col7, #T_f83c8_row9_col8, #T_f83c8_row9_col9, #T_f83c8_row9_col10, #T_f83c8_row9_col11, #T_f83c8_row9_col12, #T_f83c8_row9_col13, #T_f83c8_row9_col14, #T_f83c8_row9_col15, #T_f83c8_row9_col16, #T_f83c8_row9_col17, #T_f83c8_row10_col0, #T_f83c8_row10_col1, #T_f83c8_row10_col2, #T_f83c8_row10_col3, #T_f83c8_row10_col4, #T_f83c8_row10_col5, #T_f83c8_row10_col6, #T_f83c8_row10_col7, #T_f83c8_row10_col8, #T_f83c8_row10_col9, #T_f83c8_row10_col10, #T_f83c8_row10_col11, #T_f83c8_row10_col12, #T_f83c8_row10_col13, #T_f83c8_row10_col14, #T_f83c8_row10_col15, #T_f83c8_row10_col16, #T_f83c8_row10_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.33</td>
<td>0.15</td>
<td>0.13</td>
<td>0.41</td>
<td>0.38</td>
<td>0.14</td>
<td>0.43</td>
<td>0.30</td>
<td>0.02</td>
<td>0.12</td>
<td>0.38</td>
<td>0.05</td>
<td>0.42</td>
<td>0.03</td>
<td>0.00</td>
<td>0.24</td>
<td>0.44</td>
<td>0.28</td>
</tr>
<tr>
<td>1</td>
<td>0.01</td>
<td>0.39</td>
<td>0.42</td>
<td>0.32</td>
<td>0.38</td>
<td>0.22</td>
<td>0.33</td>
<td>0.34</td>
<td>0.03</td>
<td>0.06</td>
<td>0.06</td>
<td>0.27</td>
<td>0.26</td>
<td>0.45</td>
<td>0.35</td>
<td>0.05</td>
<td>0.21</td>
<td>0.34</td>
</tr>
<tr>
<td>2</td>
<td>0.21</td>
<td>0.29</td>
<td>0.16</td>
<td>0.14</td>
<td>0.42</td>
<td>0.06</td>
<td>0.15</td>
<td>0.10</td>
<td>0.41</td>
<td>0.08</td>
<td>0.03</td>
<td>0.22</td>
<td>0.34</td>
<td>0.20</td>
<td>0.11</td>
<td>0.01</td>
<td>0.43</td>
<td>0.35</td>
</tr>
<tr>
<td>3</td>
<td>0.27</td>
<td>0.33</td>
<td>0.06</td>
<td>0.17</td>
<td>0.42</td>
<td>0.42</td>
<td>0.24</td>
<td>0.30</td>
<td>0.11</td>
<td>0.20</td>
<td>0.17</td>
<td>0.25</td>
<td>0.17</td>
<td>0.07</td>
<td>0.32</td>
<td>0.30</td>
<td>0.17</td>
<td>0.36</td>
</tr>
<tr>
<td>4</td>
<td>0.32</td>
<td>-0.25</td>
<td>0.12</td>
<td>-0.37</td>
<td>0.41</td>
<td>0.20</td>
<td>0.06</td>
<td>-0.28</td>
<td>-0.27</td>
<td>0.43</td>
<td>-0.41</td>
<td>-0.17</td>
<td>-0.24</td>
<td>-0.31</td>
<td>0.33</td>
<td>0.31</td>
<td>0.11</td>
<td>0.03</td>
</tr>
<tr>
<td>5</td>
<td>0.04</td>
<td>0.19</td>
<td>-0.02</td>
<td>-0.34</td>
<td>0.36</td>
<td>-0.12</td>
<td>0.28</td>
<td>0.32</td>
<td>-0.11</td>
<td>-0.40</td>
<td>0.41</td>
<td>0.30</td>
<td>0.06</td>
<td>-0.28</td>
<td>-0.27</td>
<td>0.23</td>
<td>-0.41</td>
<td>-0.12</td>
</tr>
<tr>
<td>6</td>
<td>0.35</td>
<td>-0.04</td>
<td>-0.28</td>
<td>0.16</td>
<td>-0.03</td>
<td>0.35</td>
<td>-0.03</td>
<td>-0.16</td>
<td>0.39</td>
<td>-0.36</td>
<td>-0.31</td>
<td>-0.18</td>
<td>0.02</td>
<td>-0.38</td>
<td>-0.40</td>
<td>0.39</td>
<td>0.35</td>
<td>-0.19</td>
</tr>
<tr>
<td>7</td>
<td>0.33</td>
<td>-0.34</td>
<td>0.11</td>
<td>-0.29</td>
<td>0.25</td>
<td>-0.21</td>
<td>0.11</td>
<td>0.08</td>
<td>-0.19</td>
<td>-0.39</td>
<td>0.01</td>
<td>0.10</td>
<td>0.39</td>
<td>-0.25</td>
<td>-0.37</td>
<td>-0.27</td>
<td>0.04</td>
<td>0.34</td>
</tr>
<tr>
<td>8</td>
<td>-0.27</td>
<td>-0.09</td>
<td>-0.02</td>
<td>-0.45</td>
<td>-0.16</td>
<td>-0.12</td>
<td>-0.09</td>
<td>-0.43</td>
<td>-0.36</td>
<td>-0.09</td>
<td>-0.23</td>
<td>-0.42</td>
<td>-0.28</td>
<td>-0.24</td>
<td>-0.30</td>
<td>-0.31</td>
<td>-0.07</td>
<td>-0.07</td>
</tr>
<tr>
<td>9</td>
<td>-0.38</td>
<td>-0.34</td>
<td>-0.44</td>
<td>-0.42</td>
<td>-0.32</td>
<td>-0.06</td>
<td>-0.27</td>
<td>-0.28</td>
<td>-0.22</td>
<td>-0.05</td>
<td>-0.08</td>
<td>-0.07</td>
<td>-0.21</td>
<td>-0.39</td>
<td>-0.01</td>
<td>-0.26</td>
<td>-0.24</td>
<td>-0.42</td>
</tr>
<tr>
<td>10</td>
<td>-0.09</td>
<td>-0.45</td>
<td>-0.41</td>
<td>-0.36</td>
<td>-0.19</td>
<td>-0.09</td>
<td>-0.00</td>
<td>-0.34</td>
<td>-0.17</td>
<td>-0.18</td>
<td>-0.05</td>
<td>-0.39</td>
<td>-0.06</td>
<td>-0.20</td>
<td>-0.40</td>
<td>-0.33</td>
<td>-0.18</td>
<td>-0.01</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_993f6_row0_col0, #T_993f6_row0_col1, #T_993f6_row0_col3, #T_993f6_row0_col5, #T_993f6_row0_col16, #T_993f6_row0_col17, #T_993f6_row1_col0, #T_993f6_row1_col1, #T_993f6_row1_col2, #T_993f6_row1_col3, #T_993f6_row1_col4, #T_993f6_row1_col6, #T_993f6_row1_col14, #T_993f6_row1_col16, #T_993f6_row1_col17, #T_993f6_row2_col0, #T_993f6_row2_col2, #T_993f6_row2_col4, #T_993f6_row2_col6, #T_993f6_row2_col15, #T_993f6_row2_col16, #T_993f6_row2_col17, #T_993f6_row3_col0, #T_993f6_row3_col1, #T_993f6_row3_col2, #T_993f6_row3_col3, #T_993f6_row3_col16, #T_993f6_row3_col17, #T_993f6_row4_col0, #T_993f6_row4_col1, #T_993f6_row4_col2, #T_993f6_row4_col3, #T_993f6_row4_col4, #T_993f6_row4_col5, #T_993f6_row4_col6, #T_993f6_row4_col14, #T_993f6_row4_col15, #T_993f6_row4_col16, #T_993f6_row4_col17, #T_993f6_row5_col14, #T_993f6_row5_col16, #T_993f6_row6_col3, #T_993f6_row6_col14, #T_993f6_row6_col16, #T_993f6_row6_col17, #T_993f6_row7_col1, #T_993f6_row7_col2, #T_993f6_row7_col3, #T_993f6_row7_col5, #T_993f6_row7_col6, #T_993f6_row7_col14, #T_993f6_row7_col15, #T_993f6_row7_col16, #T_993f6_row7_col17, #T_993f6_row8_col1, #T_993f6_row8_col2, #T_993f6_row8_col3, #T_993f6_row8_col4, #T_993f6_row8_col6, #T_993f6_row8_col14, #T_993f6_row8_col15, #T_993f6_row8_col16, #T_993f6_row8_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_993f6_row0_col2, #T_993f6_row0_col4, #T_993f6_row0_col6, #T_993f6_row0_col7, #T_993f6_row0_col8, #T_993f6_row0_col10, #T_993f6_row0_col12, #T_993f6_row0_col13, #T_993f6_row1_col5, #T_993f6_row1_col7, #T_993f6_row1_col8, #T_993f6_row1_col10, #T_993f6_row1_col11, #T_993f6_row1_col12, #T_993f6_row1_col13, #T_993f6_row2_col1, #T_993f6_row2_col3, #T_993f6_row2_col5, #T_993f6_row3_col4, #T_993f6_row3_col5, #T_993f6_row3_col6, #T_993f6_row3_col8, #T_993f6_row3_col12, #T_993f6_row3_col13, #T_993f6_row4_col7, #T_993f6_row4_col8, #T_993f6_row4_col9, #T_993f6_row4_col10, #T_993f6_row4_col11, #T_993f6_row4_col12, #T_993f6_row4_col13, #T_993f6_row5_col0, #T_993f6_row5_col1, #T_993f6_row5_col2, #T_993f6_row5_col3, #T_993f6_row5_col4, #T_993f6_row5_col5, #T_993f6_row5_col6, #T_993f6_row5_col9, #T_993f6_row6_col0, #T_993f6_row6_col1, #T_993f6_row6_col2, #T_993f6_row6_col4, #T_993f6_row6_col5, #T_993f6_row6_col6, #T_993f6_row6_col8, #T_993f6_row6_col9, #T_993f6_row6_col10, #T_993f6_row6_col11, #T_993f6_row6_col12, #T_993f6_row6_col13, #T_993f6_row7_col0, #T_993f6_row7_col4, #T_993f6_row7_col7, #T_993f6_row7_col8, #T_993f6_row7_col9, #T_993f6_row7_col10, #T_993f6_row7_col13, #T_993f6_row8_col0, #T_993f6_row8_col5, #T_993f6_row8_col7, #T_993f6_row8_col8, #T_993f6_row8_col9, #T_993f6_row8_col10, #T_993f6_row8_col11, #T_993f6_row8_col12, #T_993f6_row8_col13 {
  background-color: #dcdddd;
  color: #000000;
}
#T_993f6_row0_col9, #T_993f6_row0_col11, #T_993f6_row0_col14, #T_993f6_row0_col15, #T_993f6_row1_col9, #T_993f6_row1_col15, #T_993f6_row2_col7, #T_993f6_row2_col8, #T_993f6_row2_col9, #T_993f6_row2_col10, #T_993f6_row2_col11, #T_993f6_row2_col12, #T_993f6_row2_col13, #T_993f6_row2_col14, #T_993f6_row3_col7, #T_993f6_row3_col9, #T_993f6_row3_col10, #T_993f6_row3_col11, #T_993f6_row3_col14, #T_993f6_row3_col15, #T_993f6_row5_col7, #T_993f6_row5_col8, #T_993f6_row5_col10, #T_993f6_row5_col11, #T_993f6_row5_col12, #T_993f6_row5_col13, #T_993f6_row5_col15, #T_993f6_row5_col17, #T_993f6_row6_col7, #T_993f6_row6_col15, #T_993f6_row7_col11, #T_993f6_row7_col12 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.01</td>
<td>0.40</td>
<td>0.00</td>
<td>1.38</td>
<td>0.00</td>
<td>0.10</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.13</td>
<td>-0.00</td>
<td>-0.26</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.55</td>
<td>-0.52</td>
<td>0.79</td>
<td>0.64</td>
</tr>
<tr>
<td>1</td>
<td>0.45</td>
<td>1.02</td>
<td>0.96</td>
<td>0.71</td>
<td>1.22</td>
<td>0.00</td>
<td>0.86</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.09</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.26</td>
<td>-0.17</td>
<td>0.54</td>
<td>1.00</td>
</tr>
<tr>
<td>2</td>
<td>0.30</td>
<td>0.00</td>
<td>0.33</td>
<td>0.00</td>
<td>0.41</td>
<td>0.00</td>
<td>0.42</td>
<td>-0.53</td>
<td>-0.89</td>
<td>-0.29</td>
<td>-0.23</td>
<td>-0.84</td>
<td>-0.16</td>
<td>-0.93</td>
<td>-0.90</td>
<td>0.08</td>
<td>0.37</td>
<td>0.08</td>
</tr>
<tr>
<td>3</td>
<td>0.21</td>
<td>0.26</td>
<td>0.33</td>
<td>0.42</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.16</td>
<td>-0.00</td>
<td>-0.61</td>
<td>-0.53</td>
<td>-0.07</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.55</td>
<td>-0.66</td>
<td>0.83</td>
<td>0.78</td>
</tr>
<tr>
<td>4</td>
<td>1.38</td>
<td>0.49</td>
<td>0.70</td>
<td>0.82</td>
<td>1.47</td>
<td>0.54</td>
<td>0.63</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.73</td>
<td>0.97</td>
<td>0.94</td>
<td>0.91</td>
</tr>
<tr>
<td>5</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-1.86</td>
<td>-0.25</td>
<td>-0.00</td>
<td>-1.57</td>
<td>-1.19</td>
<td>-0.61</td>
<td>-0.23</td>
<td>0.13</td>
<td>-1.00</td>
<td>0.50</td>
<td>-0.06</td>
</tr>
<tr>
<td>6</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.17</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.15</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.06</td>
<td>-1.00</td>
<td>0.00</td>
<td>0.12</td>
</tr>
<tr>
<td>7</td>
<td>0.00</td>
<td>0.96</td>
<td>0.35</td>
<td>0.93</td>
<td>0.00</td>
<td>0.32</td>
<td>0.17</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.17</td>
<td>-0.00</td>
<td>0.67</td>
<td>0.06</td>
<td>0.12</td>
<td>0.17</td>
</tr>
<tr>
<td>8</td>
<td>0.00</td>
<td>1.33</td>
<td>0.92</td>
<td>1.63</td>
<td>0.52</td>
<td>0.00</td>
<td>0.66</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>1.00</td>
<td>0.23</td>
<td>0.18</td>
<td>0.81</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_27b6a_row0_col0, #T_27b6a_row0_col2, #T_27b6a_row0_col3, #T_27b6a_row0_col6, #T_27b6a_row0_col10, #T_27b6a_row1_col0, #T_27b6a_row1_col1, #T_27b6a_row1_col2, #T_27b6a_row1_col3, #T_27b6a_row1_col5, #T_27b6a_row1_col7, #T_27b6a_row2_col0, #T_27b6a_row2_col4, #T_27b6a_row2_col5, #T_27b6a_row2_col6, #T_27b6a_row2_col7, #T_27b6a_row2_col8, #T_27b6a_row3_col1, #T_27b6a_row3_col2, #T_27b6a_row3_col6, #T_27b6a_row3_col7, #T_27b6a_row3_col8, #T_27b6a_row3_col10, #T_27b6a_row4_col0, #T_27b6a_row4_col1, #T_27b6a_row4_col2, #T_27b6a_row4_col3, #T_27b6a_row4_col4, #T_27b6a_row4_col5, #T_27b6a_row4_col6, #T_27b6a_row4_col7, #T_27b6a_row5_col2, #T_27b6a_row5_col4, #T_27b6a_row5_col7, #T_27b6a_row5_col8, #T_27b6a_row5_col9, #T_27b6a_row5_col10, #T_27b6a_row6_col2, #T_27b6a_row6_col7, #T_27b6a_row6_col8, #T_27b6a_row6_col9, #T_27b6a_row7_col0, #T_27b6a_row7_col1, #T_27b6a_row7_col3, #T_27b6a_row7_col8, #T_27b6a_row7_col10, #T_27b6a_row8_col0, #T_27b6a_row8_col1, #T_27b6a_row8_col2, #T_27b6a_row8_col10 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_27b6a_row0_col1, #T_27b6a_row0_col4, #T_27b6a_row0_col5, #T_27b6a_row0_col7, #T_27b6a_row0_col8, #T_27b6a_row0_col9, #T_27b6a_row1_col4, #T_27b6a_row1_col6, #T_27b6a_row1_col8, #T_27b6a_row1_col9, #T_27b6a_row1_col10, #T_27b6a_row2_col1, #T_27b6a_row2_col2, #T_27b6a_row2_col3, #T_27b6a_row2_col9, #T_27b6a_row2_col10, #T_27b6a_row3_col0, #T_27b6a_row3_col3, #T_27b6a_row3_col4, #T_27b6a_row3_col5, #T_27b6a_row3_col9, #T_27b6a_row4_col8, #T_27b6a_row4_col9, #T_27b6a_row4_col10, #T_27b6a_row5_col0, #T_27b6a_row5_col1, #T_27b6a_row5_col3, #T_27b6a_row5_col5, #T_27b6a_row5_col6, #T_27b6a_row6_col0, #T_27b6a_row6_col1, #T_27b6a_row6_col3, #T_27b6a_row6_col4, #T_27b6a_row6_col5, #T_27b6a_row6_col6, #T_27b6a_row6_col10, #T_27b6a_row7_col2, #T_27b6a_row7_col4, #T_27b6a_row7_col5, #T_27b6a_row7_col6, #T_27b6a_row7_col7, #T_27b6a_row7_col9, #T_27b6a_row8_col3, #T_27b6a_row8_col4, #T_27b6a_row8_col5, #T_27b6a_row8_col6, #T_27b6a_row8_col7, #T_27b6a_row8_col8, #T_27b6a_row8_col9 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.30</td>
<td>-1.04</td>
<td>0.75</td>
<td>0.94</td>
<td>-1.95</td>
<td>-1.30</td>
<td>0.13</td>
<td>-0.32</td>
<td>-0.02</td>
<td>-0.85</td>
<td>0.88</td>
</tr>
<tr>
<td>1</td>
<td>0.78</td>
<td>0.07</td>
<td>1.13</td>
<td>0.47</td>
<td>-0.86</td>
<td>0.37</td>
<td>-0.96</td>
<td>0.88</td>
<td>-0.05</td>
<td>-0.18</td>
<td>-0.68</td>
</tr>
<tr>
<td>2</td>
<td>1.22</td>
<td>-0.15</td>
<td>-0.43</td>
<td>-0.35</td>
<td>0.53</td>
<td>0.37</td>
<td>0.41</td>
<td>0.43</td>
<td>2.14</td>
<td>-0.41</td>
<td>-0.51</td>
</tr>
<tr>
<td>3</td>
<td>-0.81</td>
<td>0.62</td>
<td>1.13</td>
<td>-0.11</td>
<td>-0.84</td>
<td>-0.82</td>
<td>0.65</td>
<td>0.74</td>
<td>0.54</td>
<td>-0.67</td>
<td>0.23</td>
</tr>
<tr>
<td>4</td>
<td>0.12</td>
<td>0.22</td>
<td>0.87</td>
<td>0.22</td>
<td>0.68</td>
<td>0.07</td>
<td>0.29</td>
<td>0.63</td>
<td>-1.46</td>
<td>-0.32</td>
<td>-0.47</td>
</tr>
<tr>
<td>5</td>
<td>-0.64</td>
<td>-0.28</td>
<td>1.49</td>
<td>-0.87</td>
<td>0.97</td>
<td>-1.68</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.59</td>
<td>0.71</td>
<td>0.79</td>
</tr>
<tr>
<td>6</td>
<td>-0.35</td>
<td>-0.46</td>
<td>0.86</td>
<td>-0.19</td>
<td>-1.28</td>
<td>-1.13</td>
<td>-0.92</td>
<td>0.50</td>
<td>0.14</td>
<td>0.69</td>
<td>-0.43</td>
</tr>
<tr>
<td>7</td>
<td>0.16</td>
<td>0.63</td>
<td>-0.31</td>
<td>0.46</td>
<td>-0.66</td>
<td>-0.36</td>
<td>-0.38</td>
<td>-1.20</td>
<td>0.49</td>
<td>-0.47</td>
<td>0.01</td>
</tr>
<tr>
<td>8</td>
<td>0.48</td>
<td>0.45</td>
<td>0.67</td>
<td>-0.10</td>
<td>-0.42</td>
<td>-0.08</td>
<td>-1.69</td>
<td>-1.45</td>
<td>-1.32</td>
<td>-1.00</td>
<td>0.40</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_c94f9_row0_col0 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_e22a9_row0_col0, #T_e22a9_row0_col1, #T_e22a9_row0_col2, #T_e22a9_row0_col3, #T_e22a9_row0_col4, #T_e22a9_row0_col5, #T_e22a9_row0_col6, #T_e22a9_row0_col7, #T_e22a9_row0_col8, #T_e22a9_row0_col9, #T_e22a9_row0_col10, #T_e22a9_row0_col11, #T_e22a9_row0_col12, #T_e22a9_row0_col13, #T_e22a9_row0_col14, #T_e22a9_row0_col15, #T_e22a9_row0_col16, #T_e22a9_row0_col17, #T_e22a9_row1_col0, #T_e22a9_row1_col1, #T_e22a9_row1_col2, #T_e22a9_row1_col3, #T_e22a9_row1_col4, #T_e22a9_row1_col5, #T_e22a9_row1_col6, #T_e22a9_row1_col7, #T_e22a9_row1_col8, #T_e22a9_row1_col9, #T_e22a9_row1_col10, #T_e22a9_row1_col11, #T_e22a9_row1_col12, #T_e22a9_row1_col13, #T_e22a9_row1_col14, #T_e22a9_row1_col15, #T_e22a9_row1_col16, #T_e22a9_row1_col17, #T_e22a9_row2_col0, #T_e22a9_row2_col1, #T_e22a9_row2_col2, #T_e22a9_row2_col3, #T_e22a9_row2_col4, #T_e22a9_row2_col5, #T_e22a9_row2_col6, #T_e22a9_row2_col7, #T_e22a9_row2_col8, #T_e22a9_row2_col9, #T_e22a9_row2_col10, #T_e22a9_row2_col11, #T_e22a9_row2_col12, #T_e22a9_row2_col13, #T_e22a9_row2_col14, #T_e22a9_row2_col15, #T_e22a9_row2_col16, #T_e22a9_row2_col17, #T_e22a9_row3_col0, #T_e22a9_row3_col1, #T_e22a9_row3_col2, #T_e22a9_row3_col3, #T_e22a9_row3_col4, #T_e22a9_row3_col5, #T_e22a9_row3_col6, #T_e22a9_row3_col7, #T_e22a9_row3_col8, #T_e22a9_row3_col9, #T_e22a9_row3_col10, #T_e22a9_row3_col11, #T_e22a9_row3_col12, #T_e22a9_row3_col13, #T_e22a9_row3_col14, #T_e22a9_row3_col15, #T_e22a9_row3_col16, #T_e22a9_row3_col17, #T_e22a9_row4_col0, #T_e22a9_row4_col1, #T_e22a9_row4_col2, #T_e22a9_row4_col3, #T_e22a9_row4_col4, #T_e22a9_row4_col5, #T_e22a9_row4_col6, #T_e22a9_row4_col7, #T_e22a9_row4_col8, #T_e22a9_row4_col9, #T_e22a9_row4_col10, #T_e22a9_row4_col11, #T_e22a9_row4_col12, #T_e22a9_row4_col13, #T_e22a9_row4_col14, #T_e22a9_row4_col15, #T_e22a9_row4_col16, #T_e22a9_row4_col17, #T_e22a9_row5_col0, #T_e22a9_row5_col1, #T_e22a9_row5_col2, #T_e22a9_row5_col3, #T_e22a9_row5_col4, #T_e22a9_row5_col5, #T_e22a9_row5_col6, #T_e22a9_row5_col7, #T_e22a9_row5_col8, #T_e22a9_row5_col9, #T_e22a9_row5_col10, #T_e22a9_row5_col11, #T_e22a9_row5_col12, #T_e22a9_row5_col13, #T_e22a9_row5_col14, #T_e22a9_row5_col15, #T_e22a9_row5_col16, #T_e22a9_row5_col17, #T_e22a9_row6_col0, #T_e22a9_row6_col1, #T_e22a9_row6_col2, #T_e22a9_row6_col3, #T_e22a9_row6_col4, #T_e22a9_row6_col5, #T_e22a9_row6_col6, #T_e22a9_row6_col7, #T_e22a9_row6_col8, #T_e22a9_row6_col9, #T_e22a9_row6_col10, #T_e22a9_row6_col11, #T_e22a9_row6_col12, #T_e22a9_row6_col13, #T_e22a9_row6_col14, #T_e22a9_row6_col15, #T_e22a9_row6_col16, #T_e22a9_row6_col17, #T_e22a9_row7_col0, #T_e22a9_row7_col1, #T_e22a9_row7_col2, #T_e22a9_row7_col3, #T_e22a9_row7_col4, #T_e22a9_row7_col5, #T_e22a9_row7_col6, #T_e22a9_row7_col7, #T_e22a9_row7_col8, #T_e22a9_row7_col9, #T_e22a9_row7_col10, #T_e22a9_row7_col11, #T_e22a9_row7_col12, #T_e22a9_row7_col13, #T_e22a9_row7_col14, #T_e22a9_row7_col15, #T_e22a9_row7_col16, #T_e22a9_row7_col17, #T_e22a9_row8_col0, #T_e22a9_row8_col1, #T_e22a9_row8_col2, #T_e22a9_row8_col3, #T_e22a9_row8_col4, #T_e22a9_row8_col5, #T_e22a9_row8_col6, #T_e22a9_row8_col7, #T_e22a9_row8_col8, #T_e22a9_row8_col9, #T_e22a9_row8_col10, #T_e22a9_row8_col11, #T_e22a9_row8_col12, #T_e22a9_row8_col13, #T_e22a9_row8_col14, #T_e22a9_row8_col15, #T_e22a9_row8_col16, #T_e22a9_row8_col17, #T_e22a9_row9_col0, #T_e22a9_row9_col1, #T_e22a9_row9_col2, #T_e22a9_row9_col3, #T_e22a9_row9_col4, #T_e22a9_row9_col5, #T_e22a9_row9_col6, #T_e22a9_row9_col7, #T_e22a9_row9_col8, #T_e22a9_row9_col9, #T_e22a9_row9_col10, #T_e22a9_row9_col11, #T_e22a9_row9_col12, #T_e22a9_row9_col13, #T_e22a9_row9_col14, #T_e22a9_row9_col15, #T_e22a9_row9_col16, #T_e22a9_row9_col17, #T_e22a9_row10_col0, #T_e22a9_row10_col1, #T_e22a9_row10_col2, #T_e22a9_row10_col3, #T_e22a9_row10_col4, #T_e22a9_row10_col5, #T_e22a9_row10_col6, #T_e22a9_row10_col7, #T_e22a9_row10_col8, #T_e22a9_row10_col9, #T_e22a9_row10_col10, #T_e22a9_row10_col11, #T_e22a9_row10_col12, #T_e22a9_row10_col13, #T_e22a9_row10_col14, #T_e22a9_row10_col15, #T_e22a9_row10_col16, #T_e22a9_row10_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.44</td>
<td>0.02</td>
<td>0.24</td>
<td>0.22</td>
<td>0.29</td>
<td>0.35</td>
<td>0.18</td>
<td>0.03</td>
<td>0.39</td>
<td>0.17</td>
<td>0.25</td>
<td>0.02</td>
<td>0.10</td>
<td>0.13</td>
<td>0.00</td>
<td>0.42</td>
<td>0.21</td>
<td>0.31</td>
</tr>
<tr>
<td>1</td>
<td>0.35</td>
<td>0.06</td>
<td>0.26</td>
<td>0.42</td>
<td>0.05</td>
<td>0.41</td>
<td>0.16</td>
<td>0.33</td>
<td>0.03</td>
<td>0.26</td>
<td>0.11</td>
<td>0.03</td>
<td>0.23</td>
<td>0.04</td>
<td>0.37</td>
<td>0.27</td>
<td>0.32</td>
<td>0.40</td>
</tr>
<tr>
<td>2</td>
<td>0.37</td>
<td>0.30</td>
<td>0.36</td>
<td>0.14</td>
<td>0.21</td>
<td>0.40</td>
<td>0.01</td>
<td>0.28</td>
<td>0.16</td>
<td>0.44</td>
<td>0.43</td>
<td>0.23</td>
<td>0.27</td>
<td>0.22</td>
<td>0.23</td>
<td>0.25</td>
<td>0.43</td>
<td>0.05</td>
</tr>
<tr>
<td>3</td>
<td>0.32</td>
<td>0.25</td>
<td>0.05</td>
<td>0.45</td>
<td>0.08</td>
<td>0.18</td>
<td>0.26</td>
<td>0.24</td>
<td>0.34</td>
<td>0.07</td>
<td>0.07</td>
<td>0.14</td>
<td>0.04</td>
<td>0.19</td>
<td>0.29</td>
<td>0.23</td>
<td>0.43</td>
<td>0.09</td>
</tr>
<tr>
<td>4</td>
<td>0.36</td>
<td>0.05</td>
<td>0.20</td>
<td>0.41</td>
<td>0.38</td>
<td>0.29</td>
<td>0.01</td>
<td>0.44</td>
<td>0.17</td>
<td>0.04</td>
<td>0.31</td>
<td>0.34</td>
<td>0.29</td>
<td>0.16</td>
<td>0.25</td>
<td>0.18</td>
<td>0.01</td>
<td>0.28</td>
</tr>
<tr>
<td>5</td>
<td>0.34</td>
<td>0.31</td>
<td>0.38</td>
<td>0.34</td>
<td>0.08</td>
<td>0.40</td>
<td>0.15</td>
<td>0.16</td>
<td>0.14</td>
<td>0.25</td>
<td>0.15</td>
<td>0.20</td>
<td>0.10</td>
<td>0.06</td>
<td>0.44</td>
<td>0.19</td>
<td>0.42</td>
<td>0.21</td>
</tr>
<tr>
<td>6</td>
<td>0.01</td>
<td>0.38</td>
<td>0.43</td>
<td>0.18</td>
<td>0.00</td>
<td>0.43</td>
<td>0.45</td>
<td>0.28</td>
<td>0.25</td>
<td>0.18</td>
<td>0.03</td>
<td>0.26</td>
<td>0.22</td>
<td>0.26</td>
<td>0.08</td>
<td>0.23</td>
<td>0.45</td>
<td>0.42</td>
</tr>
<tr>
<td>7</td>
<td>0.04</td>
<td>0.12</td>
<td>0.28</td>
<td>0.17</td>
<td>0.11</td>
<td>0.00</td>
<td>0.15</td>
<td>0.24</td>
<td>0.05</td>
<td>0.05</td>
<td>0.27</td>
<td>0.32</td>
<td>0.33</td>
<td>0.11</td>
<td>0.09</td>
<td>0.40</td>
<td>0.19</td>
<td>0.06</td>
</tr>
<tr>
<td>8</td>
<td>0.30</td>
<td>0.17</td>
<td>0.21</td>
<td>0.42</td>
<td>0.21</td>
<td>0.29</td>
<td>0.19</td>
<td>0.38</td>
<td>0.03</td>
<td>0.34</td>
<td>0.32</td>
<td>0.30</td>
<td>0.34</td>
<td>0.15</td>
<td>0.28</td>
<td>0.11</td>
<td>0.44</td>
<td>0.19</td>
</tr>
<tr>
<td>9</td>
<td>0.10</td>
<td>0.10</td>
<td>0.35</td>
<td>0.32</td>
<td>0.24</td>
<td>0.28</td>
<td>0.30</td>
<td>0.28</td>
<td>0.10</td>
<td>0.12</td>
<td>0.30</td>
<td>0.41</td>
<td>0.15</td>
<td>0.00</td>
<td>0.10</td>
<td>0.40</td>
<td>0.18</td>
<td>0.24</td>
</tr>
<tr>
<td>10</td>
<td>0.00</td>
<td>0.22</td>
<td>0.21</td>
<td>0.09</td>
<td>0.10</td>
<td>0.13</td>
<td>0.18</td>
<td>0.37</td>
<td>0.24</td>
<td>0.29</td>
<td>0.25</td>
<td>0.23</td>
<td>0.32</td>
<td>0.14</td>
<td>0.27</td>
<td>0.34</td>
<td>0.25</td>
<td>0.10</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_9a48d_row0_col0, #T_9a48d_row0_col2, #T_9a48d_row0_col3, #T_9a48d_row0_col4, #T_9a48d_row0_col5, #T_9a48d_row0_col6, #T_9a48d_row0_col8, #T_9a48d_row0_col13, #T_9a48d_row1_col6, #T_9a48d_row1_col8, #T_9a48d_row1_col9, #T_9a48d_row1_col10, #T_9a48d_row1_col12, #T_9a48d_row1_col13, #T_9a48d_row2_col7, #T_9a48d_row2_col8, #T_9a48d_row2_col9, #T_9a48d_row2_col10, #T_9a48d_row2_col11, #T_9a48d_row2_col12, #T_9a48d_row2_col13, #T_9a48d_row3_col0, #T_9a48d_row3_col3, #T_9a48d_row3_col4, #T_9a48d_row3_col7, #T_9a48d_row3_col9, #T_9a48d_row3_col10, #T_9a48d_row3_col11, #T_9a48d_row3_col12, #T_9a48d_row3_col13, #T_9a48d_row4_col6, #T_9a48d_row4_col7, #T_9a48d_row4_col8, #T_9a48d_row4_col10, #T_9a48d_row4_col11, #T_9a48d_row4_col12, #T_9a48d_row4_col13, #T_9a48d_row5_col0, #T_9a48d_row5_col1, #T_9a48d_row5_col3, #T_9a48d_row5_col6, #T_9a48d_row5_col7, #T_9a48d_row5_col9, #T_9a48d_row5_col10, #T_9a48d_row5_col11, #T_9a48d_row5_col12, #T_9a48d_row5_col13, #T_9a48d_row6_col0, #T_9a48d_row6_col1, #T_9a48d_row6_col2, #T_9a48d_row6_col3, #T_9a48d_row6_col4, #T_9a48d_row6_col5, #T_9a48d_row6_col6, #T_9a48d_row7_col1, #T_9a48d_row7_col2, #T_9a48d_row7_col3, #T_9a48d_row7_col4, #T_9a48d_row7_col5, #T_9a48d_row7_col6, #T_9a48d_row8_col0, #T_9a48d_row8_col1, #T_9a48d_row8_col2, #T_9a48d_row8_col3, #T_9a48d_row8_col4, #T_9a48d_row8_col5, #T_9a48d_row8_col6 {
  background-color: #dcdddd;
  color: #000000;
}
#T_9a48d_row0_col1, #T_9a48d_row1_col0, #T_9a48d_row1_col1, #T_9a48d_row1_col2, #T_9a48d_row1_col3, #T_9a48d_row1_col4, #T_9a48d_row1_col5, #T_9a48d_row1_col14, #T_9a48d_row1_col15, #T_9a48d_row1_col16, #T_9a48d_row2_col0, #T_9a48d_row2_col1, #T_9a48d_row2_col2, #T_9a48d_row2_col3, #T_9a48d_row2_col4, #T_9a48d_row2_col5, #T_9a48d_row2_col6, #T_9a48d_row2_col14, #T_9a48d_row2_col15, #T_9a48d_row2_col16, #T_9a48d_row2_col17, #T_9a48d_row3_col1, #T_9a48d_row3_col2, #T_9a48d_row3_col5, #T_9a48d_row3_col6, #T_9a48d_row3_col14, #T_9a48d_row3_col15, #T_9a48d_row3_col16, #T_9a48d_row4_col0, #T_9a48d_row4_col1, #T_9a48d_row4_col2, #T_9a48d_row4_col3, #T_9a48d_row4_col4, #T_9a48d_row4_col5, #T_9a48d_row4_col14, #T_9a48d_row4_col15, #T_9a48d_row4_col16, #T_9a48d_row4_col17, #T_9a48d_row5_col2, #T_9a48d_row5_col4, #T_9a48d_row5_col5, #T_9a48d_row5_col15, #T_9a48d_row7_col0 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_9a48d_row0_col7, #T_9a48d_row0_col9, #T_9a48d_row0_col10, #T_9a48d_row0_col11, #T_9a48d_row0_col12, #T_9a48d_row0_col14, #T_9a48d_row0_col15, #T_9a48d_row0_col16, #T_9a48d_row0_col17, #T_9a48d_row1_col7, #T_9a48d_row1_col11, #T_9a48d_row1_col17, #T_9a48d_row3_col8, #T_9a48d_row3_col17, #T_9a48d_row4_col9, #T_9a48d_row5_col8, #T_9a48d_row5_col14, #T_9a48d_row5_col16, #T_9a48d_row5_col17, #T_9a48d_row6_col7, #T_9a48d_row6_col8, #T_9a48d_row6_col9, #T_9a48d_row6_col10, #T_9a48d_row6_col11, #T_9a48d_row6_col12, #T_9a48d_row6_col13, #T_9a48d_row6_col14, #T_9a48d_row6_col15, #T_9a48d_row6_col16, #T_9a48d_row6_col17, #T_9a48d_row7_col7, #T_9a48d_row7_col8, #T_9a48d_row7_col9, #T_9a48d_row7_col10, #T_9a48d_row7_col11, #T_9a48d_row7_col12, #T_9a48d_row7_col13, #T_9a48d_row7_col14, #T_9a48d_row7_col15, #T_9a48d_row7_col16, #T_9a48d_row7_col17, #T_9a48d_row8_col7, #T_9a48d_row8_col8, #T_9a48d_row8_col9, #T_9a48d_row8_col10, #T_9a48d_row8_col11, #T_9a48d_row8_col12, #T_9a48d_row8_col13, #T_9a48d_row8_col14, #T_9a48d_row8_col15, #T_9a48d_row8_col16, #T_9a48d_row8_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.00</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.93</td>
<td>-0.00</td>
<td>-0.07</td>
<td>-0.58</td>
<td>-0.88</td>
<td>-0.58</td>
<td>-0.00</td>
<td>-0.87</td>
<td>-0.49</td>
<td>-0.05</td>
<td>-1.00</td>
</tr>
<tr>
<td>1</td>
<td>0.73</td>
<td>0.10</td>
<td>0.22</td>
<td>0.18</td>
<td>0.18</td>
<td>0.16</td>
<td>0.00</td>
<td>-0.23</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.09</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.16</td>
<td>0.47</td>
<td>0.53</td>
<td>-0.27</td>
</tr>
<tr>
<td>2</td>
<td>1.15</td>
<td>0.36</td>
<td>0.82</td>
<td>1.20</td>
<td>0.80</td>
<td>1.06</td>
<td>0.61</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.53</td>
<td>0.61</td>
<td>1.00</td>
<td>0.94</td>
</tr>
<tr>
<td>3</td>
<td>0.00</td>
<td>0.45</td>
<td>0.28</td>
<td>0.00</td>
<td>0.00</td>
<td>0.11</td>
<td>0.14</td>
<td>-0.00</td>
<td>-0.21</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.15</td>
<td>0.08</td>
<td>0.72</td>
<td>-0.08</td>
</tr>
<tr>
<td>4</td>
<td>0.34</td>
<td>0.19</td>
<td>0.36</td>
<td>0.05</td>
<td>0.15</td>
<td>0.30</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.08</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.06</td>
<td>0.38</td>
<td>0.04</td>
<td>0.14</td>
</tr>
<tr>
<td>5</td>
<td>0.00</td>
<td>0.00</td>
<td>0.26</td>
<td>0.00</td>
<td>0.67</td>
<td>0.05</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.16</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.08</td>
<td>0.30</td>
<td>-0.17</td>
<td>-0.17</td>
</tr>
<tr>
<td>6</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.76</td>
<td>-0.68</td>
<td>-0.28</td>
<td>-0.11</td>
<td>-0.37</td>
<td>-0.42</td>
<td>-0.40</td>
<td>-0.88</td>
<td>-0.41</td>
<td>-0.67</td>
<td>-1.00</td>
</tr>
<tr>
<td>7</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.45</td>
<td>-0.17</td>
<td>-0.04</td>
<td>-0.57</td>
<td>-0.82</td>
<td>-0.50</td>
<td>-0.22</td>
<td>-0.07</td>
<td>-0.62</td>
<td>-0.13</td>
<td>-0.18</td>
</tr>
<tr>
<td>8</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-1.32</td>
<td>-0.35</td>
<td>-0.39</td>
<td>-0.77</td>
<td>-1.63</td>
<td>-1.12</td>
<td>-0.60</td>
<td>-0.47</td>
<td>-0.99</td>
<td>-1.00</td>
<td>-1.00</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_09cfa_row0_col0, #T_09cfa_row0_col2, #T_09cfa_row0_col3, #T_09cfa_row0_col6, #T_09cfa_row0_col10, #T_09cfa_row1_col0, #T_09cfa_row1_col1, #T_09cfa_row1_col2, #T_09cfa_row1_col3, #T_09cfa_row1_col5, #T_09cfa_row1_col7, #T_09cfa_row2_col0, #T_09cfa_row2_col4, #T_09cfa_row2_col5, #T_09cfa_row2_col6, #T_09cfa_row2_col7, #T_09cfa_row2_col8, #T_09cfa_row3_col1, #T_09cfa_row3_col2, #T_09cfa_row3_col6, #T_09cfa_row3_col7, #T_09cfa_row3_col8, #T_09cfa_row3_col10, #T_09cfa_row4_col0, #T_09cfa_row4_col1, #T_09cfa_row4_col2, #T_09cfa_row4_col3, #T_09cfa_row4_col4, #T_09cfa_row4_col5, #T_09cfa_row4_col6, #T_09cfa_row4_col7, #T_09cfa_row5_col2, #T_09cfa_row5_col4, #T_09cfa_row5_col7, #T_09cfa_row5_col8, #T_09cfa_row5_col9, #T_09cfa_row5_col10, #T_09cfa_row6_col2, #T_09cfa_row6_col7, #T_09cfa_row6_col8, #T_09cfa_row6_col9, #T_09cfa_row7_col0, #T_09cfa_row7_col1, #T_09cfa_row7_col3, #T_09cfa_row7_col8, #T_09cfa_row7_col10, #T_09cfa_row8_col0, #T_09cfa_row8_col1, #T_09cfa_row8_col2, #T_09cfa_row8_col10 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_09cfa_row0_col1, #T_09cfa_row0_col4, #T_09cfa_row0_col5, #T_09cfa_row0_col7, #T_09cfa_row0_col8, #T_09cfa_row0_col9, #T_09cfa_row1_col4, #T_09cfa_row1_col6, #T_09cfa_row1_col8, #T_09cfa_row1_col9, #T_09cfa_row1_col10, #T_09cfa_row2_col1, #T_09cfa_row2_col2, #T_09cfa_row2_col3, #T_09cfa_row2_col9, #T_09cfa_row2_col10, #T_09cfa_row3_col0, #T_09cfa_row3_col3, #T_09cfa_row3_col4, #T_09cfa_row3_col5, #T_09cfa_row3_col9, #T_09cfa_row4_col8, #T_09cfa_row4_col9, #T_09cfa_row4_col10, #T_09cfa_row5_col0, #T_09cfa_row5_col1, #T_09cfa_row5_col3, #T_09cfa_row5_col5, #T_09cfa_row5_col6, #T_09cfa_row6_col0, #T_09cfa_row6_col1, #T_09cfa_row6_col3, #T_09cfa_row6_col4, #T_09cfa_row6_col5, #T_09cfa_row6_col6, #T_09cfa_row6_col10, #T_09cfa_row7_col2, #T_09cfa_row7_col4, #T_09cfa_row7_col5, #T_09cfa_row7_col6, #T_09cfa_row7_col7, #T_09cfa_row7_col9, #T_09cfa_row8_col3, #T_09cfa_row8_col4, #T_09cfa_row8_col5, #T_09cfa_row8_col6, #T_09cfa_row8_col7, #T_09cfa_row8_col8, #T_09cfa_row8_col9 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.30</td>
<td>-1.04</td>
<td>0.75</td>
<td>0.94</td>
<td>-1.95</td>
<td>-1.30</td>
<td>0.13</td>
<td>-0.32</td>
<td>-0.02</td>
<td>-0.85</td>
<td>0.88</td>
</tr>
<tr>
<td>1</td>
<td>0.78</td>
<td>0.07</td>
<td>1.13</td>
<td>0.47</td>
<td>-0.86</td>
<td>0.37</td>
<td>-0.96</td>
<td>0.88</td>
<td>-0.05</td>
<td>-0.18</td>
<td>-0.68</td>
</tr>
<tr>
<td>2</td>
<td>1.22</td>
<td>-0.15</td>
<td>-0.43</td>
<td>-0.35</td>
<td>0.53</td>
<td>0.37</td>
<td>0.41</td>
<td>0.43</td>
<td>2.14</td>
<td>-0.41</td>
<td>-0.51</td>
</tr>
<tr>
<td>3</td>
<td>-0.81</td>
<td>0.62</td>
<td>1.13</td>
<td>-0.11</td>
<td>-0.84</td>
<td>-0.82</td>
<td>0.65</td>
<td>0.74</td>
<td>0.54</td>
<td>-0.67</td>
<td>0.23</td>
</tr>
<tr>
<td>4</td>
<td>0.12</td>
<td>0.22</td>
<td>0.87</td>
<td>0.22</td>
<td>0.68</td>
<td>0.07</td>
<td>0.29</td>
<td>0.63</td>
<td>-1.46</td>
<td>-0.32</td>
<td>-0.47</td>
</tr>
<tr>
<td>5</td>
<td>-0.64</td>
<td>-0.28</td>
<td>1.49</td>
<td>-0.87</td>
<td>0.97</td>
<td>-1.68</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.59</td>
<td>0.71</td>
<td>0.79</td>
</tr>
<tr>
<td>6</td>
<td>-0.35</td>
<td>-0.46</td>
<td>0.86</td>
<td>-0.19</td>
<td>-1.28</td>
<td>-1.13</td>
<td>-0.92</td>
<td>0.50</td>
<td>0.14</td>
<td>0.69</td>
<td>-0.43</td>
</tr>
<tr>
<td>7</td>
<td>0.16</td>
<td>0.63</td>
<td>-0.31</td>
<td>0.46</td>
<td>-0.66</td>
<td>-0.36</td>
<td>-0.38</td>
<td>-1.20</td>
<td>0.49</td>
<td>-0.47</td>
<td>0.01</td>
</tr>
<tr>
<td>8</td>
<td>0.48</td>
<td>0.45</td>
<td>0.67</td>
<td>-0.10</td>
<td>-0.42</td>
<td>-0.08</td>
<td>-1.69</td>
<td>-1.45</td>
<td>-1.32</td>
<td>-1.00</td>
<td>0.40</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_3fe34_row0_col0, #T_3fe34_row1_col0, #T_3fe34_row2_col0, #T_3fe34_row3_col0, #T_3fe34_row4_col0, #T_3fe34_row5_col0, #T_3fe34_row6_col0, #T_3fe34_row7_col0, #T_3fe34_row8_col0, #T_3fe34_row9_col0, #T_3fe34_row10_col0 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.00</td>
</tr>
<tr>
<td>1</td>
<td>1.00</td>
</tr>
<tr>
<td>2</td>
<td>1.00</td>
</tr>
<tr>
<td>3</td>
<td>1.00</td>
</tr>
<tr>
<td>4</td>
<td>1.00</td>
</tr>
<tr>
<td>5</td>
<td>1.00</td>
</tr>
<tr>
<td>6</td>
<td>1.00</td>
</tr>
<tr>
<td>7</td>
<td>1.00</td>
</tr>
<tr>
<td>8</td>
<td>1.00</td>
</tr>
<tr>
<td>9</td>
<td>1.00</td>
</tr>
<tr>
<td>10</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_e1c39_row0_col0, #T_e1c39_row0_col1, #T_e1c39_row0_col2, #T_e1c39_row0_col3, #T_e1c39_row0_col4, #T_e1c39_row0_col5, #T_e1c39_row0_col6, #T_e1c39_row0_col7, #T_e1c39_row0_col8, #T_e1c39_row0_col9, #T_e1c39_row0_col10, #T_e1c39_row0_col11, #T_e1c39_row0_col12, #T_e1c39_row0_col13, #T_e1c39_row0_col14, #T_e1c39_row0_col15, #T_e1c39_row0_col16, #T_e1c39_row0_col17, #T_e1c39_row1_col0, #T_e1c39_row1_col1, #T_e1c39_row1_col2, #T_e1c39_row1_col3, #T_e1c39_row1_col4, #T_e1c39_row1_col5, #T_e1c39_row1_col6, #T_e1c39_row1_col7, #T_e1c39_row1_col8, #T_e1c39_row1_col9, #T_e1c39_row1_col10, #T_e1c39_row1_col11, #T_e1c39_row1_col12, #T_e1c39_row1_col13, #T_e1c39_row1_col14, #T_e1c39_row1_col15, #T_e1c39_row1_col16, #T_e1c39_row1_col17, #T_e1c39_row2_col0, #T_e1c39_row2_col1, #T_e1c39_row2_col2, #T_e1c39_row2_col3, #T_e1c39_row2_col4, #T_e1c39_row2_col5, #T_e1c39_row2_col6, #T_e1c39_row2_col7, #T_e1c39_row2_col8, #T_e1c39_row2_col9, #T_e1c39_row2_col10, #T_e1c39_row2_col11, #T_e1c39_row2_col12, #T_e1c39_row2_col13, #T_e1c39_row2_col14, #T_e1c39_row2_col15, #T_e1c39_row2_col16, #T_e1c39_row2_col17, #T_e1c39_row3_col0, #T_e1c39_row3_col1, #T_e1c39_row3_col2, #T_e1c39_row3_col3, #T_e1c39_row3_col4, #T_e1c39_row3_col5, #T_e1c39_row3_col6, #T_e1c39_row3_col7, #T_e1c39_row3_col8, #T_e1c39_row3_col9, #T_e1c39_row3_col10, #T_e1c39_row3_col11, #T_e1c39_row3_col12, #T_e1c39_row3_col13, #T_e1c39_row3_col14, #T_e1c39_row3_col15, #T_e1c39_row3_col16, #T_e1c39_row3_col17, #T_e1c39_row4_col0, #T_e1c39_row4_col1, #T_e1c39_row4_col2, #T_e1c39_row4_col3, #T_e1c39_row4_col4, #T_e1c39_row4_col5, #T_e1c39_row4_col6, #T_e1c39_row4_col7, #T_e1c39_row4_col8, #T_e1c39_row4_col9, #T_e1c39_row4_col10, #T_e1c39_row4_col11, #T_e1c39_row4_col12, #T_e1c39_row4_col13, #T_e1c39_row4_col14, #T_e1c39_row4_col15, #T_e1c39_row4_col16, #T_e1c39_row4_col17, #T_e1c39_row5_col0, #T_e1c39_row5_col1, #T_e1c39_row5_col2, #T_e1c39_row5_col3, #T_e1c39_row5_col4, #T_e1c39_row5_col5, #T_e1c39_row5_col6, #T_e1c39_row5_col7, #T_e1c39_row5_col8, #T_e1c39_row5_col9, #T_e1c39_row5_col10, #T_e1c39_row5_col11, #T_e1c39_row5_col12, #T_e1c39_row5_col13, #T_e1c39_row5_col14, #T_e1c39_row5_col15, #T_e1c39_row5_col16, #T_e1c39_row5_col17, #T_e1c39_row6_col0, #T_e1c39_row6_col1, #T_e1c39_row6_col2, #T_e1c39_row6_col3, #T_e1c39_row6_col4, #T_e1c39_row6_col5, #T_e1c39_row6_col6, #T_e1c39_row6_col7, #T_e1c39_row6_col8, #T_e1c39_row6_col9, #T_e1c39_row6_col10, #T_e1c39_row6_col11, #T_e1c39_row6_col12, #T_e1c39_row6_col13, #T_e1c39_row6_col14, #T_e1c39_row6_col15, #T_e1c39_row6_col16, #T_e1c39_row6_col17, #T_e1c39_row7_col0, #T_e1c39_row7_col1, #T_e1c39_row7_col2, #T_e1c39_row7_col3, #T_e1c39_row7_col4, #T_e1c39_row7_col5, #T_e1c39_row7_col6, #T_e1c39_row7_col7, #T_e1c39_row7_col8, #T_e1c39_row7_col9, #T_e1c39_row7_col10, #T_e1c39_row7_col11, #T_e1c39_row7_col12, #T_e1c39_row7_col13, #T_e1c39_row7_col14, #T_e1c39_row7_col15, #T_e1c39_row7_col16, #T_e1c39_row7_col17, #T_e1c39_row8_col0, #T_e1c39_row8_col1, #T_e1c39_row8_col2, #T_e1c39_row8_col3, #T_e1c39_row8_col4, #T_e1c39_row8_col5, #T_e1c39_row8_col6, #T_e1c39_row8_col7, #T_e1c39_row8_col8, #T_e1c39_row8_col9, #T_e1c39_row8_col10, #T_e1c39_row8_col11, #T_e1c39_row8_col12, #T_e1c39_row8_col13, #T_e1c39_row8_col14, #T_e1c39_row8_col15, #T_e1c39_row8_col16, #T_e1c39_row8_col17, #T_e1c39_row9_col0, #T_e1c39_row9_col1, #T_e1c39_row9_col2, #T_e1c39_row9_col3, #T_e1c39_row9_col4, #T_e1c39_row9_col5, #T_e1c39_row9_col6, #T_e1c39_row9_col7, #T_e1c39_row9_col8, #T_e1c39_row9_col9, #T_e1c39_row9_col10, #T_e1c39_row9_col11, #T_e1c39_row9_col12, #T_e1c39_row9_col13, #T_e1c39_row9_col14, #T_e1c39_row9_col15, #T_e1c39_row9_col16, #T_e1c39_row9_col17, #T_e1c39_row10_col0, #T_e1c39_row10_col1, #T_e1c39_row10_col2, #T_e1c39_row10_col3, #T_e1c39_row10_col4, #T_e1c39_row10_col5, #T_e1c39_row10_col6, #T_e1c39_row10_col7, #T_e1c39_row10_col8, #T_e1c39_row10_col9, #T_e1c39_row10_col10, #T_e1c39_row10_col11, #T_e1c39_row10_col12, #T_e1c39_row10_col13, #T_e1c39_row10_col14, #T_e1c39_row10_col15, #T_e1c39_row10_col16, #T_e1c39_row10_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.31</td>
<td>0.02</td>
<td>0.11</td>
<td>0.29</td>
<td>0.10</td>
<td>0.33</td>
<td>0.37</td>
<td>0.06</td>
<td>0.39</td>
<td>0.35</td>
<td>0.15</td>
<td>0.13</td>
<td>0.15</td>
<td>0.45</td>
<td>0.07</td>
<td>0.19</td>
<td>0.03</td>
<td>0.06</td>
</tr>
<tr>
<td>1</td>
<td>0.12</td>
<td>0.02</td>
<td>0.06</td>
<td>0.41</td>
<td>0.32</td>
<td>0.24</td>
<td>0.34</td>
<td>0.28</td>
<td>0.22</td>
<td>0.06</td>
<td>0.33</td>
<td>0.27</td>
<td>0.25</td>
<td>0.23</td>
<td>0.43</td>
<td>0.09</td>
<td>0.45</td>
<td>0.27</td>
</tr>
<tr>
<td>2</td>
<td>0.19</td>
<td>0.11</td>
<td>0.19</td>
<td>0.25</td>
<td>0.07</td>
<td>0.42</td>
<td>0.32</td>
<td>0.35</td>
<td>0.15</td>
<td>0.05</td>
<td>0.00</td>
<td>0.24</td>
<td>0.22</td>
<td>0.39</td>
<td>0.44</td>
<td>0.11</td>
<td>0.19</td>
<td>0.10</td>
</tr>
<tr>
<td>3</td>
<td>0.15</td>
<td>0.37</td>
<td>0.21</td>
<td>0.41</td>
<td>0.25</td>
<td>0.04</td>
<td>0.37</td>
<td>0.04</td>
<td>0.05</td>
<td>0.22</td>
<td>0.31</td>
<td>0.35</td>
<td>0.35</td>
<td>0.08</td>
<td>0.38</td>
<td>0.01</td>
<td>0.25</td>
<td>0.29</td>
</tr>
<tr>
<td>4</td>
<td>0.17</td>
<td>0.45</td>
<td>0.24</td>
<td>0.32</td>
<td>0.01</td>
<td>0.00</td>
<td>0.19</td>
<td>0.34</td>
<td>0.17</td>
<td>0.19</td>
<td>0.18</td>
<td>0.34</td>
<td>0.02</td>
<td>0.24</td>
<td>0.03</td>
<td>0.41</td>
<td>0.26</td>
<td>0.00</td>
</tr>
<tr>
<td>5</td>
<td>0.29</td>
<td>0.10</td>
<td>0.07</td>
<td>0.34</td>
<td>0.04</td>
<td>0.30</td>
<td>0.39</td>
<td>0.27</td>
<td>0.39</td>
<td>0.16</td>
<td>0.33</td>
<td>0.45</td>
<td>0.06</td>
<td>0.19</td>
<td>0.23</td>
<td>0.04</td>
<td>0.36</td>
<td>0.04</td>
</tr>
<tr>
<td>6</td>
<td>0.13</td>
<td>0.15</td>
<td>0.22</td>
<td>0.40</td>
<td>0.14</td>
<td>0.30</td>
<td>0.11</td>
<td>0.45</td>
<td>0.14</td>
<td>0.17</td>
<td>0.26</td>
<td>0.16</td>
<td>0.36</td>
<td>0.10</td>
<td>0.17</td>
<td>0.32</td>
<td>0.14</td>
<td>0.08</td>
</tr>
<tr>
<td>7</td>
<td>0.25</td>
<td>0.25</td>
<td>0.24</td>
<td>0.45</td>
<td>0.17</td>
<td>0.45</td>
<td>0.30</td>
<td>0.35</td>
<td>0.41</td>
<td>0.40</td>
<td>0.11</td>
<td>0.26</td>
<td>0.32</td>
<td>0.08</td>
<td>0.22</td>
<td>0.34</td>
<td>0.05</td>
<td>0.09</td>
</tr>
<tr>
<td>8</td>
<td>0.16</td>
<td>0.27</td>
<td>0.10</td>
<td>0.23</td>
<td>0.08</td>
<td>0.21</td>
<td>0.19</td>
<td>0.16</td>
<td>0.06</td>
<td>0.04</td>
<td>0.17</td>
<td>0.05</td>
<td>0.39</td>
<td>0.11</td>
<td>0.26</td>
<td>0.25</td>
<td>0.13</td>
<td>0.05</td>
</tr>
<tr>
<td>9</td>
<td>0.17</td>
<td>0.17</td>
<td>0.00</td>
<td>0.13</td>
<td>0.12</td>
<td>0.03</td>
<td>0.39</td>
<td>0.11</td>
<td>0.01</td>
<td>0.29</td>
<td>0.43</td>
<td>0.20</td>
<td>0.21</td>
<td>0.43</td>
<td>0.39</td>
<td>0.18</td>
<td>0.19</td>
<td>0.27</td>
</tr>
<tr>
<td>10</td>
<td>0.26</td>
<td>0.23</td>
<td>0.43</td>
<td>0.04</td>
<td>0.25</td>
<td>0.36</td>
<td>0.21</td>
<td>0.36</td>
<td>0.37</td>
<td>0.36</td>
<td>0.08</td>
<td>0.14</td>
<td>0.25</td>
<td>0.24</td>
<td>0.30</td>
<td>0.33</td>
<td>0.04</td>
<td>0.07</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_5b704_row0_col0, #T_5b704_row0_col1, #T_5b704_row0_col3, #T_5b704_row0_col4, #T_5b704_row0_col5, #T_5b704_row0_col6, #T_5b704_row0_col12, #T_5b704_row1_col1, #T_5b704_row1_col2, #T_5b704_row1_col8, #T_5b704_row1_col9, #T_5b704_row1_col11, #T_5b704_row1_col12, #T_5b704_row1_col13, #T_5b704_row2_col7, #T_5b704_row2_col8, #T_5b704_row2_col9, #T_5b704_row2_col10, #T_5b704_row2_col11, #T_5b704_row2_col12, #T_5b704_row2_col13, #T_5b704_row3_col0, #T_5b704_row3_col1, #T_5b704_row3_col6, #T_5b704_row3_col7, #T_5b704_row3_col8, #T_5b704_row3_col12, #T_5b704_row4_col7, #T_5b704_row4_col8, #T_5b704_row4_col9, #T_5b704_row4_col10, #T_5b704_row4_col11, #T_5b704_row4_col13, #T_5b704_row5_col3, #T_5b704_row5_col4, #T_5b704_row5_col6, #T_5b704_row5_col7, #T_5b704_row5_col9, #T_5b704_row5_col12, #T_5b704_row5_col13, #T_5b704_row6_col0, #T_5b704_row6_col1, #T_5b704_row6_col2, #T_5b704_row6_col3, #T_5b704_row6_col4, #T_5b704_row6_col5, #T_5b704_row6_col6, #T_5b704_row7_col0, #T_5b704_row7_col1, #T_5b704_row7_col2, #T_5b704_row7_col3, #T_5b704_row7_col5, #T_5b704_row7_col6, #T_5b704_row8_col0, #T_5b704_row8_col1, #T_5b704_row8_col2, #T_5b704_row8_col3, #T_5b704_row8_col4, #T_5b704_row8_col5, #T_5b704_row8_col6 {
  background-color: #dcdddd;
  color: #000000;
}
#T_5b704_row0_col2, #T_5b704_row1_col0, #T_5b704_row1_col3, #T_5b704_row1_col4, #T_5b704_row1_col5, #T_5b704_row1_col6, #T_5b704_row1_col14, #T_5b704_row1_col16, #T_5b704_row1_col17, #T_5b704_row2_col0, #T_5b704_row2_col1, #T_5b704_row2_col2, #T_5b704_row2_col3, #T_5b704_row2_col4, #T_5b704_row2_col5, #T_5b704_row2_col6, #T_5b704_row2_col14, #T_5b704_row2_col15, #T_5b704_row2_col16, #T_5b704_row3_col2, #T_5b704_row3_col3, #T_5b704_row3_col4, #T_5b704_row3_col5, #T_5b704_row3_col14, #T_5b704_row3_col15, #T_5b704_row3_col17, #T_5b704_row4_col0, #T_5b704_row4_col1, #T_5b704_row4_col2, #T_5b704_row4_col3, #T_5b704_row4_col4, #T_5b704_row4_col5, #T_5b704_row4_col6, #T_5b704_row4_col14, #T_5b704_row4_col15, #T_5b704_row4_col16, #T_5b704_row4_col17, #T_5b704_row5_col0, #T_5b704_row5_col1, #T_5b704_row5_col2, #T_5b704_row5_col5, #T_5b704_row5_col14, #T_5b704_row5_col15, #T_5b704_row7_col4, #T_5b704_row7_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_5b704_row0_col7, #T_5b704_row0_col8, #T_5b704_row0_col9, #T_5b704_row0_col10, #T_5b704_row0_col11, #T_5b704_row0_col13, #T_5b704_row0_col14, #T_5b704_row0_col15, #T_5b704_row0_col16, #T_5b704_row0_col17, #T_5b704_row1_col7, #T_5b704_row1_col10, #T_5b704_row1_col15, #T_5b704_row2_col17, #T_5b704_row3_col9, #T_5b704_row3_col10, #T_5b704_row3_col11, #T_5b704_row3_col13, #T_5b704_row3_col16, #T_5b704_row4_col12, #T_5b704_row5_col8, #T_5b704_row5_col10, #T_5b704_row5_col11, #T_5b704_row5_col16, #T_5b704_row5_col17, #T_5b704_row6_col7, #T_5b704_row6_col8, #T_5b704_row6_col9, #T_5b704_row6_col10, #T_5b704_row6_col11, #T_5b704_row6_col12, #T_5b704_row6_col13, #T_5b704_row6_col14, #T_5b704_row6_col15, #T_5b704_row6_col16, #T_5b704_row6_col17, #T_5b704_row7_col7, #T_5b704_row7_col8, #T_5b704_row7_col9, #T_5b704_row7_col10, #T_5b704_row7_col11, #T_5b704_row7_col12, #T_5b704_row7_col13, #T_5b704_row7_col14, #T_5b704_row7_col15, #T_5b704_row7_col16, #T_5b704_row8_col7, #T_5b704_row8_col8, #T_5b704_row8_col9, #T_5b704_row8_col10, #T_5b704_row8_col11, #T_5b704_row8_col12, #T_5b704_row8_col13, #T_5b704_row8_col14, #T_5b704_row8_col15, #T_5b704_row8_col16, #T_5b704_row8_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.00</td>
<td>0.00</td>
<td>0.08</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.82</td>
<td>-0.58</td>
<td>-0.32</td>
<td>-1.07</td>
<td>-1.09</td>
<td>-0.00</td>
<td>-0.63</td>
<td>-0.21</td>
<td>-0.74</td>
<td>-1.00</td>
<td>-0.15</td>
</tr>
<tr>
<td>1</td>
<td>0.36</td>
<td>0.00</td>
<td>0.00</td>
<td>0.51</td>
<td>0.11</td>
<td>0.72</td>
<td>0.76</td>
<td>-0.12</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.05</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.56</td>
<td>-0.34</td>
<td>0.13</td>
<td>0.22</td>
</tr>
<tr>
<td>2</td>
<td>0.72</td>
<td>0.68</td>
<td>0.32</td>
<td>1.10</td>
<td>0.10</td>
<td>0.84</td>
<td>0.68</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.20</td>
<td>0.97</td>
<td>0.33</td>
<td>-0.07</td>
</tr>
<tr>
<td>3</td>
<td>0.00</td>
<td>0.00</td>
<td>0.36</td>
<td>0.35</td>
<td>0.36</td>
<td>0.82</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.19</td>
<td>-0.29</td>
<td>-0.13</td>
<td>-0.00</td>
<td>-0.20</td>
<td>0.67</td>
<td>0.20</td>
<td>-0.00</td>
<td>0.14</td>
</tr>
<tr>
<td>4</td>
<td>0.18</td>
<td>0.14</td>
<td>0.26</td>
<td>0.68</td>
<td>0.09</td>
<td>0.38</td>
<td>0.36</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.07</td>
<td>-0.00</td>
<td>0.14</td>
<td>0.15</td>
<td>0.33</td>
<td>0.10</td>
</tr>
<tr>
<td>5</td>
<td>0.01</td>
<td>0.55</td>
<td>0.50</td>
<td>0.00</td>
<td>0.00</td>
<td>0.21</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.27</td>
<td>-0.00</td>
<td>-0.44</td>
<td>-0.25</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.44</td>
<td>0.83</td>
<td>-0.24</td>
<td>-0.01</td>
</tr>
<tr>
<td>6</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.89</td>
<td>-0.85</td>
<td>-0.48</td>
<td>-0.77</td>
<td>-0.90</td>
<td>-0.21</td>
<td>-0.30</td>
<td>-0.09</td>
<td>-0.69</td>
<td>-0.83</td>
<td>-0.03</td>
</tr>
<tr>
<td>7</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.01</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.79</td>
<td>-0.59</td>
<td>-0.65</td>
<td>-0.21</td>
<td>-0.55</td>
<td>-0.19</td>
<td>-0.37</td>
<td>-0.17</td>
<td>-0.71</td>
<td>-0.10</td>
<td>0.03</td>
</tr>
<tr>
<td>8</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-1.24</td>
<td>-0.48</td>
<td>-0.95</td>
<td>-1.13</td>
<td>-0.71</td>
<td>-1.40</td>
<td>-0.30</td>
<td>-0.76</td>
<td>-1.00</td>
<td>-0.47</td>
<td>-0.39</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_80235_row0_col0, #T_80235_row0_col2, #T_80235_row0_col3, #T_80235_row0_col6, #T_80235_row0_col10, #T_80235_row1_col0, #T_80235_row1_col1, #T_80235_row1_col2, #T_80235_row1_col3, #T_80235_row1_col5, #T_80235_row1_col7, #T_80235_row2_col0, #T_80235_row2_col4, #T_80235_row2_col5, #T_80235_row2_col6, #T_80235_row2_col7, #T_80235_row2_col8, #T_80235_row3_col1, #T_80235_row3_col2, #T_80235_row3_col6, #T_80235_row3_col7, #T_80235_row3_col8, #T_80235_row3_col10, #T_80235_row4_col0, #T_80235_row4_col1, #T_80235_row4_col2, #T_80235_row4_col3, #T_80235_row4_col4, #T_80235_row4_col5, #T_80235_row4_col6, #T_80235_row4_col7, #T_80235_row5_col2, #T_80235_row5_col4, #T_80235_row5_col7, #T_80235_row5_col8, #T_80235_row5_col9, #T_80235_row5_col10, #T_80235_row6_col2, #T_80235_row6_col7, #T_80235_row6_col8, #T_80235_row6_col9, #T_80235_row7_col0, #T_80235_row7_col1, #T_80235_row7_col3, #T_80235_row7_col8, #T_80235_row7_col10, #T_80235_row8_col0, #T_80235_row8_col1, #T_80235_row8_col2, #T_80235_row8_col10 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_80235_row0_col1, #T_80235_row0_col4, #T_80235_row0_col5, #T_80235_row0_col7, #T_80235_row0_col8, #T_80235_row0_col9, #T_80235_row1_col4, #T_80235_row1_col6, #T_80235_row1_col8, #T_80235_row1_col9, #T_80235_row1_col10, #T_80235_row2_col1, #T_80235_row2_col2, #T_80235_row2_col3, #T_80235_row2_col9, #T_80235_row2_col10, #T_80235_row3_col0, #T_80235_row3_col3, #T_80235_row3_col4, #T_80235_row3_col5, #T_80235_row3_col9, #T_80235_row4_col8, #T_80235_row4_col9, #T_80235_row4_col10, #T_80235_row5_col0, #T_80235_row5_col1, #T_80235_row5_col3, #T_80235_row5_col5, #T_80235_row5_col6, #T_80235_row6_col0, #T_80235_row6_col1, #T_80235_row6_col3, #T_80235_row6_col4, #T_80235_row6_col5, #T_80235_row6_col6, #T_80235_row6_col10, #T_80235_row7_col2, #T_80235_row7_col4, #T_80235_row7_col5, #T_80235_row7_col6, #T_80235_row7_col7, #T_80235_row7_col9, #T_80235_row8_col3, #T_80235_row8_col4, #T_80235_row8_col5, #T_80235_row8_col6, #T_80235_row8_col7, #T_80235_row8_col8, #T_80235_row8_col9 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.30</td>
<td>-1.04</td>
<td>0.75</td>
<td>0.94</td>
<td>-1.95</td>
<td>-1.30</td>
<td>0.13</td>
<td>-0.32</td>
<td>-0.02</td>
<td>-0.85</td>
<td>0.88</td>
</tr>
<tr>
<td>1</td>
<td>0.78</td>
<td>0.07</td>
<td>1.13</td>
<td>0.47</td>
<td>-0.86</td>
<td>0.37</td>
<td>-0.96</td>
<td>0.88</td>
<td>-0.05</td>
<td>-0.18</td>
<td>-0.68</td>
</tr>
<tr>
<td>2</td>
<td>1.22</td>
<td>-0.15</td>
<td>-0.43</td>
<td>-0.35</td>
<td>0.53</td>
<td>0.37</td>
<td>0.41</td>
<td>0.43</td>
<td>2.14</td>
<td>-0.41</td>
<td>-0.51</td>
</tr>
<tr>
<td>3</td>
<td>-0.81</td>
<td>0.62</td>
<td>1.13</td>
<td>-0.11</td>
<td>-0.84</td>
<td>-0.82</td>
<td>0.65</td>
<td>0.74</td>
<td>0.54</td>
<td>-0.67</td>
<td>0.23</td>
</tr>
<tr>
<td>4</td>
<td>0.12</td>
<td>0.22</td>
<td>0.87</td>
<td>0.22</td>
<td>0.68</td>
<td>0.07</td>
<td>0.29</td>
<td>0.63</td>
<td>-1.46</td>
<td>-0.32</td>
<td>-0.47</td>
</tr>
<tr>
<td>5</td>
<td>-0.64</td>
<td>-0.28</td>
<td>1.49</td>
<td>-0.87</td>
<td>0.97</td>
<td>-1.68</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.59</td>
<td>0.71</td>
<td>0.79</td>
</tr>
<tr>
<td>6</td>
<td>-0.35</td>
<td>-0.46</td>
<td>0.86</td>
<td>-0.19</td>
<td>-1.28</td>
<td>-1.13</td>
<td>-0.92</td>
<td>0.50</td>
<td>0.14</td>
<td>0.69</td>
<td>-0.43</td>
</tr>
<tr>
<td>7</td>
<td>0.16</td>
<td>0.63</td>
<td>-0.31</td>
<td>0.46</td>
<td>-0.66</td>
<td>-0.36</td>
<td>-0.38</td>
<td>-1.20</td>
<td>0.49</td>
<td>-0.47</td>
<td>0.01</td>
</tr>
<tr>
<td>8</td>
<td>0.48</td>
<td>0.45</td>
<td>0.67</td>
<td>-0.10</td>
<td>-0.42</td>
<td>-0.08</td>
<td>-1.69</td>
<td>-1.45</td>
<td>-1.32</td>
<td>-1.00</td>
<td>0.40</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_058ae_row0_col0 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>-1.00</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_2b479_row0_col0, #T_2b479_row0_col1, #T_2b479_row0_col2, #T_2b479_row0_col3, #T_2b479_row0_col4, #T_2b479_row0_col5, #T_2b479_row0_col6, #T_2b479_row0_col7, #T_2b479_row0_col8, #T_2b479_row0_col9, #T_2b479_row0_col10, #T_2b479_row0_col11, #T_2b479_row0_col12, #T_2b479_row0_col13, #T_2b479_row0_col14, #T_2b479_row0_col15, #T_2b479_row0_col16, #T_2b479_row0_col17, #T_2b479_row1_col0, #T_2b479_row1_col1, #T_2b479_row1_col2, #T_2b479_row1_col3, #T_2b479_row1_col4, #T_2b479_row1_col5, #T_2b479_row1_col6, #T_2b479_row1_col7, #T_2b479_row1_col8, #T_2b479_row1_col9, #T_2b479_row1_col10, #T_2b479_row1_col11, #T_2b479_row1_col12, #T_2b479_row1_col13, #T_2b479_row1_col14, #T_2b479_row1_col15, #T_2b479_row1_col16, #T_2b479_row1_col17, #T_2b479_row2_col0, #T_2b479_row2_col1, #T_2b479_row2_col2, #T_2b479_row2_col3, #T_2b479_row2_col4, #T_2b479_row2_col5, #T_2b479_row2_col6, #T_2b479_row2_col7, #T_2b479_row2_col8, #T_2b479_row2_col9, #T_2b479_row2_col10, #T_2b479_row2_col11, #T_2b479_row2_col12, #T_2b479_row2_col13, #T_2b479_row2_col14, #T_2b479_row2_col15, #T_2b479_row2_col16, #T_2b479_row2_col17, #T_2b479_row3_col0, #T_2b479_row3_col1, #T_2b479_row3_col2, #T_2b479_row3_col3, #T_2b479_row3_col4, #T_2b479_row3_col5, #T_2b479_row3_col6, #T_2b479_row3_col7, #T_2b479_row3_col8, #T_2b479_row3_col9, #T_2b479_row3_col10, #T_2b479_row3_col11, #T_2b479_row3_col12, #T_2b479_row3_col13, #T_2b479_row3_col14, #T_2b479_row3_col15, #T_2b479_row3_col16, #T_2b479_row3_col17, #T_2b479_row4_col0, #T_2b479_row4_col1, #T_2b479_row4_col2, #T_2b479_row4_col3, #T_2b479_row4_col4, #T_2b479_row4_col5, #T_2b479_row4_col6, #T_2b479_row4_col7, #T_2b479_row4_col8, #T_2b479_row4_col9, #T_2b479_row4_col10, #T_2b479_row4_col11, #T_2b479_row4_col12, #T_2b479_row4_col13, #T_2b479_row4_col14, #T_2b479_row4_col15, #T_2b479_row4_col16, #T_2b479_row4_col17, #T_2b479_row5_col0, #T_2b479_row5_col1, #T_2b479_row5_col2, #T_2b479_row5_col3, #T_2b479_row5_col4, #T_2b479_row5_col5, #T_2b479_row5_col6, #T_2b479_row5_col7, #T_2b479_row5_col8, #T_2b479_row5_col9, #T_2b479_row5_col10, #T_2b479_row5_col11, #T_2b479_row5_col12, #T_2b479_row5_col13, #T_2b479_row5_col14, #T_2b479_row5_col15, #T_2b479_row5_col16, #T_2b479_row5_col17, #T_2b479_row6_col0, #T_2b479_row6_col1, #T_2b479_row6_col2, #T_2b479_row6_col3, #T_2b479_row6_col4, #T_2b479_row6_col5, #T_2b479_row6_col6, #T_2b479_row6_col7, #T_2b479_row6_col8, #T_2b479_row6_col9, #T_2b479_row6_col10, #T_2b479_row6_col11, #T_2b479_row6_col12, #T_2b479_row6_col13, #T_2b479_row6_col14, #T_2b479_row6_col15, #T_2b479_row6_col16, #T_2b479_row6_col17, #T_2b479_row7_col0, #T_2b479_row7_col1, #T_2b479_row7_col2, #T_2b479_row7_col3, #T_2b479_row7_col4, #T_2b479_row7_col5, #T_2b479_row7_col6, #T_2b479_row7_col7, #T_2b479_row7_col8, #T_2b479_row7_col9, #T_2b479_row7_col10, #T_2b479_row7_col11, #T_2b479_row7_col12, #T_2b479_row7_col13, #T_2b479_row7_col14, #T_2b479_row7_col15, #T_2b479_row7_col16, #T_2b479_row7_col17, #T_2b479_row8_col0, #T_2b479_row8_col1, #T_2b479_row8_col2, #T_2b479_row8_col3, #T_2b479_row8_col4, #T_2b479_row8_col5, #T_2b479_row8_col6, #T_2b479_row8_col7, #T_2b479_row8_col8, #T_2b479_row8_col9, #T_2b479_row8_col10, #T_2b479_row8_col11, #T_2b479_row8_col12, #T_2b479_row8_col13, #T_2b479_row8_col14, #T_2b479_row8_col15, #T_2b479_row8_col16, #T_2b479_row8_col17, #T_2b479_row9_col0, #T_2b479_row9_col1, #T_2b479_row9_col2, #T_2b479_row9_col3, #T_2b479_row9_col4, #T_2b479_row9_col5, #T_2b479_row9_col6, #T_2b479_row9_col7, #T_2b479_row9_col8, #T_2b479_row9_col9, #T_2b479_row9_col10, #T_2b479_row9_col11, #T_2b479_row9_col12, #T_2b479_row9_col13, #T_2b479_row9_col14, #T_2b479_row9_col15, #T_2b479_row9_col16, #T_2b479_row9_col17, #T_2b479_row10_col0, #T_2b479_row10_col1, #T_2b479_row10_col2, #T_2b479_row10_col3, #T_2b479_row10_col4, #T_2b479_row10_col5, #T_2b479_row10_col6, #T_2b479_row10_col7, #T_2b479_row10_col8, #T_2b479_row10_col9, #T_2b479_row10_col10, #T_2b479_row10_col11, #T_2b479_row10_col12, #T_2b479_row10_col13, #T_2b479_row10_col14, #T_2b479_row10_col15, #T_2b479_row10_col16, #T_2b479_row10_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>-0.29</td>
<td>-0.12</td>
<td>-0.00</td>
<td>-0.17</td>
<td>-0.33</td>
<td>-0.17</td>
<td>-0.33</td>
<td>-0.36</td>
<td>-0.28</td>
<td>-0.16</td>
<td>-0.24</td>
<td>-0.22</td>
<td>-0.10</td>
<td>-0.13</td>
<td>-0.02</td>
<td>-0.38</td>
<td>-0.23</td>
<td>-0.02</td>
</tr>
<tr>
<td>1</td>
<td>-0.36</td>
<td>-0.13</td>
<td>-0.05</td>
<td>-0.07</td>
<td>-0.41</td>
<td>-0.30</td>
<td>-0.38</td>
<td>-0.06</td>
<td>-0.40</td>
<td>-0.42</td>
<td>-0.44</td>
<td>-0.03</td>
<td>-0.27</td>
<td>-0.03</td>
<td>-0.32</td>
<td>-0.31</td>
<td>-0.35</td>
<td>-0.40</td>
</tr>
<tr>
<td>2</td>
<td>-0.30</td>
<td>-0.07</td>
<td>-0.40</td>
<td>-0.06</td>
<td>-0.10</td>
<td>-0.21</td>
<td>-0.16</td>
<td>-0.22</td>
<td>-0.06</td>
<td>-0.36</td>
<td>-0.40</td>
<td>-0.42</td>
<td>-0.23</td>
<td>-0.22</td>
<td>-0.20</td>
<td>-0.33</td>
<td>-0.45</td>
<td>-0.06</td>
</tr>
<tr>
<td>3</td>
<td>-0.05</td>
<td>-0.08</td>
<td>-0.07</td>
<td>-0.30</td>
<td>-0.44</td>
<td>-0.23</td>
<td>-0.40</td>
<td>-0.25</td>
<td>-0.13</td>
<td>-0.31</td>
<td>-0.11</td>
<td>-0.13</td>
<td>-0.13</td>
<td>-0.34</td>
<td>-0.15</td>
<td>-0.05</td>
<td>-0.36</td>
<td>-0.13</td>
</tr>
<tr>
<td>4</td>
<td>-0.45</td>
<td>-0.34</td>
<td>-0.41</td>
<td>-0.39</td>
<td>-0.15</td>
<td>-0.10</td>
<td>-0.40</td>
<td>-0.32</td>
<td>-0.19</td>
<td>-0.13</td>
<td>-0.29</td>
<td>-0.39</td>
<td>-0.43</td>
<td>-0.29</td>
<td>-0.13</td>
<td>-0.05</td>
<td>-0.39</td>
<td>-0.01</td>
</tr>
<tr>
<td>5</td>
<td>-0.09</td>
<td>-0.38</td>
<td>-0.00</td>
<td>-0.12</td>
<td>-0.07</td>
<td>-0.42</td>
<td>-0.01</td>
<td>-0.12</td>
<td>-0.26</td>
<td>-0.28</td>
<td>-0.16</td>
<td>-0.06</td>
<td>-0.08</td>
<td>-0.43</td>
<td>-0.23</td>
<td>-0.28</td>
<td>-0.28</td>
<td>-0.07</td>
</tr>
<tr>
<td>6</td>
<td>-0.34</td>
<td>-0.38</td>
<td>-0.15</td>
<td>-0.44</td>
<td>-0.41</td>
<td>-0.19</td>
<td>-0.25</td>
<td>-0.41</td>
<td>-0.34</td>
<td>-0.22</td>
<td>-0.43</td>
<td>-0.36</td>
<td>-0.25</td>
<td>-0.28</td>
<td>-0.06</td>
<td>-0.12</td>
<td>-0.15</td>
<td>-0.16</td>
</tr>
<tr>
<td>7</td>
<td>-0.17</td>
<td>-0.39</td>
<td>-0.40</td>
<td>-0.26</td>
<td>-0.40</td>
<td>-0.20</td>
<td>-0.10</td>
<td>-0.14</td>
<td>-0.42</td>
<td>-0.21</td>
<td>-0.18</td>
<td>-0.25</td>
<td>-0.15</td>
<td>-0.21</td>
<td>-0.13</td>
<td>-0.41</td>
<td>-0.14</td>
<td>-0.14</td>
</tr>
<tr>
<td>8</td>
<td>-0.38</td>
<td>-0.03</td>
<td>-0.10</td>
<td>-0.21</td>
<td>-0.13</td>
<td>-0.04</td>
<td>-0.19</td>
<td>-0.00</td>
<td>-0.09</td>
<td>-0.38</td>
<td>-0.01</td>
<td>-0.27</td>
<td>-0.24</td>
<td>-0.24</td>
<td>-0.13</td>
<td>-0.18</td>
<td>-0.37</td>
<td>-0.21</td>
</tr>
<tr>
<td>9</td>
<td>-0.43</td>
<td>-0.08</td>
<td>-0.20</td>
<td>-0.29</td>
<td>-0.10</td>
<td>-0.27</td>
<td>-0.08</td>
<td>-0.43</td>
<td>-0.22</td>
<td>-0.37</td>
<td>-0.27</td>
<td>-0.24</td>
<td>-0.15</td>
<td>-0.22</td>
<td>-0.01</td>
<td>-0.45</td>
<td>-0.35</td>
<td>-0.31</td>
</tr>
<tr>
<td>10</td>
<td>-0.38</td>
<td>-0.44</td>
<td>-0.20</td>
<td>-0.31</td>
<td>-0.42</td>
<td>-0.23</td>
<td>-0.03</td>
<td>-0.31</td>
<td>-0.11</td>
<td>-0.35</td>
<td>-0.01</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.39</td>
<td>-0.45</td>
<td>-0.14</td>
<td>-0.03</td>
<td>-0.10</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_78217_row0_col0, #T_78217_row0_col1, #T_78217_row0_col2, #T_78217_row0_col3, #T_78217_row0_col5, #T_78217_row0_col6, #T_78217_row0_col14, #T_78217_row0_col15, #T_78217_row0_col16, #T_78217_row0_col17, #T_78217_row1_col0, #T_78217_row1_col1, #T_78217_row1_col3, #T_78217_row3_col3, #T_78217_row3_col5, #T_78217_row4_col17, #T_78217_row5_col1, #T_78217_row5_col4, #T_78217_row5_col5, #T_78217_row6_col0, #T_78217_row6_col1, #T_78217_row6_col2, #T_78217_row6_col3, #T_78217_row6_col4, #T_78217_row6_col5, #T_78217_row6_col6, #T_78217_row6_col14, #T_78217_row6_col15, #T_78217_row6_col16, #T_78217_row6_col17, #T_78217_row7_col0, #T_78217_row7_col1, #T_78217_row7_col2, #T_78217_row7_col3, #T_78217_row7_col4, #T_78217_row7_col5, #T_78217_row7_col6, #T_78217_row7_col14, #T_78217_row7_col15, #T_78217_row7_col16, #T_78217_row7_col17, #T_78217_row8_col0, #T_78217_row8_col1, #T_78217_row8_col2, #T_78217_row8_col3, #T_78217_row8_col4, #T_78217_row8_col5, #T_78217_row8_col6, #T_78217_row8_col14, #T_78217_row8_col15, #T_78217_row8_col16, #T_78217_row8_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_78217_row0_col4, #T_78217_row0_col7, #T_78217_row0_col8, #T_78217_row0_col9, #T_78217_row0_col10, #T_78217_row0_col11, #T_78217_row0_col12, #T_78217_row0_col13, #T_78217_row1_col2, #T_78217_row1_col4, #T_78217_row1_col5, #T_78217_row1_col6, #T_78217_row1_col7, #T_78217_row1_col12, #T_78217_row2_col0, #T_78217_row2_col1, #T_78217_row2_col2, #T_78217_row2_col3, #T_78217_row2_col4, #T_78217_row2_col5, #T_78217_row2_col6, #T_78217_row3_col0, #T_78217_row3_col1, #T_78217_row3_col2, #T_78217_row3_col4, #T_78217_row3_col6, #T_78217_row3_col7, #T_78217_row3_col13, #T_78217_row4_col0, #T_78217_row4_col1, #T_78217_row4_col2, #T_78217_row4_col3, #T_78217_row4_col4, #T_78217_row4_col5, #T_78217_row4_col6, #T_78217_row4_col9, #T_78217_row5_col0, #T_78217_row5_col2, #T_78217_row5_col3, #T_78217_row5_col6, #T_78217_row5_col8, #T_78217_row6_col7, #T_78217_row6_col8, #T_78217_row6_col9, #T_78217_row6_col10, #T_78217_row6_col11, #T_78217_row6_col12, #T_78217_row6_col13, #T_78217_row7_col7, #T_78217_row7_col8, #T_78217_row7_col9, #T_78217_row7_col10, #T_78217_row7_col11, #T_78217_row7_col12, #T_78217_row7_col13, #T_78217_row8_col7, #T_78217_row8_col8, #T_78217_row8_col9, #T_78217_row8_col10, #T_78217_row8_col11, #T_78217_row8_col12, #T_78217_row8_col13 {
  background-color: #dcdddd;
  color: #000000;
}
#T_78217_row1_col8, #T_78217_row1_col9, #T_78217_row1_col10, #T_78217_row1_col11, #T_78217_row1_col13, #T_78217_row1_col14, #T_78217_row1_col15, #T_78217_row1_col16, #T_78217_row1_col17, #T_78217_row2_col7, #T_78217_row2_col8, #T_78217_row2_col9, #T_78217_row2_col10, #T_78217_row2_col11, #T_78217_row2_col12, #T_78217_row2_col13, #T_78217_row2_col14, #T_78217_row2_col15, #T_78217_row2_col16, #T_78217_row2_col17, #T_78217_row3_col8, #T_78217_row3_col9, #T_78217_row3_col10, #T_78217_row3_col11, #T_78217_row3_col12, #T_78217_row3_col14, #T_78217_row3_col15, #T_78217_row3_col16, #T_78217_row3_col17, #T_78217_row4_col7, #T_78217_row4_col8, #T_78217_row4_col10, #T_78217_row4_col11, #T_78217_row4_col12, #T_78217_row4_col13, #T_78217_row4_col14, #T_78217_row4_col15, #T_78217_row4_col16, #T_78217_row5_col7, #T_78217_row5_col9, #T_78217_row5_col10, #T_78217_row5_col11, #T_78217_row5_col12, #T_78217_row5_col13, #T_78217_row5_col14, #T_78217_row5_col15, #T_78217_row5_col16, #T_78217_row5_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.05</td>
<td>0.88</td>
<td>0.59</td>
<td>0.61</td>
<td>0.00</td>
<td>0.70</td>
<td>0.64</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.24</td>
<td>0.74</td>
<td>1.00</td>
<td>0.55</td>
</tr>
<tr>
<td>1</td>
<td>0.27</td>
<td>0.26</td>
<td>0.00</td>
<td>0.41</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.23</td>
<td>-0.33</td>
<td>-0.21</td>
<td>-0.20</td>
<td>-0.00</td>
<td>-0.02</td>
<td>-0.04</td>
<td>-0.82</td>
<td>-0.52</td>
<td>-0.02</td>
</tr>
<tr>
<td>2</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.36</td>
<td>-0.77</td>
<td>-0.71</td>
<td>-0.39</td>
<td>-1.00</td>
<td>-0.82</td>
<td>-0.67</td>
<td>-0.11</td>
<td>-0.74</td>
<td>-0.97</td>
<td>-0.31</td>
</tr>
<tr>
<td>3</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.01</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.15</td>
<td>-0.50</td>
<td>-0.38</td>
<td>-0.33</td>
<td>-0.20</td>
<td>-0.00</td>
<td>-0.39</td>
<td>-0.20</td>
<td>-0.12</td>
<td>-0.36</td>
</tr>
<tr>
<td>4</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.45</td>
<td>-0.46</td>
<td>-0.00</td>
<td>-0.84</td>
<td>-0.48</td>
<td>-0.36</td>
<td>-0.13</td>
<td>-0.08</td>
<td>-0.28</td>
<td>-0.33</td>
<td>0.13</td>
</tr>
<tr>
<td>5</td>
<td>0.00</td>
<td>0.02</td>
<td>0.00</td>
<td>0.00</td>
<td>0.12</td>
<td>0.33</td>
<td>0.00</td>
<td>-0.41</td>
<td>-0.00</td>
<td>-0.44</td>
<td>-0.33</td>
<td>-0.90</td>
<td>-0.56</td>
<td>-0.04</td>
<td>-0.24</td>
<td>-0.27</td>
<td>-0.48</td>
<td>-0.16</td>
</tr>
<tr>
<td>6</td>
<td>0.74</td>
<td>1.20</td>
<td>0.11</td>
<td>0.90</td>
<td>0.84</td>
<td>0.65</td>
<td>0.87</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.60</td>
<td>0.01</td>
<td>0.53</td>
<td>0.12</td>
</tr>
<tr>
<td>7</td>
<td>0.47</td>
<td>0.89</td>
<td>0.91</td>
<td>0.62</td>
<td>0.26</td>
<td>0.37</td>
<td>0.01</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.07</td>
<td>0.61</td>
<td>0.29</td>
<td>0.01</td>
</tr>
<tr>
<td>8</td>
<td>1.30</td>
<td>1.17</td>
<td>0.98</td>
<td>1.61</td>
<td>1.09</td>
<td>0.59</td>
<td>0.65</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.09</td>
<td>0.93</td>
<td>0.94</td>
<td>0.81</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_2d4a1_row0_col0, #T_2d4a1_row0_col2, #T_2d4a1_row0_col3, #T_2d4a1_row0_col6, #T_2d4a1_row0_col10, #T_2d4a1_row1_col0, #T_2d4a1_row1_col1, #T_2d4a1_row1_col2, #T_2d4a1_row1_col3, #T_2d4a1_row1_col5, #T_2d4a1_row1_col7, #T_2d4a1_row2_col0, #T_2d4a1_row2_col4, #T_2d4a1_row2_col5, #T_2d4a1_row2_col6, #T_2d4a1_row2_col7, #T_2d4a1_row2_col8, #T_2d4a1_row3_col1, #T_2d4a1_row3_col2, #T_2d4a1_row3_col6, #T_2d4a1_row3_col7, #T_2d4a1_row3_col8, #T_2d4a1_row3_col10, #T_2d4a1_row4_col0, #T_2d4a1_row4_col1, #T_2d4a1_row4_col2, #T_2d4a1_row4_col3, #T_2d4a1_row4_col4, #T_2d4a1_row4_col5, #T_2d4a1_row4_col6, #T_2d4a1_row4_col7, #T_2d4a1_row5_col2, #T_2d4a1_row5_col4, #T_2d4a1_row5_col7, #T_2d4a1_row5_col8, #T_2d4a1_row5_col9, #T_2d4a1_row5_col10, #T_2d4a1_row6_col2, #T_2d4a1_row6_col7, #T_2d4a1_row6_col8, #T_2d4a1_row6_col9, #T_2d4a1_row7_col0, #T_2d4a1_row7_col1, #T_2d4a1_row7_col3, #T_2d4a1_row7_col8, #T_2d4a1_row7_col10, #T_2d4a1_row8_col0, #T_2d4a1_row8_col1, #T_2d4a1_row8_col2, #T_2d4a1_row8_col10 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_2d4a1_row0_col1, #T_2d4a1_row0_col4, #T_2d4a1_row0_col5, #T_2d4a1_row0_col7, #T_2d4a1_row0_col8, #T_2d4a1_row0_col9, #T_2d4a1_row1_col4, #T_2d4a1_row1_col6, #T_2d4a1_row1_col8, #T_2d4a1_row1_col9, #T_2d4a1_row1_col10, #T_2d4a1_row2_col1, #T_2d4a1_row2_col2, #T_2d4a1_row2_col3, #T_2d4a1_row2_col9, #T_2d4a1_row2_col10, #T_2d4a1_row3_col0, #T_2d4a1_row3_col3, #T_2d4a1_row3_col4, #T_2d4a1_row3_col5, #T_2d4a1_row3_col9, #T_2d4a1_row4_col8, #T_2d4a1_row4_col9, #T_2d4a1_row4_col10, #T_2d4a1_row5_col0, #T_2d4a1_row5_col1, #T_2d4a1_row5_col3, #T_2d4a1_row5_col5, #T_2d4a1_row5_col6, #T_2d4a1_row6_col0, #T_2d4a1_row6_col1, #T_2d4a1_row6_col3, #T_2d4a1_row6_col4, #T_2d4a1_row6_col5, #T_2d4a1_row6_col6, #T_2d4a1_row6_col10, #T_2d4a1_row7_col2, #T_2d4a1_row7_col4, #T_2d4a1_row7_col5, #T_2d4a1_row7_col6, #T_2d4a1_row7_col7, #T_2d4a1_row7_col9, #T_2d4a1_row8_col3, #T_2d4a1_row8_col4, #T_2d4a1_row8_col5, #T_2d4a1_row8_col6, #T_2d4a1_row8_col7, #T_2d4a1_row8_col8, #T_2d4a1_row8_col9 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.30</td>
<td>-1.04</td>
<td>0.75</td>
<td>0.94</td>
<td>-1.95</td>
<td>-1.30</td>
<td>0.13</td>
<td>-0.32</td>
<td>-0.02</td>
<td>-0.85</td>
<td>0.88</td>
</tr>
<tr>
<td>1</td>
<td>0.78</td>
<td>0.07</td>
<td>1.13</td>
<td>0.47</td>
<td>-0.86</td>
<td>0.37</td>
<td>-0.96</td>
<td>0.88</td>
<td>-0.05</td>
<td>-0.18</td>
<td>-0.68</td>
</tr>
<tr>
<td>2</td>
<td>1.22</td>
<td>-0.15</td>
<td>-0.43</td>
<td>-0.35</td>
<td>0.53</td>
<td>0.37</td>
<td>0.41</td>
<td>0.43</td>
<td>2.14</td>
<td>-0.41</td>
<td>-0.51</td>
</tr>
<tr>
<td>3</td>
<td>-0.81</td>
<td>0.62</td>
<td>1.13</td>
<td>-0.11</td>
<td>-0.84</td>
<td>-0.82</td>
<td>0.65</td>
<td>0.74</td>
<td>0.54</td>
<td>-0.67</td>
<td>0.23</td>
</tr>
<tr>
<td>4</td>
<td>0.12</td>
<td>0.22</td>
<td>0.87</td>
<td>0.22</td>
<td>0.68</td>
<td>0.07</td>
<td>0.29</td>
<td>0.63</td>
<td>-1.46</td>
<td>-0.32</td>
<td>-0.47</td>
</tr>
<tr>
<td>5</td>
<td>-0.64</td>
<td>-0.28</td>
<td>1.49</td>
<td>-0.87</td>
<td>0.97</td>
<td>-1.68</td>
<td>-0.33</td>
<td>0.16</td>
<td>0.59</td>
<td>0.71</td>
<td>0.79</td>
</tr>
<tr>
<td>6</td>
<td>-0.35</td>
<td>-0.46</td>
<td>0.86</td>
<td>-0.19</td>
<td>-1.28</td>
<td>-1.13</td>
<td>-0.92</td>
<td>0.50</td>
<td>0.14</td>
<td>0.69</td>
<td>-0.43</td>
</tr>
<tr>
<td>7</td>
<td>0.16</td>
<td>0.63</td>
<td>-0.31</td>
<td>0.46</td>
<td>-0.66</td>
<td>-0.36</td>
<td>-0.38</td>
<td>-1.20</td>
<td>0.49</td>
<td>-0.47</td>
<td>0.01</td>
</tr>
<tr>
<td>8</td>
<td>0.48</td>
<td>0.45</td>
<td>0.67</td>
<td>-0.10</td>
<td>-0.42</td>
<td>-0.08</td>
<td>-1.69</td>
<td>-1.45</td>
<td>-1.32</td>
<td>-1.00</td>
<td>0.40</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_92fe3_row0_col0, #T_92fe3_row1_col0, #T_92fe3_row2_col0, #T_92fe3_row3_col0, #T_92fe3_row4_col0, #T_92fe3_row5_col0, #T_92fe3_row6_col0, #T_92fe3_row7_col0, #T_92fe3_row8_col0, #T_92fe3_row9_col0, #T_92fe3_row10_col0 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>-1.00</td>
</tr>
<tr>
<td>1</td>
<td>-1.00</td>
</tr>
<tr>
<td>2</td>
<td>-1.00</td>
</tr>
<tr>
<td>3</td>
<td>-1.00</td>
</tr>
<tr>
<td>4</td>
<td>-1.00</td>
</tr>
<tr>
<td>5</td>
<td>-1.00</td>
</tr>
<tr>
<td>6</td>
<td>-1.00</td>
</tr>
<tr>
<td>7</td>
<td>-1.00</td>
</tr>
<tr>
<td>8</td>
<td>-1.00</td>
</tr>
<tr>
<td>9</td>
<td>-1.00</td>
</tr>
<tr>
<td>10</td>
<td>-1.00</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_1c817_row0_col0, #T_1c817_row0_col1, #T_1c817_row0_col2, #T_1c817_row0_col3, #T_1c817_row0_col4, #T_1c817_row0_col5, #T_1c817_row0_col6, #T_1c817_row0_col7, #T_1c817_row0_col8, #T_1c817_row0_col9, #T_1c817_row0_col10, #T_1c817_row0_col11, #T_1c817_row0_col12, #T_1c817_row0_col13, #T_1c817_row0_col14, #T_1c817_row0_col15, #T_1c817_row0_col16, #T_1c817_row0_col17, #T_1c817_row1_col0, #T_1c817_row1_col1, #T_1c817_row1_col2, #T_1c817_row1_col3, #T_1c817_row1_col4, #T_1c817_row1_col5, #T_1c817_row1_col6, #T_1c817_row1_col7, #T_1c817_row1_col8, #T_1c817_row1_col9, #T_1c817_row1_col10, #T_1c817_row1_col11, #T_1c817_row1_col12, #T_1c817_row1_col13, #T_1c817_row1_col14, #T_1c817_row1_col15, #T_1c817_row1_col16, #T_1c817_row1_col17, #T_1c817_row2_col0, #T_1c817_row2_col1, #T_1c817_row2_col2, #T_1c817_row2_col3, #T_1c817_row2_col4, #T_1c817_row2_col5, #T_1c817_row2_col6, #T_1c817_row2_col7, #T_1c817_row2_col8, #T_1c817_row2_col9, #T_1c817_row2_col10, #T_1c817_row2_col11, #T_1c817_row2_col12, #T_1c817_row2_col13, #T_1c817_row2_col14, #T_1c817_row2_col15, #T_1c817_row2_col16, #T_1c817_row2_col17, #T_1c817_row3_col0, #T_1c817_row3_col1, #T_1c817_row3_col2, #T_1c817_row3_col3, #T_1c817_row3_col4, #T_1c817_row3_col5, #T_1c817_row3_col6, #T_1c817_row3_col7, #T_1c817_row3_col8, #T_1c817_row3_col9, #T_1c817_row3_col10, #T_1c817_row3_col11, #T_1c817_row3_col12, #T_1c817_row3_col13, #T_1c817_row3_col14, #T_1c817_row3_col15, #T_1c817_row3_col16, #T_1c817_row3_col17, #T_1c817_row4_col0, #T_1c817_row4_col1, #T_1c817_row4_col2, #T_1c817_row4_col3, #T_1c817_row4_col4, #T_1c817_row4_col5, #T_1c817_row4_col6, #T_1c817_row4_col7, #T_1c817_row4_col8, #T_1c817_row4_col9, #T_1c817_row4_col10, #T_1c817_row4_col11, #T_1c817_row4_col12, #T_1c817_row4_col13, #T_1c817_row4_col14, #T_1c817_row4_col15, #T_1c817_row4_col16, #T_1c817_row4_col17, #T_1c817_row5_col0, #T_1c817_row5_col1, #T_1c817_row5_col2, #T_1c817_row5_col3, #T_1c817_row5_col4, #T_1c817_row5_col5, #T_1c817_row5_col6, #T_1c817_row5_col7, #T_1c817_row5_col8, #T_1c817_row5_col9, #T_1c817_row5_col10, #T_1c817_row5_col11, #T_1c817_row5_col12, #T_1c817_row5_col13, #T_1c817_row5_col14, #T_1c817_row5_col15, #T_1c817_row5_col16, #T_1c817_row5_col17, #T_1c817_row6_col0, #T_1c817_row6_col1, #T_1c817_row6_col2, #T_1c817_row6_col3, #T_1c817_row6_col4, #T_1c817_row6_col5, #T_1c817_row6_col6, #T_1c817_row6_col7, #T_1c817_row6_col8, #T_1c817_row6_col9, #T_1c817_row6_col10, #T_1c817_row6_col11, #T_1c817_row6_col12, #T_1c817_row6_col13, #T_1c817_row6_col14, #T_1c817_row6_col15, #T_1c817_row6_col16, #T_1c817_row6_col17, #T_1c817_row7_col0, #T_1c817_row7_col1, #T_1c817_row7_col2, #T_1c817_row7_col3, #T_1c817_row7_col4, #T_1c817_row7_col5, #T_1c817_row7_col6, #T_1c817_row7_col7, #T_1c817_row7_col8, #T_1c817_row7_col9, #T_1c817_row7_col10, #T_1c817_row7_col11, #T_1c817_row7_col12, #T_1c817_row7_col13, #T_1c817_row7_col14, #T_1c817_row7_col15, #T_1c817_row7_col16, #T_1c817_row7_col17, #T_1c817_row8_col0, #T_1c817_row8_col1, #T_1c817_row8_col2, #T_1c817_row8_col3, #T_1c817_row8_col4, #T_1c817_row8_col5, #T_1c817_row8_col6, #T_1c817_row8_col7, #T_1c817_row8_col8, #T_1c817_row8_col9, #T_1c817_row8_col10, #T_1c817_row8_col11, #T_1c817_row8_col12, #T_1c817_row8_col13, #T_1c817_row8_col14, #T_1c817_row8_col15, #T_1c817_row8_col16, #T_1c817_row8_col17, #T_1c817_row9_col0, #T_1c817_row9_col1, #T_1c817_row9_col2, #T_1c817_row9_col3, #T_1c817_row9_col4, #T_1c817_row9_col5, #T_1c817_row9_col6, #T_1c817_row9_col7, #T_1c817_row9_col8, #T_1c817_row9_col9, #T_1c817_row9_col10, #T_1c817_row9_col11, #T_1c817_row9_col12, #T_1c817_row9_col13, #T_1c817_row9_col14, #T_1c817_row9_col15, #T_1c817_row9_col16, #T_1c817_row9_col17, #T_1c817_row10_col0, #T_1c817_row10_col1, #T_1c817_row10_col2, #T_1c817_row10_col3, #T_1c817_row10_col4, #T_1c817_row10_col5, #T_1c817_row10_col6, #T_1c817_row10_col7, #T_1c817_row10_col8, #T_1c817_row10_col9, #T_1c817_row10_col10, #T_1c817_row10_col11, #T_1c817_row10_col12, #T_1c817_row10_col13, #T_1c817_row10_col14, #T_1c817_row10_col15, #T_1c817_row10_col16, #T_1c817_row10_col17 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>-0.45</td>
<td>-0.28</td>
<td>-0.30</td>
<td>-0.41</td>
<td>-0.17</td>
<td>-0.39</td>
<td>-0.22</td>
<td>-0.45</td>
<td>-0.28</td>
<td>-0.40</td>
<td>-0.18</td>
<td>-0.20</td>
<td>-0.16</td>
<td>-0.18</td>
<td>-0.10</td>
<td>-0.13</td>
<td>-0.14</td>
<td>-0.35</td>
</tr>
<tr>
<td>1</td>
<td>-0.09</td>
<td>-0.27</td>
<td>-0.09</td>
<td>-0.14</td>
<td>-0.02</td>
<td>-0.36</td>
<td>-0.21</td>
<td>-0.05</td>
<td>-0.05</td>
<td>-0.01</td>
<td>-0.02</td>
<td>-0.45</td>
<td>-0.03</td>
<td>-0.09</td>
<td>-0.01</td>
<td>-0.05</td>
<td>-0.39</td>
<td>-0.05</td>
</tr>
<tr>
<td>2</td>
<td>-0.17</td>
<td>-0.15</td>
<td>-0.37</td>
<td>-0.35</td>
<td>-0.32</td>
<td>-0.03</td>
<td>-0.24</td>
<td>-0.31</td>
<td>-0.35</td>
<td>-0.41</td>
<td>-0.00</td>
<td>-0.37</td>
<td>-0.18</td>
<td>-0.26</td>
<td>-0.09</td>
<td>-0.44</td>
<td>-0.09</td>
<td>-0.17</td>
</tr>
<tr>
<td>3</td>
<td>-0.42</td>
<td>-0.17</td>
<td>-0.11</td>
<td>-0.31</td>
<td>-0.32</td>
<td>-0.11</td>
<td>-0.20</td>
<td>-0.10</td>
<td>-0.34</td>
<td>-0.15</td>
<td>-0.24</td>
<td>-0.22</td>
<td>-0.22</td>
<td>-0.08</td>
<td>-0.40</td>
<td>-0.02</td>
<td>-0.23</td>
<td>-0.38</td>
</tr>
<tr>
<td>4</td>
<td>-0.13</td>
<td>-0.17</td>
<td>-0.06</td>
<td>-0.13</td>
<td>-0.32</td>
<td>-0.42</td>
<td>-0.28</td>
<td>-0.44</td>
<td>-0.03</td>
<td>-0.26</td>
<td>-0.38</td>
<td>-0.45</td>
<td>-0.08</td>
<td>-0.06</td>
<td>-0.04</td>
<td>-0.33</td>
<td>-0.27</td>
<td>-0.38</td>
</tr>
<tr>
<td>5</td>
<td>-0.32</td>
<td>-0.38</td>
<td>-0.19</td>
<td>-0.19</td>
<td>-0.33</td>
<td>-0.01</td>
<td>-0.15</td>
<td>-0.08</td>
<td>-0.31</td>
<td>-0.27</td>
<td>-0.07</td>
<td>-0.11</td>
<td>-0.21</td>
<td>-0.22</td>
<td>-0.18</td>
<td>-0.27</td>
<td>-0.19</td>
<td>-0.15</td>
</tr>
<tr>
<td>6</td>
<td>-0.30</td>
<td>-0.16</td>
<td>-0.09</td>
<td>-0.25</td>
<td>-0.23</td>
<td>-0.44</td>
<td>-0.25</td>
<td>-0.16</td>
<td>-0.05</td>
<td>-0.13</td>
<td>-0.20</td>
<td>-0.09</td>
<td>-0.14</td>
<td>-0.18</td>
<td>-0.15</td>
<td>-0.22</td>
<td>-0.37</td>
<td>-0.38</td>
</tr>
<tr>
<td>7</td>
<td>-0.20</td>
<td>-0.14</td>
<td>-0.12</td>
<td>-0.10</td>
<td>-0.42</td>
<td>-0.42</td>
<td>-0.14</td>
<td>-0.04</td>
<td>-0.44</td>
<td>-0.11</td>
<td>-0.10</td>
<td>-0.17</td>
<td>-0.06</td>
<td>-0.29</td>
<td>-0.22</td>
<td>-0.24</td>
<td>-0.01</td>
<td>-0.45</td>
</tr>
<tr>
<td>8</td>
<td>-0.31</td>
<td>-0.11</td>
<td>-0.16</td>
<td>-0.21</td>
<td>-0.16</td>
<td>-0.39</td>
<td>-0.12</td>
<td>-0.36</td>
<td>-0.36</td>
<td>-0.29</td>
<td>-0.24</td>
<td>-0.24</td>
<td>-0.20</td>
<td>-0.18</td>
<td>-0.33</td>
<td>-0.39</td>
<td>-0.20</td>
<td>-0.02</td>
</tr>
<tr>
<td>9</td>
<td>-0.41</td>
<td>-0.14</td>
<td>-0.12</td>
<td>-0.21</td>
<td>-0.01</td>
<td>-0.37</td>
<td>-0.03</td>
<td>-0.22</td>
<td>-0.38</td>
<td>-0.22</td>
<td>-0.09</td>
<td>-0.22</td>
<td>-0.19</td>
<td>-0.17</td>
<td>-0.13</td>
<td>-0.32</td>
<td>-0.30</td>
<td>-0.21</td>
</tr>
<tr>
<td>10</td>
<td>-0.31</td>
<td>-0.05</td>
<td>-0.02</td>
<td>-0.36</td>
<td>-0.04</td>
<td>-0.15</td>
<td>-0.03</td>
<td>-0.12</td>
<td>-0.36</td>
<td>-0.21</td>
<td>-0.40</td>
<td>-0.03</td>
<td>-0.04</td>
<td>-0.03</td>
<td>-0.23</td>
<td>-0.01</td>
<td>-0.02</td>
<td>-0.41</td>
</tr>
</tbody>
</table>
<style type="text/css">
#T_edcff_row0_col0, #T_edcff_row0_col1, #T_edcff_row0_col2, #T_edcff_row0_col4, #T_edcff_row0_col5, #T_edcff_row0_col6, #T_edcff_row0_col15, #T_edcff_row0_col16, #T_edcff_row0_col17, #T_edcff_row1_col5, #T_edcff_row1_col16, #T_edcff_row3_col0, #T_edcff_row3_col1, #T_edcff_row3_col16, #T_edcff_row3_col17, #T_edcff_row4_col0, #T_edcff_row4_col14, #T_edcff_row5_col0, #T_edcff_row5_col1, #T_edcff_row5_col4, #T_edcff_row5_col14, #T_edcff_row5_col16, #T_edcff_row6_col0, #T_edcff_row6_col1, #T_edcff_row6_col2, #T_edcff_row6_col3, #T_edcff_row6_col4, #T_edcff_row6_col5, #T_edcff_row6_col6, #T_edcff_row6_col14, #T_edcff_row6_col15, #T_edcff_row6_col16, #T_edcff_row6_col17, #T_edcff_row7_col0, #T_edcff_row7_col1, #T_edcff_row7_col2, #T_edcff_row7_col3, #T_edcff_row7_col4, #T_edcff_row7_col5, #T_edcff_row7_col6, #T_edcff_row7_col14, #T_edcff_row7_col15, #T_edcff_row7_col16, #T_edcff_row7_col17, #T_edcff_row8_col0, #T_edcff_row8_col1, #T_edcff_row8_col2, #T_edcff_row8_col3, #T_edcff_row8_col4, #T_edcff_row8_col5, #T_edcff_row8_col6, #T_edcff_row8_col14, #T_edcff_row8_col15, #T_edcff_row8_col16, #T_edcff_row8_col17 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_edcff_row0_col3, #T_edcff_row0_col7, #T_edcff_row0_col9, #T_edcff_row0_col10, #T_edcff_row0_col11, #T_edcff_row0_col12, #T_edcff_row0_col13, #T_edcff_row1_col0, #T_edcff_row1_col1, #T_edcff_row1_col2, #T_edcff_row1_col3, #T_edcff_row1_col4, #T_edcff_row1_col6, #T_edcff_row1_col10, #T_edcff_row2_col0, #T_edcff_row2_col1, #T_edcff_row2_col2, #T_edcff_row2_col3, #T_edcff_row2_col4, #T_edcff_row2_col5, #T_edcff_row2_col6, #T_edcff_row3_col2, #T_edcff_row3_col3, #T_edcff_row3_col4, #T_edcff_row3_col5, #T_edcff_row3_col6, #T_edcff_row3_col7, #T_edcff_row3_col9, #T_edcff_row3_col10, #T_edcff_row3_col12, #T_edcff_row4_col1, #T_edcff_row4_col2, #T_edcff_row4_col3, #T_edcff_row4_col4, #T_edcff_row4_col5, #T_edcff_row4_col6, #T_edcff_row4_col8, #T_edcff_row4_col10, #T_edcff_row5_col2, #T_edcff_row5_col3, #T_edcff_row5_col5, #T_edcff_row5_col6, #T_edcff_row5_col12, #T_edcff_row6_col7, #T_edcff_row6_col9, #T_edcff_row6_col10, #T_edcff_row6_col11, #T_edcff_row6_col12, #T_edcff_row6_col13, #T_edcff_row7_col7, #T_edcff_row7_col8, #T_edcff_row7_col9, #T_edcff_row7_col10, #T_edcff_row7_col11, #T_edcff_row7_col12, #T_edcff_row7_col13, #T_edcff_row8_col7, #T_edcff_row8_col8, #T_edcff_row8_col9, #T_edcff_row8_col10, #T_edcff_row8_col11, #T_edcff_row8_col12, #T_edcff_row8_col13 {
  background-color: #dcdddd;
  color: #000000;
}
#T_edcff_row0_col8, #T_edcff_row0_col14, #T_edcff_row1_col7, #T_edcff_row1_col8, #T_edcff_row1_col9, #T_edcff_row1_col11, #T_edcff_row1_col12, #T_edcff_row1_col13, #T_edcff_row1_col14, #T_edcff_row1_col15, #T_edcff_row1_col17, #T_edcff_row2_col7, #T_edcff_row2_col8, #T_edcff_row2_col9, #T_edcff_row2_col10, #T_edcff_row2_col11, #T_edcff_row2_col12, #T_edcff_row2_col13, #T_edcff_row2_col14, #T_edcff_row2_col15, #T_edcff_row2_col16, #T_edcff_row2_col17, #T_edcff_row3_col8, #T_edcff_row3_col11, #T_edcff_row3_col13, #T_edcff_row3_col14, #T_edcff_row3_col15, #T_edcff_row4_col7, #T_edcff_row4_col9, #T_edcff_row4_col11, #T_edcff_row4_col12, #T_edcff_row4_col13, #T_edcff_row4_col15, #T_edcff_row4_col16, #T_edcff_row4_col17, #T_edcff_row5_col7, #T_edcff_row5_col8, #T_edcff_row5_col9, #T_edcff_row5_col10, #T_edcff_row5_col11, #T_edcff_row5_col13, #T_edcff_row5_col15, #T_edcff_row5_col17, #T_edcff_row6_col8 {
  background-color: #b40426;
  color: #f1f1f1;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.20</td>
<td>0.84</td>
<td>0.11</td>
<td>0.00</td>
<td>0.55</td>
<td>1.24</td>
<td>0.55</td>
<td>-0.00</td>
<td>-0.02</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.20</td>
<td>0.98</td>
<td>1.00</td>
<td>0.30</td>
</tr>
<tr>
<td>1</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.19</td>
<td>0.00</td>
<td>-0.14</td>
<td>-0.87</td>
<td>-0.50</td>
<td>-0.00</td>
<td>-0.34</td>
<td>-0.28</td>
<td>-0.53</td>
<td>-0.24</td>
<td>-0.34</td>
<td>0.23</td>
<td>-0.09</td>
</tr>
<tr>
<td>2</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-1.34</td>
<td>-0.82</td>
<td>-1.02</td>
<td>-0.75</td>
<td>-0.74</td>
<td>-0.56</td>
<td>-0.68</td>
<td>-0.71</td>
<td>-1.00</td>
<td>-0.65</td>
<td>-0.56</td>
</tr>
<tr>
<td>3</td>
<td>0.23</td>
<td>0.18</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.00</td>
<td>-0.27</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.21</td>
<td>-0.00</td>
<td>-0.28</td>
<td>-0.21</td>
<td>-0.24</td>
<td>0.02</td>
<td>0.00</td>
</tr>
<tr>
<td>4</td>
<td>0.09</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.08</td>
<td>-0.00</td>
<td>-0.14</td>
<td>-0.00</td>
<td>-0.50</td>
<td>-0.01</td>
<td>-0.25</td>
<td>0.23</td>
<td>-0.20</td>
<td>-0.14</td>
<td>-0.66</td>
</tr>
<tr>
<td>5</td>
<td>0.18</td>
<td>0.49</td>
<td>0.00</td>
<td>0.00</td>
<td>0.03</td>
<td>0.00</td>
<td>0.00</td>
<td>-0.79</td>
<td>-0.36</td>
<td>-0.49</td>
<td>-0.39</td>
<td>-0.69</td>
<td>-0.00</td>
<td>-0.09</td>
<td>0.08</td>
<td>-0.84</td>
<td>0.10</td>
<td>-0.25</td>
</tr>
<tr>
<td>6</td>
<td>0.64</td>
<td>0.76</td>
<td>0.08</td>
<td>0.50</td>
<td>0.62</td>
<td>0.79</td>
<td>0.68</td>
<td>-0.00</td>
<td>-0.06</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.28</td>
<td>0.24</td>
<td>0.86</td>
<td>0.87</td>
</tr>
<tr>
<td>7</td>
<td>0.32</td>
<td>0.24</td>
<td>0.23</td>
<td>0.18</td>
<td>0.76</td>
<td>0.62</td>
<td>0.28</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>0.13</td>
<td>0.73</td>
<td>0.09</td>
<td>0.87</td>
</tr>
<tr>
<td>8</td>
<td>1.23</td>
<td>0.50</td>
<td>0.27</td>
<td>0.51</td>
<td>1.08</td>
<td>2.00</td>
<td>0.60</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>-0.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">MonoDense</span><span class="p">(</span>
    <span class="n">units</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
    <span class="n">monotonicity_indicator</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">is_convex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">is_concave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">display_kernel</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">monotonicity_indicator</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 5, 7, 8)]         0

 mono_dense_5 (MonoDense)    (None, 5, 7, 12)          108

=================================================================
Total params: 108
Trainable params: 108
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>
<style type="text/css">
#T_d6491_row0_col0, #T_d6491_row1_col0, #T_d6491_row2_col0 {
  background-color: #3b4cc0;
  color: #f1f1f1;
}
#T_d6491_row3_col0, #T_d6491_row4_col0, #T_d6491_row5_col0 {
  background-color: #b40426;
  color: #f1f1f1;
}
#T_d6491_row6_col0, #T_d6491_row7_col0 {
  background-color: #dcdddd;
  color: #000000;
}
</style>

<table>
<thead>
<tr>
<th> </th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.00</td>
</tr>
<tr>
<td>1</td>
<td>1.00</td>
</tr>
<tr>
<td>2</td>
<td>1.00</td>
</tr>
<tr>
<td>3</td>
<td>-1.00</td>
</tr>
<tr>
<td>4</td>
<td>-1.00</td>
</tr>
<tr>
<td>5</td>
<td>-1.00</td>
</tr>
<tr>
<td>6</td>
<td>0.00</td>
</tr>
<tr>
<td>7</td>
<td>0.00</td>
</tr>
</tbody>
</table>
<h3 id="mono-blocks">Mono blocks<a class="headerlink" href="#mono-blocks" title="Permanent link">¤</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">x</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># monotonicity indicator must be broadcastable to input shape, so we use the vector of length 8</span>
<span class="n">monotonicity_indicator</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>

<span class="c1"># this mono block has 4 layers with the final one having the shape</span>
<span class="n">mono_block</span> <span class="o">=</span> <span class="n">_create_mono_block</span><span class="p">(</span>
    <span class="n">units</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span>
    <span class="n">monotonicity_indicator</span><span class="o">=</span><span class="n">monotonicity_indicator</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;elu&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mono_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">mono_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">MonoDense</span><span class="p">)]</span>
<span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">mono_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">monotonicity_indicator</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
<span class="k">for</span> <span class="n">mono_layer</span> <span class="ow">in</span> <span class="n">mono_layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">mono_layer</span><span class="o">.</span><span class="n">monotonicity_indicator</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>

<span class="k">for</span> <span class="n">mono_layer</span> <span class="ow">in</span> <span class="n">mono_layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="k">assert</span> <span class="n">mono_layer</span><span class="o">.</span><span class="n">org_activation</span> <span class="o">==</span> <span class="s2">&quot;elu&quot;</span>
<span class="k">assert</span> <span class="n">mono_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">org_activation</span> <span class="o">==</span> <span class="kc">None</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Model: &quot;model_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 5, 7, 8)]         0

 mono_dense_0 (MonoDense)    (None, 5, 7, 16)          144

 dropout (Dropout)           (None, 5, 7, 16)          0

 mono_dense_1_increasing (Mo  (None, 5, 7, 16)         272       
 noDense)

 dropout_1 (Dropout)         (None, 5, 7, 16)          0

 mono_dense_2_increasing (Mo  (None, 5, 7, 16)         272       
 noDense)

 dropout_2 (Dropout)         (None, 5, 7, 16)          0

 mono_dense_3_increasing (Mo  (None, 5, 7, 3)          51        
 noDense)

=================================================================
Total params: 739
Trainable params: 739
Non-trainable params: 0
_________________________________________________________________
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">param</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">_prepare_mono_input_n_param</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">actual</span> <span class="o">==</span> <span class="n">expected</span><span class="p">,</span> <span class="n">actual</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">param</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>

<span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">actual</span> <span class="o">=</span> <span class="n">_prepare_mono_input_n_param</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>

<span class="n">e</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;ExceptionInfo ValueError(&quot;Uncompatible types: type(inputs)=&lt;class &#39;keras.engine.keras_tensor.KerasTensor&#39;&gt;, type(param)=&lt;class &#39;dict&#39;&gt;&quot;) tblen=2&gt;
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">_prepare_mono_input_n_param</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">a</span><span class="p">},</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">actual</span> <span class="o">==</span> <span class="p">([</span><span class="n">a</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">_prepare_mono_input_n_param</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">a</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">b</span><span class="p">},</span> <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
<span class="k">assert</span> <span class="n">actual</span> <span class="o">==</span> <span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">actual</span> <span class="o">=</span> <span class="n">_prepare_mono_input_n_param</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))},</span> <span class="p">{</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
<span class="n">e</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;ExceptionInfo ValueError(&quot;{&#39;a&#39;} != {&#39;a&#39;, &#39;b&#39;}&quot;) tblen=2&gt;
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">_prepare_mono_input_n_param</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">actual</span> <span class="o">==</span> <span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">a</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">_prepare_mono_input_n_param</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">actual</span> <span class="o">==</span> <span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;1&quot;</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">monotonicity_indicator</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">is_convex</span> <span class="o">=</span> <span class="p">[</span><span class="kc">True</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">is_concave</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>
<span class="n">names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="s2">&quot;abc&quot;</span><span class="p">)</span>
<span class="n">has_convex</span><span class="p">,</span> <span class="n">has_concave</span> <span class="o">=</span> <span class="n">_check_convexity_params</span><span class="p">(</span>
    <span class="n">monotonicity_indicator</span><span class="p">,</span> <span class="n">is_convex</span><span class="p">,</span> <span class="n">is_concave</span><span class="p">,</span> <span class="n">names</span>
<span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">has_convex</span><span class="p">,</span> <span class="n">has_concave</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<h3 id="type-1-architecture">Type-1 architecture<a class="headerlink" href="#type-1-architecture" title="Permanent link">¤</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">n_layers</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="s2">&quot;abcd&quot;</span><span class="p">)}</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">_create_type_1</span><span class="p">(</span>
    <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
    <span class="n">units</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">final_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;elu&quot;</span><span class="p">,</span>
    <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span>
    <span class="n">final_activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">,</span>
    <span class="n">monotonicity_indicator</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">is_convex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">mono_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">MonoDense</span><span class="p">)]</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mono_layers</span><span class="p">)</span> <span class="o">==</span> <span class="n">n_layers</span>

<span class="c1"># check monotonicity indicator</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span>
    <span class="n">mono_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">monotonicity_indicator</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">mono_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">monotonicity_indicator</span> <span class="o">==</span> <span class="mi">1</span>

<span class="c1"># check convexity and concavity</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">mono_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">is_convex</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">mono_layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">is_concave</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Model: &quot;model_2&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 a (InputLayer)                 [(None, 1)]          0           []

 b (InputLayer)                 [(None, 1)]          0           []

 c (InputLayer)                 [(None, 1)]          0           []

 d (InputLayer)                 [(None, 1)]          0           []

 concatenate (Concatenate)      (None, 4)            0           [&#39;a[0][0]&#39;,                      
                                                                  &#39;b[0][0]&#39;,                      
                                                                  &#39;c[0][0]&#39;,                      
                                                                  &#39;d[0][0]&#39;]

 mono_dense_0_convex (MonoDense  (None, 64)          320         [&#39;concatenate[0][0]&#39;]            
 )

 dropout_3 (Dropout)            (None, 64)           0           [&#39;mono_dense_0_convex[0][0]&#39;]

 mono_dense_1_increasing_convex  (None, 64)          4160        [&#39;dropout_3[0][0]&#39;]              
  (MonoDense)

 dropout_4 (Dropout)            (None, 64)           0           [&#39;mono_dense_1_increasing_convex[
                                                                 0][0]&#39;]

 mono_dense_2_increasing_convex  (None, 64)          4160        [&#39;dropout_4[0][0]&#39;]              
  (MonoDense)

 dropout_5 (Dropout)            (None, 64)           0           [&#39;mono_dense_2_increasing_convex[
                                                                 0][0]&#39;]

 mono_dense_3_increasing_convex  (None, 10)          650         [&#39;dropout_5[0][0]&#39;]              
  (MonoDense)

 tf.nn.softmax (TFOpLambda)     (None, 10)           0           [&#39;mono_dense_3_increasing_convex[
                                                                 0][0]&#39;]

==================================================================================================
Total params: 9,290
Trainable params: 9,290
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre></div>
<h3 id="type-2-architecture">Type-2 architecture<a class="headerlink" href="#type-2-architecture" title="Permanent link">¤</a></h3>
<div class="highlight"><pre><span></span><code><span class="n">monotonicity_indicator</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">input_units</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">monotonicity_indicator</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
    <span class="p">[[</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span> <span class="o">*</span> <span class="n">input_units</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">monotonicity_indicator</span><span class="p">],</span> <span class="p">[]</span>
<span class="p">)</span>
<span class="n">monotonicity_indicator</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[1, 1, 0, 0, 1, 1]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">dropout</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;*&quot;</span> <span class="o">*</span> <span class="mi">120</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dropout</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">Input</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="s2">&quot;abcd&quot;</span><span class="p">)}</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">_create_type_2</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">final_units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;elu&quot;</span><span class="p">,</span>
        <span class="n">final_activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">monotonicity_indicator</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">is_convex</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">is_concave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>************************************************************************************************************************

dropout=False

Model: &quot;model_3&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 a (InputLayer)                 [(None, 1)]          0           []

 b (InputLayer)                 [(None, 1)]          0           []

 c (InputLayer)                 [(None, 1)]          0           []

 d (InputLayer)                 [(None, 1)]          0           []

 mono_dense_a_increasing_convex  (None, 8)           16          [&#39;a[0][0]&#39;]                      
  (MonoDense)

 dense_b (Dense)                (None, 8)            16          [&#39;b[0][0]&#39;]

 mono_dense_c_decreasing (MonoD  (None, 8)           16          [&#39;c[0][0]&#39;]                      
 ense)

 dense_d (Dense)                (None, 8)            16          [&#39;d[0][0]&#39;]

 preprocessed_features (Concate  (None, 32)          0           [&#39;mono_dense_a_increasing_convex[
 nate)                                                           0][0]&#39;,                          
                                                                  &#39;dense_b[0][0]&#39;,                
                                                                  &#39;mono_dense_c_decreasing[0][0]&#39;,
                                                                  &#39;dense_d[0][0]&#39;]

 mono_dense_0_convex (MonoDense  (None, 32)          1056        [&#39;preprocessed_features[0][0]&#39;]  
 )

 mono_dense_1_increasing_convex  (None, 32)          1056        [&#39;mono_dense_0_convex[0][0]&#39;]    
  (MonoDense)

 mono_dense_2_increasing_convex  (None, 10)          330         [&#39;mono_dense_1_increasing_convex[
  (MonoDense)                                                    0][0]&#39;]

 tf.nn.softmax_1 (TFOpLambda)   (None, 10)           0           [&#39;mono_dense_2_increasing_convex[
                                                                 0][0]&#39;]

==================================================================================================
Total params: 2,506
Trainable params: 2,506
Non-trainable params: 0
__________________________________________________________________________________________________
************************************************************************************************************************

dropout=True

Model: &quot;model_4&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 a (InputLayer)                 [(None, 1)]          0           []

 b (InputLayer)                 [(None, 1)]          0           []

 c (InputLayer)                 [(None, 1)]          0           []

 d (InputLayer)                 [(None, 1)]          0           []

 mono_dense_a_increasing_convex  (None, 8)           16          [&#39;a[0][0]&#39;]                      
  (MonoDense)

 dense_b (Dense)                (None, 8)            16          [&#39;b[0][0]&#39;]

 mono_dense_c_decreasing (MonoD  (None, 8)           16          [&#39;c[0][0]&#39;]                      
 ense)

 dense_d (Dense)                (None, 8)            16          [&#39;d[0][0]&#39;]

 preprocessed_features (Concate  (None, 32)          0           [&#39;mono_dense_a_increasing_convex[
 nate)                                                           0][0]&#39;,                          
                                                                  &#39;dense_b[0][0]&#39;,                
                                                                  &#39;mono_dense_c_decreasing[0][0]&#39;,
                                                                  &#39;dense_d[0][0]&#39;]

 mono_dense_0_convex (MonoDense  (None, 32)          1056        [&#39;preprocessed_features[0][0]&#39;]  
 )

 dropout_6 (Dropout)            (None, 32)           0           [&#39;mono_dense_0_convex[0][0]&#39;]

 mono_dense_1_increasing_convex  (None, 32)          1056        [&#39;dropout_6[0][0]&#39;]              
  (MonoDense)

 dropout_7 (Dropout)            (None, 32)           0           [&#39;mono_dense_1_increasing_convex[
                                                                 0][0]&#39;]

 mono_dense_2_increasing_convex  (None, 10)          330         [&#39;dropout_7[0][0]&#39;]              
  (MonoDense)

 tf.nn.softmax_2 (TFOpLambda)   (None, 10)           0           [&#39;mono_dense_2_increasing_convex[
                                                                 0][0]&#39;]

==================================================================================================
Total params: 2,506
Trainable params: 2,506
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre></div>


  




                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      2022 onwards, AIRT Technologies d.o.o.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
    and 
    <a href="https://nbdev-mkdocs.airt.ai/" target="_blank" rel="noopener">
      Material for nbdev
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.indexes", "navigation.top", "search.suggest", "search.highlight", "search.share", "content.code.copy"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.b4d07000.min.js"></script>
      
        <script src="../overrides/js/extra.js"></script>
      
        <script src="../overrides/js/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>